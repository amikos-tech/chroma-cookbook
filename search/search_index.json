{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ChromaDB Cookbook","text":"<p>This is a collection of ways to use, deploy, configure and troubleshoot ChromaDB. It is a result of experiences of supporting ChromaDB community over the past months.</p> <p>Latest ChromaDB version: 0.4.22</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>We suggest you first head to the Concepts section to get familiar with ChromaDB concepts, such as Documents, Metadata, Embeddings, etc.</p> <p>Once you're comfortable with the concepts, you can head to the Quickstart section to get</p>"},{"location":"contributing/getting-started/","title":"Getting Started with Contributing to Chroma","text":""},{"location":"contributing/getting-started/#overview","title":"Overview","text":"<p>Here are some steps to follow:</p> <ul> <li>Fork the repository (if you are part of an organization to which you cannot grant permissions it might be advisable to fork under your own user account to allow other community members to contribute by granting them permissions, something that is a bit more difficult at organizational level)</li> <li>Clone your forked repo locally (git clone ...) under a dir with an apt name for the change you want to make e.g. <code>my_awesome_feature</code></li> <li>Create a branch for your change (git checkout -b my_awesome_feature)</li> <li>Make your changes</li> <li>Test (see Testing)</li> <li>Lint (see Linting)</li> <li>Commit your changes (git commit -am 'Added some feature')</li> <li>Push to the branch (git push origin my_awesome_feature)</li> <li>Create a new Pull Request (PR) from your forked repository to the main Chroma repository</li> </ul>"},{"location":"contributing/getting-started/#testing","title":"Testing","text":"<p>It is generally good to test your changes before submitting a PR.</p> <p>To run the full test suite:</p> <pre><code>pip install -r requirements_dev.txt\npytest\n</code></pre> <p>To run a specific test:</p> <pre><code>pytest chromadb/tests/test_api.py::test_get_collection\n</code></pre> <p>If you want to see the output of print statements in the tests, you can run:</p> <pre><code>pytest -s\n</code></pre> <p>If you want your pytest to stop on first failure, you can run:</p> <pre><code>pytest -x\n</code></pre>"},{"location":"contributing/getting-started/#integration-tests","title":"Integration Tests","text":"<p>You can only run the integration tests by running:</p> <pre><code>sh bin/bin/integration-test\n</code></pre> <p>The above will create a docker container and will run the integration tests against it. This will also include JS client.</p>"},{"location":"contributing/getting-started/#linting","title":"Linting","text":""},{"location":"contributing/useful-shortcuts/","title":"Useful Shortcuts for Contributors","text":""},{"location":"contributing/useful-shortcuts/#git","title":"Git","text":""},{"location":"contributing/useful-shortcuts/#aliases","title":"Aliases","text":""},{"location":"contributing/useful-shortcuts/#create-venv-and-install-dependencies","title":"Create venv and install dependencies","text":"<p>Add the following to your <code>.bashrc</code>, <code>.zshrc</code> or <code>.profile</code>:</p> <pre><code>alias chroma-init='python -m virtualenv venv &amp;&amp; source venv/bin/activate &amp;&amp; pip install -r requirements.txt &amp;&amp; pip install -r requirements_dev.txt'\n</code></pre>"},{"location":"core/clients/","title":"Chroma Clients","text":""},{"location":"core/clients/#persistent-client","title":"Persistent Client","text":"<p>To create your a local persistent client use the <code>PersistentClient</code> class. This client will store all data locally in a directory on your machine at the path you specify.</p> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")\n</code></pre> <p><code>path</code> parameter must be a local path on the machine where Chroma is running. If the path does not exist, it will be created. The path can be relative or absolute. If the path is not specified, the default is <code>chroma/</code> in the current working directory.</p>"},{"location":"core/clients/#uses-of-persistent-client","title":"Uses of Persistent Client","text":"<p>The persistent client is useful for:</p> <ul> <li>Local development: You can use the persistent client to develop locally and test out ChromaDB.</li> <li>Embedded applications: You can use the persistent client to embed ChromaDB in your application. For example, if   you are building a web application, you can use the persistent client to store data locally on the server.</li> </ul>"},{"location":"core/clients/#http-client","title":"HTTP Client","text":"<p>Chroma also provides HTTP Client, suitable for use in a client-server mode. This client can be used to connect to a remote ChromaDB server.</p> <pre><code>import chromadb\n\nclient = chromadb.HttpClient(host=\"localhost\", port=\"8000\")\n</code></pre> <p>HTTP client takes two optional parameters:</p> <ul> <li><code>host</code>: The host of the remote server. If not specified, the default is <code>localhost</code>.</li> <li><code>port</code>: The port of the remote server. If not specified, the default is <code>8000</code>.</li> <li><code>ssl</code>: If <code>True</code>, the client will use HTTPS. If not specified, the default is <code>False</code>.</li> </ul>"},{"location":"core/clients/#uses-of-http-client","title":"Uses of HTTP Client","text":"<p>The HTTP client is ideal for when you want to scale your application or move off of local machine storage. It is important to note that there are trade-offs associated with using HTTP client:</p> <ul> <li>Network latency - The time it takes to send a request to the server and receive a response.</li> <li>Serialization and deserialization overhead - The time it takes to convert data to a format that can be sent over the   network and then convert it back to its original format.</li> <li>Security - The data is sent over the network, so it is important to ensure that the connection is secure (we recommend   using both HTTPS and authentication).</li> <li>Availability - The server must be available for the client to connect to it.</li> <li>Bandwidth usage - The amount of data sent over the network.</li> <li>Data privacy and compliance - Storing data on a remote server may require compliance with data protection laws and   regulations.</li> <li>Difficulty in debugging - Debugging network issues can be more difficult than debugging local issues. The same applies   to server-side issues.</li> </ul>"},{"location":"core/clients/#host-parameter-special-cases","title":"Host parameter special cases","text":"<p>The <code>host</code> parameter supports a more advanced syntax than just the hostname. You can specify the whole endpoint ULR ( without the API paths), e.g. <code>https://chromadb.example.com:8000/my_server/path/</code>. This is useful when you want to use a reverse proxy or load balancer in front of your ChromaDB server.</p>"},{"location":"core/clients/#ephemeral-client","title":"Ephemeral Client","text":"<p>Ephemeral client is a client that does not store any data on disk. It is useful for fast prototyping and testing. To get started with an ephemeral client, use the <code>EphemeralClient</code> class.</p> <pre><code>import chromadb\n\nclient = chromadb.EphemeralClient()\n</code></pre>"},{"location":"core/clients/#environmental-variable-configured-client","title":"Environmental Variable Configured Client","text":"<p>You can also configure the client using environmental variables. This is useful when you want to configure any of the client configurations listed above via environmental variables.</p> <pre><code>import chromadb\n\nclient = chromadb.Client()\n</code></pre> <p>Short list of env variables that can be used to configure the client:</p> <p>Note: For complete list of available settings check (<code>chromadb.config.Settings</code>).</p> Env Variable Description Default chroma_api_impl The API implementation to use. There are two options: <code>chromadb.api.segment.SegmentAPI</code> (persistent client)  <code>chromadb.api.fastapi.FastAPI</code> (Http client) <code>chromadb.api.segment.SegmentAPI</code> chroma_server_host The host of the remote server. This is required for HttpClient only. <code>None</code>/<code>null</code> chroma_server_http_port The port of the remote server. This is required for HttpClient only. <code>8000</code> chroma_server_headers The headers to be sent to the server. This is required for HttpClient only. <code>None</code>/<code>null</code>"},{"location":"core/collections/","title":"Collections","text":"<p>Collections are the grouping mechanism for embeddings, documents, and metadata.</p>"},{"location":"core/collections/#collection-basics","title":"Collection Basics","text":""},{"location":"core/collections/#collection-properties","title":"Collection Properties","text":"<p>Each collection is characterized by the following properties:</p> <ul> <li><code>name</code>: The name of the collection</li> <li><code>metadata</code>: A dictionary of metadata associated with the collection. The metadata is a dictionary of key-value pairs.</li> </ul>"},{"location":"core/collections/#creating-a-collection","title":"Creating a collection","text":"<pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncol = client.create_collection(\"test\")\n</code></pre> <p>Alternatively you can use the <code>get_or_create_collection</code> method to create a collection if it doesn't exist already.</p> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncol = client.get_or_create_collection(\"test\")\n</code></pre>"},{"location":"core/collections/#deleting-a-collection","title":"Deleting a collection","text":"<pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\nclient.delete_collection(\"test\")\n</code></pre>"},{"location":"core/collections/#listing-all-collections","title":"Listing all collections","text":"<pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncollections = client.list_collections()\n</code></pre>"},{"location":"core/collections/#getting-a-collection","title":"Getting a collection","text":"<pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncol = client.get_collection(\"test\")\n</code></pre>"},{"location":"core/collections/#modifying-a-collection","title":"Modifying a collection","text":"<p>Both collection properties (<code>name</code> and <code>metadata</code>) can be modified.</p> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncol = client.get_collection(\"test\")\ncol.modify(name=\"test2\", metadata={\"key\": \"value\"})\n</code></pre> <p>Metadata</p> <p>Metadata is always overwritten when modified. If you want to add a new key-value pair to the metadata, you must first get the existing metadata and then add the new key-value pair to it.</p>"},{"location":"core/collections/#collection-utilities","title":"Collection Utilities","text":""},{"location":"core/collections/#copying-local-collection-to-remote","title":"Copying Local Collection to Remote","text":"<p>The following example demonstrates how to copy a local collection to a remote ChromaDB server. (it also works in reverse)</p> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"my_local_data\")\nremote_client = chromadb.HttpClient()\n\ncollection = client.get_or_create_collection(\"local_collection\")\ncollection.add(\n    ids=[\"1\",\"2\"],\n    documents=[\"hello world\",\"hello ChromaDB\"],\n    metadatas=[{\"a\":1},{\"b\":2}])\nremote_collection = remote_client.get_or_create_collection(\"remote_collection\",\n                                                           metadata=collection.metadata)\nexisting_count = collection.count()\nbatch_size = 10\nfor i in range(0, existing_count, batch_size):\n    batch = collection.get(\n        include=[\"metadatas\", \"documents\", \"embeddings\"], \n        limit=batch_size, \n        offset=i)\n    remote_collection.add(\n        ids=batch[\"ids\"], \n        documents=batch[\"documents\"], \n        metadatas=batch[\"metadatas\"],\n        embeddings=batch[\"embeddings\"])\n</code></pre> <p>Using ChromaDB Data Pipes</p> <p>There is a more efficient way to copy data between local and remote collections using ChromaDB Data Pipes package. <pre><code>pip install chromadb-data-pipes\ncdp export \"file://path/to_local_data/local_collection\" | \\\ncdp import \"http://remote_chromadb:port/remote_collection\" --create\n</code></pre></p>"},{"location":"core/collections/#cloning-a-collection","title":"Cloning a collection","text":"<p>Here are some reasons why you might want to clone a collection:</p> <ul> <li>Change distance function (via metadata - <code>hnsw:space</code>)</li> <li>Change HNSW hyper parameters (<code>hnsw:M</code>, <code>hnsw:construction_ef</code>, <code>hnsw:search_ef</code>)</li> </ul> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncol = client.get_or_create_collection(\"test\")  # create a new collection with L2 (default)\n\ncol.add(ids=[f\"{i}\" for i in range(1000)], documents=[f\"document {i}\" for i in range(1000)])\nnewCol = client.get_or_create_collection(\"test1\", metadata={\n    \"hnsw:space\": \"cosine\"})  # let's change the distance function to cosine\n\nexisting_count = col.count()\nbatch_size = 10\nfor i in range(0, existing_count, batch_size):\n    batch = col.get(include=[\"metadatas\", \"documents\", \"embeddings\"], limit=batch_size, offset=i)\n    newCol.add(ids=batch[\"ids\"], documents=batch[\"documents\"], metadatas=batch[\"metadatas\"],\n               embeddings=batch[\"embeddings\"])\n\nprint(newCol.count())\nprint(newCol.get(offset=0, limit=10))  # get first 10 documents\n</code></pre>"},{"location":"core/collections/#updating-documentrecord-metadata","title":"Updating Document/Record Metadata","text":"<p>In this example we loop through all documents of a collection and strip all metadata fields of leading and trailing whitespace. Change the <code>update_metadata</code> function to suit your needs.</p> <pre><code>from chromadb import Settings\nimport chromadb\n\nclient = chromadb.PersistentClient(path=\"test\", settings=Settings(allow_reset=True))\nclient.reset()  # reset the database so we can run this script multiple times\ncol = client.get_or_create_collection(\"test\")\ncount = col.count()\n\n\ndef update_metadata(metadata: dict):\n    return {k: v.strip() for k, v in metadata.items()}\n\n\nfor i in range(0, count, 10):\n    batch = col.get(include=[\"metadatas\"], limit=10, offset=i)\n    col.update(ids=batch[\"ids\"], metadatas=[update_metadata(metadata) for metadata in batch[\"metadatas\"]])\n</code></pre>"},{"location":"core/concepts/","title":"Chroma Core Concepts","text":""},{"location":"core/concepts/#tenancy-and-db-hierarchies","title":"Tenancy and DB Hierarchies","text":"<p>The following picture illustrates the tenancy and DB hierarchy in Chroma:</p> <p></p> <p>Storage</p> <p>In Chroma single-node, all data about tenancy, databases, collections and documents is stored in a single SQLite database.</p>"},{"location":"core/concepts/#tenants","title":"Tenants","text":"<p>A tenant is a logical grouping for a set of databases. A tenant is designed to model a single organization or user. A tenant can have multiple databases.</p>"},{"location":"core/concepts/#databases","title":"Databases","text":"<p>A database is a logical grouping for a set of collections. A database is designed to model a single application or project. A database can have multiple collections.</p>"},{"location":"core/concepts/#collections","title":"Collections","text":"<p>Collections are the grouping mechanism for embeddings, documents, and metadata.</p>"},{"location":"core/concepts/#documents","title":"Documents","text":"<p>Chunks of text</p> <p>Documents in ChromaDB lingo are chunks of text that fits within the embedding model's context window.  Unlike other frameworks that use the term \"document\" to mean a file,  ChromaDB uses the term \"document\" to mean a chunk of text.</p> <p>Documents are raw chunks of text that are associated with an embedding. Documents are stored in the database and can be queried for.</p>"},{"location":"core/concepts/#metadata","title":"Metadata","text":"<p>Metadata is a dictionary of key-value pairs that can be associated with an embedding. Metadata is stored in the database and can be queried for.</p> <p>Metadata values can be of the following types:</p> <ul> <li>strings</li> <li>integers</li> <li>floats</li> <li>booleans</li> </ul>"},{"location":"core/concepts/#embedding-function","title":"Embedding Function","text":"<p>Also referred to as embedding model, embedding functions in ChromaDB are wrappers that expose a consistent interface for generating embedding vectors from documents or text queries.</p> <p>For a list of supported embedding functions see Chroma's official documentation.</p>"},{"location":"core/concepts/#distance-function","title":"Distance Function","text":"<p>Distance functions help in calculating the difference (distance) between two embedding vectors. ChromaDB supports the following distance functions:</p> <ul> <li>cosine - Useful for text similarity</li> <li>euclidean (L2) - useful for text similarity, more sensitive to noise than <code>cosine</code></li> <li>Inner Product (IP) - recommender systems</li> </ul>"},{"location":"core/concepts/#embedding-vector","title":"Embedding Vector","text":"<p>A representation of a document in the embedding space in te form of a vector, list of 32-bit floats (or ints).</p>"},{"location":"core/concepts/#embedding-model","title":"Embedding Model","text":""},{"location":"core/concepts/#document-and-metadata-index","title":"Document and Metadata Index","text":""},{"location":"core/concepts/#vector-index-hnsw-index","title":"Vector Index (HNSW Index)","text":"<p>Under the hood (ca. v0.4.22) Chroma uses its own fork HNSW lib for indexing and searching vectors.</p> <p>In a single-node mode, Chroma will create a single HNSW index for each collection. The index is stored in a subdir of your persistent dir, named after the collection id (UUID-based).</p> <p>The HNSW lib uses fast ANN algo to search the vectors in the index.</p>"},{"location":"core/document-ids/","title":"Document IDs","text":"<p>Chroma is unopinionated about document IDs and delegates those decisions to the user. This frees users to build semantics around their IDs.</p>"},{"location":"core/document-ids/#note-on-compound-ids","title":"Note on Compound IDs","text":"<p>While you can choose to use IDs that are composed of multiple sub-IDs (e.g. <code>user_id</code> + <code>document_id</code>), it is important to highlight that Chroma does not support querying by partial ID.</p>"},{"location":"core/document-ids/#common-practices","title":"Common Practices","text":""},{"location":"core/document-ids/#uuids","title":"UUIDs","text":"<p>UUIDs are a common choice for document IDs. They are unique, and can be generated in a distributed fashion. They are also opaque, which means that they do not contain any information about the document itself. This can be a good thing, as it allows you to change the document without changing the ID.</p> <pre><code>import uuid\nimport chromadb\n\nmy_documents = [\n    \"Hello, world!\",\n    \"Hello, Chroma!\"\n]\n\nclient = chromadb.Client()\n\ncollection.add(ids=[uuid.uuid4() for _ in range(len(documents))], documents=my_documents)\n</code></pre>"},{"location":"core/document-ids/#hashes","title":"Hashes","text":"<p>Hashes are another common choice for document IDs. They are unique, and can be generated in a distributed fashion. They are also opaque, which means that they do not contain any information about the document itself. This can be a good thing, as it allows you to change the document without changing the ID.</p> <pre><code>import hashlib\nimport os\nimport chromadb\n\ndef generate_sha256_hash():\n    # Generate a random number\n    random_data = os.urandom(16)\n    # Create a SHA256 hash object\n    sha256_hash = hashlib.sha256()\n    # Update the hash object with the random data\n    sha256_hash.update(random_data)\n    # Return the hexadecimal representation of the hash\n    return sha256_hash.hexdigest()\n\n\nmy_documents = [\n    \"Hello, world!\",\n    \"Hello, Chroma!\"\n]\n\nclient = chromadb.Client()\n\ncollection.add(ids=[generate_sha256_hash() for _ in range(len(documents))], documents=my_documents)\n</code></pre> <p>It is also possible to use the document as basis for the hash, the downside of that is that when the document changes and you have a semantic around the text as relating to the hash, you may need to update the hash.</p> <pre><code>import hashlib\nimport chromadb\n\ndef generate_sha256_hash_from_text(text):\n    # Create a SHA256 hash object\n    sha256_hash = hashlib.sha256()\n    # Update the hash object with the text encoded to bytes\n    sha256_hash.update(text.encode('utf-8'))\n    # Return the hexadecimal representation of the hash\n    return sha256_hash.hexdigest()\nmy_documents = [\n    \"Hello, world!\",\n    \"Hello, Chroma!\"\n]\n\nclient = chromadb.Client()\n\ncollection.add(ids=[generate_sha256_hash_from_text(documents[i]) for i in range(len(documents))], documents=my_documents)\n</code></pre>"},{"location":"core/document-ids/#semantic-strategies","title":"Semantic Strategies","text":"<p>In this section we'll explore a few different use cases for building semantics around document IDs.</p> <ul> <li>URL Slugs - if your docs are web pages with permalinks (e.g. blog posts), you can use the URL slug as the document ID.</li> <li>File Paths - if your docs are files on disk, you can use the file path as the document ID.</li> </ul>"},{"location":"core/install/","title":"Installation","text":""},{"location":"core/install/#core-chromadb","title":"Core ChromaDB","text":"<p>To install the latest version of chromadb, run:</p> <pre><code>pip install chromadb\n</code></pre> <p>To install a specific version of chromadb, run:</p> <pre><code>pip install chromadb==&lt;x.y.z&gt;\n</code></pre> <p>Releases</p> <p>You can find Chroma releases in PyPI here.</p>"},{"location":"core/install/#chromadb-python-client","title":"ChromaDB Python Client","text":"<p>To install the latest version of the ChromaDB Python client, run:</p> <pre><code>pip install chromadb-client\n</code></pre> <p>Releases</p> <p>You can find Chroma releases in PyPI here.</p>"},{"location":"core/system_constraints/","title":"Chroma System Constraints","text":"<p>This section contains common constraints of Chroma.</p> <ul> <li>Chroma is thread-safe</li> <li>Chroma is not process-safe</li> <li>Multiple Chroma Clients (Ephemeral, Persistent, Http) can be created from one or more threads within the same process</li> <li>A collection's name is unique within a Tenant and DB</li> <li>A collection's dimensions cannot change after creation =&gt; you cannot change the embedding function after creation</li> <li>Chroma operates in two modes - standalone (PersistentClient, EphemeralClient) and client/server (HttpClient with   ChromaServer)</li> <li>The distance function cannot be changed after collection creation.</li> </ul>"},{"location":"core/system_constraints/#operational-modes","title":"Operational Modes","text":"<p>Chroma can be operated in two modes:</p> <ul> <li>Standalone - This allows embedding Chroma in your python application without the need to communicate with external   processes.</li> <li>Client/Server - This allows embedding Chroma in your python application as a thin-client with minimal dependencies and   communicating with it via REST API. This is useful when you want to use Chroma from multiple processes or even   multiple machines.</li> </ul> <p>Depending on the mode you choose, you will need to consider the following component responsibilities:</p> <ul> <li>Standalone:<ul> <li>Clients (Persistent, Ephemeral) - Responsible for persistence, embedding, querying</li> </ul> </li> <li>Client/Server:<ul> <li>Clients (HttpClient) - Responsible for embedding, communication with Chroma server via REST API</li> <li>Server - Responsible for persistence and querying</li> </ul> </li> </ul> <p></p>"},{"location":"core/advanced/wal-pruning/","title":"Write-ahead Log (WAL) Pruning","text":"<p>As of this writing (v0.4.22) Chroma stores its WAL forever. This means that the WAL will grow indefinitely. This is obviously not ideal. Here we provide a small script + a few steps how to prune your WAL and keep it at a reasonable size. Pruning the WAL is particularly important if you have many writes to Chroma (e.g. documents are added, updated or deleted frequently).</p>"},{"location":"core/advanced/wal-pruning/#tooling","title":"Tooling","text":"<p>We have worked on a tooling to provide users with a way to prune their WAL - chroma-ops.</p> <p>To prune your WAL you can run the following command:</p> <pre><code>pip install chroma-ops\nchops cleanup-wal /path/to/persist_dir\n</code></pre> <p>\u26a0\ufe0f IMPORTANT: It is always a good thing to backup your data before you prune the WAL.</p>"},{"location":"core/advanced/wal-pruning/#manual","title":"Manual","text":"<p>Steps:</p> <p>Stop Chroma</p> <p>It is vitally important that you stop Chroma before you prune the WAL.  If you don't stop Chroma you risk corrupting</p> <ul> <li>\u26a0\ufe0f Stop Chroma</li> <li>\ud83d\udcbe Create a backup of your <code>chroma.sqlite3</code> file in your persistent dir</li> <li>\ud83d\udc40 Check your current <code>chroma.sqlite3</code> size (e.g. <code>ls -lh /path/to/persist/dir/chroma.sqlite3</code>)</li> <li>\ud83d\udda5\ufe0f Run the script below</li> <li>\ud83d\udd2d Check your current <code>chroma.sqlite3</code> size again to verify that the WAL has been pruned</li> <li>\ud83d\ude80 Start Chroma</li> </ul> <p>Script (store it in a file like <code>compact-wal.sql</code>)</p> wal_clean.py<pre><code>#!/usr/bin/env python3\n# Call the script: python wal_clean.py ./chroma-test-compact\nimport os\nimport sqlite3\nfrom typing import cast, Optional, Dict\nimport argparse\nimport pickle\n\n\nclass PersistentData:\n    \"\"\"Stores the data and metadata needed for a PersistentLocalHnswSegment\"\"\"\n\n    dimensionality: Optional[int]\n    total_elements_added: int\n    max_seq_id: int\n\n    id_to_label: Dict[str, int]\n    label_to_id: Dict[int, str]\n    id_to_seq_id: Dict[str, int]\n\n\ndef load_from_file(filename: str) -&gt; \"PersistentData\":\n    \"\"\"Load persistent data from a file\"\"\"\n    with open(filename, \"rb\") as f:\n        ret = cast(PersistentData, pickle.load(f))\n        return ret\n\n\ndef clean_wal(chroma_persist_dir: str):\n    if not os.path.exists(chroma_persist_dir):\n        raise Exception(f\"Persist {chroma_persist_dir} dir does not exist\")\n    if not os.path.exists(f'{chroma_persist_dir}/chroma.sqlite3'):\n        raise Exception(\n            f\"SQL file not found int persist dir {chroma_persist_dir}/chroma.sqlite3\")\n    # Connect to SQLite database\n    conn = sqlite3.connect(f'{chroma_persist_dir}/chroma.sqlite3')\n\n    # Create a cursor object\n    cursor = conn.cursor()\n\n    # SQL query\n    query = \"SELECT id,topic FROM segments where scope='VECTOR'\"  # Replace with your query\n\n    # Execute the query\n    cursor.execute(query)\n\n    # Fetch the results (if needed)\n    results = cursor.fetchall()\n    wal_cleanup_queries = []\n    for row in results:\n        # print(row)\n        metadata = load_from_file(\n            f'{chroma_persist_dir}/{row[0]}/index_metadata.pickle')\n        wal_cleanup_queries.append(\n            f\"DELETE FROM embeddings_queue WHERE seq_id &lt; {metadata.max_seq_id} AND topic='{row[1]}';\")\n\n    cursor.executescript('\\n'.join(wal_cleanup_queries))\n    # Close the cursor and connection\n    cursor.close()\n    conn.close()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('persist_dir', type=str)\n    arg = parser.parse_args()\n    print(arg.persist_dir)\n    clean_wal(arg.persist_dir)\n</code></pre> <p>Run the script</p> <pre><code># Let's create a backup\ntar -czvf /path/to/persist/dir/chroma.sqlite3.backup.tar.gz /path/to/persist/dir/chroma.sqlite3\nlsof /path/to/persist/dir/chroma.sqlite3 # make sure that no process is using the file\npython wal_clean.py /path/to/persist/dir/\n# start chroma\n</code></pre>"},{"location":"core/advanced/wal/","title":"Write-ahead Log (WAL)","text":"<p>Chroma uses WAL to ensure data durability, even if things go wrong (e.g. server crashes). To achieve the latter Chroma uses what is known in the DB-industry as WAL or Write-Ahead Log. The purpose of the WAL is to ensure that each user request (aka transaction) is safely stored before acknowledging back to the user. Subsequently, in fact immediately after writing to the WAL, the data is also written to the index. This enables Chroma to serve as real-time search engine, where the data is available for querying immediately after it is written to the WAL.</p> <p>Below is a diagram that illustrates the WAL in ChromaDB (ca. v0.4.22):</p> <p></p>"},{"location":"core/advanced/wal/#vector-indices-overview","title":"Vector Indices Overview","text":"<p>The diagram below illustrates how data gets transferred from the WAL to the binary vector indices (Bruteforce and HNSW):</p> <p></p> <p>For each collection Chroma maintains two binary indices - Bruteforce (in-memory, fast) and HNSW lib (persisted to disk, slow when adding new vectors and persisting). As you can imagine, the BF index serves the role of a buffer that holds the uncommitted to HNWS persisted index portion of the WAL. The HNSW index itself has a max sequence id counter, stored in a metadata file, that indicates from which position in the WAL the buffering to the BF index should begin. The latter buffering usually happens when the collection is first accessed.</p> <p>There are two transfer points (in the diagram, sync threshold) for BF to HNSW:</p> <ul> <li><code>hnsw:batch_size</code> - forces the BF vectors to be added to HNSW in-memory (this is a slow operation)</li> <li> <p><code>hnsw:sync_threshold</code> - forces Chroma to dump the HNSW in-memory index to disk (this is a slow operation)</p> </li> <li> <p>Both of the above sync points are controlled via Collection-level metadata with respective named params. It is   customary <code>hnsw:sync_threshold</code> &gt; <code>hnsw:batch_size</code></p> </li> </ul>"},{"location":"core/advanced/wal/#metadata-indices-overview","title":"Metadata Indices Overview","text":"<p>The following diagram illustrates how data gets transferred from the WAL to the metadata index:</p> <p></p>"},{"location":"core/advanced/wal/#further-reading","title":"Further Reading","text":"<p>For the DevOps minded folks we have a few more resources:</p> <ul> <li>WAL Pruning - Clean up your WAL</li> </ul>"},{"location":"embeddings/bring-your-own-embeddings/","title":"Creating your own embedding function","text":"<pre><code>from chromadb.api.types import (\n    Documents,\n    EmbeddingFunction,\n    Embeddings\n)\n\nclass MyCustomEmbeddingFunction(EmbeddingFunction[Documents]):\n    def __init__(\n        self,\n        my_ef_param: str\n    ):\n        \"\"\"Initialize the embedding function.\"\"\"\n\n    def __call__(self, input: Documents) -&gt; Embeddings:\n        \"\"\"Embed the input documents.\"\"\"\n        return self._my_ef(input)\n</code></pre> <p>Now let's break the above down.</p> <p>First you create a class that inherits from <code>EmbeddingFunction[Documents]</code>. The <code>Documents</code> type is a list of <code>Document</code> objects. Each <code>Document</code> object has a <code>text</code> attribute that contains the text of the document. Chroma also supports multi-modal</p>"},{"location":"embeddings/gpu-support/","title":"Embedding Functions GPU Support","text":"<p>By default, Chroma does not require GPU support for embedding functions. However, if you want to use GPU support, some of the functions, especially those running locally provide GPU support.</p>"},{"location":"embeddings/gpu-support/#default-embedding-functions-onnxruntime","title":"Default Embedding Functions (Onnxruntime)","text":"<p>To use the default embedding functions with GPU support, you need to install <code>onnxruntime-gpu</code> package. You can install it with the following command:</p> <pre><code>pip install onnxruntime-gpu\n</code></pre> <p>Note: To ensure no conflicts, you can uninstall <code>onnxruntime</code> (e.g. <code>pip uninstall onnxruntime</code>) in a separate environment.</p> <p>List available providers:</p> <pre><code>import onnxruntime\n\nprint(onnxruntime.get_available_providers())\n</code></pre> <p>Select the desired provider and set it as preferred before using the embedding functions (in the below example, we use <code>CUDAExecutionProvider</code>):</p> <pre><code>import time\nfrom chromadb.utils.embedding_functions import ONNXMiniLM_L6_V2\n\nef = ONNXMiniLM_L6_V2(preferred_providers=['CUDAExecutionProvider'])\n\ndocs = []\nfor i in range(1000):\n    docs.append(f\"this is a document with id {i}\")\n\nstart_time = time.perf_counter()\nembeddings = ef(docs)\nend_time = time.perf_counter()\nprint(f\"Elapsed time: {end_time - start_time} seconds\")\n</code></pre> <p>IMPORTANT OBSERVATION: Our observations are that for GPU support using sentence transformers with model <code>all-MiniLM-L6-v2</code> outperforms onnxruntime with GPU support. In practical terms on a Colab T4 GPU, the onnxruntime example above runs for about 100s whereas the equivalent sentence transformers example runs for about 1.8s.</p>"},{"location":"embeddings/gpu-support/#sentence-transformers","title":"Sentence Transformers","text":"<pre><code>import time\nfrom chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n# This will download the model to your machine and set it up for GPU support\nef = SentenceTransformerEmbeddingFunction(model_name=\"thenlper/gte-small\", device=\"cuda\")\n\n# Test with 10k documents\ndocs = []\nfor i in range(10000):\n    docs.append(f\"this is a document with id {i}\")\n\nstart_time = time.perf_counter()\nembeddings = ef(docs)\nend_time = time.perf_counter()\nprint(f\"Elapsed time: {end_time - start_time} seconds\")\n</code></pre> <p>Note: You can run the above example in google Colab - see the notebook</p>"},{"location":"integrations/langchain/embeddings/","title":"Langchain Embeddings","text":""},{"location":"integrations/langchain/embeddings/#embedding-functions","title":"Embedding Functions","text":"<p>Chroma and Langchain both offer embedding functions which are wrappers on top of popular embedding models.</p> <p>Unfortunately Chroma and LC's embedding functions are not compatible with each other. Below we offer two adapters to convert Chroma's embedding functions to LC's and vice versa.</p> <p>Here is the adapter to convert Chroma's embedding functions to LC's:</p> <pre><code>from langchain_core.embeddings import Embeddings\nfrom chromadb.api.types import EmbeddingFunction\n\nclass ChromaEmbeddingsAdapter(Embeddings):\n  def __init__(self,ef:EmbeddingFunction):\n    self.ef = ef\n\n  def embed_documents(self,texts):\n    return self.ef(texts)\n\n  def embed_query(self, query):\n    return self.ef([query])[0]\n</code></pre> <p>Here is the adapter to convert LC's embedding functions to Chroma's:</p> <pre><code>from langchain_core.embeddings import Embeddings\nfrom chromadb.api.types import EmbeddingFunction\n\nclass LangChainEmbeddingAdapter(EmbeddingFunction):\n  def __init__(self,ef:Embeddings):\n    self.ef = ef\n\n  def __call__(self, input: Documents) -&gt; Embeddings:\n    # LC EFs also have embed_query but Chroma doesn't support that so we just use embed_documents\n    # TODO: better type checking\n    return self.ef.embed_documents(input)\n</code></pre>"},{"location":"integrations/langchain/retrievers/","title":"\ud83e\udd9c\u26d3\ufe0f Langchain Retriever","text":"<p>TBD: describe what retrievers are in LC and how they work.</p>"},{"location":"integrations/langchain/retrievers/#vector-store-retriever","title":"Vector Store Retriever","text":"<p>In the below example we demonstrate how to use Chroma as a vector store retriever with a filter query.</p> <p>Note that the filter is supplied whenever we create the retriever object so the filter applies to all queries (<code>get_relevant_documents</code>).</p> <pre><code>from langchain.document_loaders import OnlinePDFLoader\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.vectorstores import Chroma\nfrom typing import Dict, Any\nimport chromadb\nfrom langchain_core.embeddings import Embeddings\n\nclient = chromadb.PersistentClient(path=\"./chroma\")\n\ncol = client.get_or_create_collection(\"test\")\n\ncol.upsert([f\"{i}\" for i in range(10)],documents=[f\"This is document #{i}\" for i in range(10)],metadatas=[{\"id\":f\"{i}\"} for i in range(10)])\n\nef = chromadb.utils.embedding_functions.DefaultEmbeddingFunction()\n\nclass DefChromaEF(Embeddings):\n  def __init__(self,ef):\n    self.ef = ef\n\n  def embed_documents(self,texts):\n    return self.ef(texts)\n\n  def embed_query(self, query):\n    return self.ef([query])[0]\n\n\ndb = Chroma(client=client, collection_name=\"test\",embedding_function=DefChromaEF(ef))\n\nretriever = db.as_retriever(search_kwargs={\"filter\":{\"id\":\"1\"}})\n\ndocs = retriever.get_relevant_documents(\"document\")\n\nassert len(docs)==1\n</code></pre> <p>Ref: https://colab.research.google.com/drive/1L0RwQVVBtvTTd6Le523P4uzz3m3fm0pH#scrollTo=xROOfxLohE5j</p>"},{"location":"integrations/llamaindex/embeddings/","title":"LlamaIndex Embeddings","text":""},{"location":"integrations/llamaindex/embeddings/#embedding-functions","title":"Embedding Functions","text":"<p>Chroma and LlamaIndex both offer embedding functions which are wrappers on top of popular embedding models.</p> <p>Unfortunately Chroma and LI's embedding functions are not compatible with each other. Below we offer an adapters to convert LI embedding function to Chroma one.</p> <pre><code>from llama_index.embeddings.base import BaseEmbedding\nfrom chromadb.api.types import EmbeddingFunction\n\nclass LlamaIndexEmbeddingAdapter(EmbeddingFunction):\n  def __init__(self,ef:BaseEmbedding):\n    self.ef = ef\n\n  def __call__(self, input: Documents) -&gt; Embeddings:\n    return [node.embedding for node in self.ef(input)]\n</code></pre> <p>An example of how to use the above with LlamaIndex:</p> <p>Note: Make sure you have <code>OPENAI_API_KEY</code> as env var.</p> <pre><code>from llama_index.embeddings import OpenAIEmbedding\nfrom llama_index import ServiceContext, set_global_service_context\nimport chromadb\n\nembed_model = OpenAIEmbedding(embed_batch_size=10)\n\nclient = chromadb.Client()\n\ncol = client.get_or_create_collection(\"test_collection\",embedding_function=LlamaIndexEmbeddingAdapter(embed_model))\n\ncol.add(ids=[\"1\"],documents=[\"this is a test document\"])\n# your embeddings should be of 1536 dimensions (OpenAI's ADA model)\n</code></pre>"},{"location":"running/running-chroma/","title":"Running Chroma","text":""},{"location":"running/running-chroma/#local-server","title":"Local Server","text":"<p>Article Link</p> <pre><code>This article is also available on Medium [Running ChromaDB \u2014 Part 1: Local Server](https://medium.com/@amikostech/running-chromadb-part-1-local-server-2c61cb1c9f2c).\n</code></pre>"},{"location":"running/running-chroma/#chroma-cli","title":"Chroma CLI","text":"<p>The simplest way to run Chroma locally is via the Chroma <code>cli</code> which is part of the core Chroma package.</p> <p>Prerequisites:</p> <ul> <li>Python 3.8 to 3.11 - Download Python | Python.org</li> </ul> <pre><code>pip install chromadb\nchroma run --host localhost --port 8000 --path ./my_chroma_data\n</code></pre> <p><code>--host</code> The host to which to listen to, by default it is <code>[localhost](http://localhost)</code> , but if you want to expose it to your entire network then you can specify `0.0.0.0``</p> <p><code>--port</code> The port on which to listen to, by default this is <code>8000</code>.</p> <p><code>--path</code> The path where to persist your Chroma data locally.</p>"},{"location":"running/running-chroma/#docker","title":"Docker","text":"<p>Running Chroma server locally can be achieved via a simple docker command as shown below.</p> <p>Prerequisites:</p> <ul> <li>Docker - Overview of Docker Desktop | Docker Docs</li> </ul> <pre><code>docker run -d --rm --name chromadb -v ./chroma:/chroma/chroma -e IS_PERSISTENT=TRUE -e ANONYMIZED_TELEMETRY=TRUE chromadb/chroma:latest\n</code></pre> <p>Options:</p> <ul> <li><code>-v</code> specifies a local dir which is where Chroma will store its data so when the container is destroyed the data remains</li> <li><code>-e</code> <code>IS_PERSISTENT=TRUE</code> let\u2019s Chroma know to persist data</li> <li><code>-e ANONYMIZED_TELEMETRY=TRUE</code> allows you to turn on (<code>TRUE</code>) or off (<code>FALSE</code>) anonymous product telemetry which helps the Chroma team in making informed decisions about Chroma OSS and commercial direction.</li> <li><code>chromadb/chroma:latest</code> indicates the latest Chroma version but can be replaced with any valid tag if a prior version is needed (e.g. <code>chroma:0.4.18</code>)</li> </ul>"},{"location":"running/running-chroma/#docker-compose-cloned-repo","title":"Docker Compose (Cloned Repo)","text":"<p>If you are feeling adventurous you can also use the Chroma <code>main</code> branch to run a local Chroma server with the latest changes:</p> <p>Prerequisites:</p> <ul> <li>Docker - Overview of Docker Desktop | Docker Docs</li> <li>Git - Git - Downloads (git-scm.com)</li> </ul> <pre><code>git clone https://github.com/chroma-core/chroma &amp;&amp; cd chroma\ndocker compose up -d --build\n</code></pre> <p>If you want to run a specific version of Chroma you can checkout the version tag you need:</p> <pre><code>git checkout release/0.4.20\n</code></pre>"},{"location":"running/running-chroma/#docker-compose-without-cloning-the-repo","title":"Docker Compose (Without Cloning the Repo)","text":"<p>If you do not wish or are able to clone the repo locally, Chroma server can also be run with docker compose by creating (or using a gist) a <code>docker-compose.yaml</code></p> <p>Prerequisites:</p> <ul> <li>Docker - Overview of Docker Desktop | Docker Docs</li> <li>cURL (if you want to use the gist approach)</li> </ul> <pre><code>version: '3.9'\n\nnetworks:\n  net:\n    driver: bridge\nservices:\n  chromadb:\n    image: chromadb/chroma:latest\n    volumes:\n      - ./chromadb:/chroma/chroma\n    environment:\n      - IS_PERSISTENT=TRUE\n      - ANONYMIZED_TELEMETRY=${ANONYMIZED_TELEMETRY:-TRUE}\n    ports:\n      - 8000:8000\n    networks:\n      - net\n</code></pre> <p>The above will create a container with the latest Chroma (<code>chromadb/chroma:latest</code>), will expose it to port <code>8000</code> on the local machine and will persist data in <code>./chromadb</code> relative path from where the <code>docker-compose.yaml</code> has been ran.</p> <p>We have also created a small gist with the above file for convenience:</p> <pre><code>curl -s https://gist.githubusercontent.com/tazarov/4fd933274bbacb3b9f286b15c01e904b/raw/87268142d64d8ee0f7f98c27a62a5d089923a1df/docker-compose.yaml | docker-compose -f - up\n</code></pre>"},{"location":"running/running-chroma/#minikube-with-helm-chart","title":"Minikube With Helm Chart","text":"<p>Note: This deployment can just as well be done with <code>KinD</code> depending on your preference. </p> <p>A more advanced approach to running Chroma locally (but also on a remote cluster) is to deploy it using a Helm chart.</p> <p>Disclaimer: The chart used here is not a 1st party chart, but is contributed by a core contributor to Chroma. </p> <p>Prerequisites:</p> <ul> <li>Docker - Overview of Docker Desktop | Docker Docs</li> <li>Install minikube - minikube start | minikube (k8s.io)</li> <li>kubectl - Install Tools | Kubernetes</li> <li>Helm - Helm | Installing Helm</li> </ul> <p>Once you have all of the above running Chroma in a local <code>minikube</code> cluster quite simple</p> <p>Create a <code>minikube</code> cluster:</p> <pre><code>minikube start --addons=ingress -p chroma\nminikube profile chroma\n</code></pre> <p>Get and install the chart:</p> <pre><code>helm repo add chroma https://amikos-tech.github.io/chromadb-chart/\nhelm repo update\nhelm install chroma chroma/chromadb --set chromadb.apiVersion=\"0.4.20\"\n</code></pre> <p>By default the chart will enable authentication in Chroma. To get the token run the following:</p> <pre><code>kubectl --namespace default get secret chromadb-auth -o jsonpath=\"{.data.token}\" | base64 --decode\n# or use this to directly export variable\nexport CHROMA_TOKEN=$(kubectl --namespace default get secret chromadb-auth -o jsonpath=\"{.data.token}\" | base64 --decode)\n</code></pre> <p>The first step to connect and start using Chroma is to forward your port:</p> <pre><code>minikube service chroma-chromadb --url\n</code></pre> <p>The above should print something like this:</p> <pre><code>http://127.0.0.1:61892\n\u2757  Because you are using a Docker driver on darwin, the terminal needs to be open to run it.\n</code></pre> <p>Note: Depending on your OS the message might be slightly different. </p> <p>Test it out (<code>pip install chromadb</code>):</p> <pre><code>import chromadb\nfrom chromadb.config import Settings\n\nclient = chromadb.HttpClient(host=\"http://127.0.0.1:61892\",\n    settings=Settings(chroma_client_auth_provider=\"chromadb.auth.token.TokenAuthClientProvider\",\n                      chroma_client_auth_credentials=\"&lt;your_chroma_token&gt;\"))\nclient.heartbeat()  # this should work with or without authentication - it is a public endpoint\n\nclient.get_version()  # this should work with or without authentication - it is a public endpoint\n\nclient.list_collections()  # this is a protected endpoint and requires authentication\n</code></pre> <p>For more information about the helm chart consult - https://github.com/amikos-tech/chromadb-chart</p>"},{"location":"strategies/backup/","title":"ChromaDB Backups","text":"<p>Depending on your use case there are a few different ways to backup your ChromaDB data.</p> <ul> <li>API export - this approach is relatively simple, slow for large datasets and may result in a backup that is missing   some updates, should your data change frequently.</li> <li>Disk snapshot - this approach is fast, but is highly dependent on the underlying storage. Should your cloud provider   and underlying volume support snapshots, this is a good option.</li> <li>Filesystem backup - this approach is also fast, but requires stopping your Chroma container to avoid data corruption.   This is a good option if you can afford to stop your Chroma container for a few minutes.</li> </ul> <p>Other Options</p> <p>Have another option in mind, feel free to add it to the above list.</p>"},{"location":"strategies/backup/#api-export","title":"API Export","text":"<p>TBD</p>"},{"location":"strategies/backup/#disk-snapshot","title":"Disk Snapshot","text":"<p>TBD</p>"},{"location":"strategies/backup/#filesystem-backup","title":"Filesystem Backup","text":"<p>TBD</p>"},{"location":"strategies/batching/","title":"Batching","text":"<p>It is often that you may need to ingest a large number of documents into Chroma. The problem you may face is related to the underlying SQLite version of the machine running Chroma which imposes a maximum number of statements and parameters which Chroma translates into a batchable record size, exposed via the <code>max_batch_size</code> parameter of the <code>ChromaClient</code> class.</p> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")\nprint(\"Number of documents that can be inserted at once: \",client.max_batch_size)\n</code></pre>"},{"location":"strategies/batching/#creating-batches","title":"Creating Batches","text":"<p>Due to consistency and data integrity reasons, Chroma does not offer, yet, out-of-the-box batching support. The below code snippet shows how to create batches of documents and ingest them into Chroma.</p> <pre><code>import chromadb\nfrom chromadb.utils.batch_utils import create_batches\nimport uuid\n\nclient = chromadb.PersistentClient(path=\"test-large-batch\")\nlarge_batch = [(f\"{uuid.uuid4()}\", f\"document {i}\", [0.1] * 1536) for i in range(100000)]\nids, documents, embeddings = zip(*large_batch)\nbatches = create_batches(api=client,ids=list(ids), documents=list(documents), embeddings=list(embeddings))\ncollection = client.get_or_create_collection(\"test\")\nfor batch in batches:\n    print(f\"Adding batch of size {len(batch[0])}\")\n    collection.add(ids=batch[0],\n                   documents=batch[3],\n                   embeddings=batch[1],\n                   metadatas=batch[2])\n</code></pre>"},{"location":"strategies/keyword-search/","title":"Keyword Search","text":"<p>Chroma uses SQLite for storing metadata and documents. Additionally documents are indexed using SQLite FTS5 for fast text search.</p> <pre><code>import chromadb\nfrom chromadb.config import Settings\n\nclient = chromadb.PersistentClient(path=\"test\", settings=Settings(allow_reset=True))\n\nclient.reset()\ncol = client.get_or_create_collection(\"test\")\n\ncol.upsert(ids=[\"1\", \"2\", \"3\"], documents=[\"He is a technology freak and he loves AI topics\", \"AI technology are advancing at a fast pace\", \"Innovation in LLMs is a hot topic\"],metadatas=[{\"author\": \"John Doe\"}, {\"author\": \"Jane Doe\"}, {\"author\": \"John Doe\"}])\ncol.query(query_texts=[\"technology\"], where_document={\"$or\":[{\"$contains\":\"technology\"}, {\"$contains\":\"freak\"}]})\n</code></pre> <p>The above should return:</p> <pre><code>{'ids': [['2', '1']],\n 'distances': [[1.052205477809135, 1.3074231535113972]],\n 'metadatas': [[{'author': 'Jane Doe'}, {'author': 'John Doe'}]],\n 'embeddings': None,\n 'documents': [['AI technology are advancing at a fast pace',\n   'He is a technology freak and he loves AI topics']],\n 'uris': None,\n 'data': None}\n</code></pre>"},{"location":"strategies/memory-management/","title":"Memory Management","text":""},{"location":"strategies/memory-management/#persistentclient","title":"PersistentClient","text":"<p>Note: The below code snippets assume you are working with a <code>PersistentClient</code></p> <p>At the time of writing (Chroma v0.4.22), Chroma does not allow you to manually unloading of collections from memory.</p> <p>Here we provide a simple utility function to help users unload collections from memory.</p> <p>Note: The function relies on Chroma internal APIs which may change. While we try to keep this documentation up-to-date, there may be versions of Chroma for which the below code won't work.</p> <pre><code>import chromadb\nfrom chromadb.segment import VectorReader\nfrom chromadb.types import SegmentScope\n\n\ndef unload_index(collection_name:str, chroma_client:chromadb.PersistentClient):\n    \"\"\"\n    Unloads binary hnsw index from memory and removes both segments (binary and metadata) from the segment cache.\n    \"\"\"\n    collection = chroma_client.get_collection(collection_name)\n    segment_manager = chroma_client._server._manager\n    segment = segment_manager.get_segment(collection.id, VectorReader)\n    segment.close_persistent_index()\n    if collection.id in segment_manager._segment_cache:\n        for scope in [SegmentScope.VECTOR, SegmentScope.METADATA]:\n            if scope in segment_manager._segment_cache[collection.id]:\n                del segment_manager._segment_cache[collection.id][segment[\"scope\"]]\n        del segment_manager._segment_cache[collection.id]\n</code></pre>"},{"location":"strategies/multi-tenancy/","title":"Multi-tenancy Strategies","text":"<p>Single-note Chroma</p> <p>The below strategies are applicable to single-node Chroma only.</p>"},{"location":"strategies/multi-tenancy/#introduction","title":"Introduction","text":"<p>There are several multi-tenancy strategies available to users of Chroma. The actual strategy will depend on the needs of the user and the application. The strategies below apply to multi-user environments, but do no factor in partly-shared resources like groups or teams.</p> <ul> <li>Doc-Per-User: In this scenario, the app maintains multiple collections and each collection document is associated   with a single user.</li> <li>Doc-Per-Collection: In this scenario, the app maintains multiple collections and each collection is   associated with a single user.</li> <li>Doc-Per-Database: In this scenario, the app maintains multiple databases with a single tenant and each database is   associated with a single user.</li> <li>Doc-Per-Tenant: In this scenario, the app maintains multiple tenants and each tenant is associated with a single   user.</li> </ul>"},{"location":"strategies/multi-tenancy/#doc-per-user","title":"Doc-Per-User","text":"<p>To implement this strategy you need to add some sort of user identification to each document that belongs to a user. For this example we will assume it is <code>user_id</code>.</p> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient()\ncollection = client.get_or_create_collection(\"my-collection\")\ncollection.add(\n    documents=[\"This is document1\", \"This is document2\"],\n    metadatas=[{\"user_id\": \"user1\"}, {\"user_id\": \"user2\"}],\n    ids=[\"doc1\", \"doc2\"],\n)\n</code></pre> <p>At query time you will have to provide the <code>user_id</code> as a filter to your query like so:</p> <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    where=[{\"user_id\": \"user1\"}],\n)\n</code></pre> <p>To successfully implement this strategy your code needs to consistently add and filter on the <code>user_id</code> metadata to ensure separation of data.</p> <p>Drawbacks:</p> <ul> <li>Error-prone: Messing up the filtering can lead to data being leaked across users.</li> <li>Scalability: As the number of users and documents grow, doing filtering on metadata can become slow.</li> </ul>"},{"location":"strategies/multi-tenancy/#doc-per-collection","title":"Doc-Per-Collection","text":"<p>To implement this strategy you need to create a collection for each user. For this example we will assume it is <code>user_id</code>.</p> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient()\nuser_id = \"user1\"\ncollection = client.get_or_create_collection(f\"user-collection:{user_id}\")\ncollection.add(\n    documents=[\"This is document1\", \"This is document2\"],\n    ids=[\"doc1\", \"doc2\"],\n)\n</code></pre> <p>At query time you will have to provide the <code>user_id</code> as a filter to your query like so:</p> <pre><code>user_id = \"user1\"\nuser_collection = client.get_collection(f\"user-collection:{user_id}\")\nresults = user_collection.query(\n    query_texts=[\"This is a query document\"],\n)\n</code></pre> <p>To successfully implement this strategy your code needs to consistently create and query the correct collection for the user.</p> <p>Drawbacks:</p> <ul> <li>Error-prone: Messing up the collection name can lead to data being leaked across users.</li> <li>Shared document search: If you want to maintain some documents shared then you will have to create a separate   collection for those documents and allow users to query the shared collection as well.</li> </ul>"},{"location":"strategies/multi-tenancy/#doc-per-database","title":"Doc-Per-Database","text":"<p>TBD</p>"},{"location":"strategies/multi-tenancy/#doc-per-tenant","title":"Doc-Per-Tenant","text":"<p>TBD</p>"},{"location":"strategies/privacy/","title":"Privacy Strategies","text":""},{"location":"strategies/privacy/#overview","title":"Overview","text":"<p>TBD</p>"},{"location":"strategies/privacy/#encryption","title":"Encryption","text":""},{"location":"strategies/privacy/#document-encryption","title":"Document Encryption","text":""},{"location":"strategies/privacy/#client-side-document-encryption","title":"Client-side Document Encryption","text":"<p>See the notebook on client-side document encryption.</p>"},{"location":"strategies/time-based-queries/","title":"Time-based Queries","text":""},{"location":"strategies/time-based-queries/#filtering-documents-by-timestamps","title":"Filtering Documents By Timestamps","text":"<p>In the example below, we create a collection with 100 documents, each with a random timestamp in the last two weeks. We then query the collection for documents that were created in the last week.</p> <p>The example demonstrates how Chroma metadata can be leveraged to filter documents based on how recently they were added or updated.</p> <pre><code>import uuid\nimport chromadb\n\nimport datetime\nimport random\n\nnow = datetime.datetime.now()\ntwo_weeks_ago = now - datetime.timedelta(days=14)\n\ndates = [\n    two_weeks_ago + datetime.timedelta(days=random.randint(0, 14))\n    for _ in range(100)\n]\ndates = [int(date.timestamp()) for date in dates]\n\n# convert epoch seconds to iso format\n\ndef iso_date(epoch_seconds): return datetime.datetime.fromtimestamp(\n    epoch_seconds).isoformat()\n\nclient = chromadb.EphemeralClient()\n\ncol = client.get_or_create_collection(\"test\")\n\ncol.add(ids=[f\"{uuid.uuid4()}\" for _ in range(100)], documents=[\n    f\"document {i}\" for i in range(100)], metadatas=[{\"date\": date} for date in dates])\n\nres = col.get(where={\"date\": {\"$gt\": (now - datetime.timedelta(days=7)).timestamp()}})\n\nfor i in res['metadatas']:\n    print(iso_date(i['date']))\n</code></pre> <p>Ref: https://gist.github.com/tazarov/3c9301d22ab863dca0b6fb1e5e3511b1</p>"}]}