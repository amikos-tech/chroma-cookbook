{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Get Cooking","text":"<p>This is a collection of small guides and recipes to help you get started with Chroma.</p> <p>Latest ChromaDB version: 1.5.1</p> API Changelog (1.5.1 and 1.5.0) <p>Version 1.5.1 (February 19, 2026)</p> Area API-facing change Reference Advanced Search Removed beta label from Advanced Search API #6396 Collections Reject <code>fork_collection</code> for multi-region databases #6400 Schema / FTS Added option to disable FTS in schema #6214 <p>Version 1.5.0 (February 9, 2026)</p> Area API-facing change Reference Search Exported search options parameter #6160 Collections Rust sysdb impl for <code>get collections</code> #6146 Collections Rust sysdb impl for <code>get collection with segments</code> #6147 Collections Rust sysdb impl for <code>update collection</code> #6163 Schema Added option to enable quantization in schema #6295"},{"location":"#new-and-noteworthy","title":"New and Noteworthy","text":"<ul> <li>\ud83e\udde9 Deployment Patterns - Added two practical deployment walkthroughs: embed Chroma directly in a Python app, or run it as a standalone server and connect with <code>HttpClient</code> - \ud83d\udcc5<code>24-Feb-2026</code></li> <li>\ud83d\udcca Resource Requirements - Added an interactive sizing calculator, clearer RAM formulas, and explicit disk caveats for large documents and FTS index overhead - \ud83d\udcc5<code>21-Feb-2026</code></li> <li>\ud83d\ude80 Running Chroma - Refreshed CLI/Docker/Compose/Minikube guidance, aligned Helm chart notes, and added collapsed optional YAML config examples - \ud83d\udcc5<code>20-Feb-2026</code></li> <li>\ud83e\udded Core Concepts - Reworked into General vs Power Users tracks, with interactive local/distributed execution diagrams and data-flow visuals - \ud83d\udcc5<code>19-Feb-2026</code></li> <li>\ud83c\udfaf Collections Query IDs - Documented <code>query(..., ids=...)</code> for restricting similarity search to specific records - \ud83d\udcc5<code>17-Feb-2026</code></li> <li>\ud83d\udd0d Filters - Added multi-language filter examples and <code>$regex</code>/<code>$not_regex</code> operators - \ud83d\udcc5<code>17-Feb-2026</code></li> <li>\ud83d\udd27 Installation - Updated package names and added Go/Rust install examples - \ud83d\udcc5<code>17-Feb-2026</code></li> <li>\u2692\ufe0f Configuration - Added 1.0 docs for HNSW, SPANN index, and embedding functions - \ud83d\udcc5<code>17-Feb-2026</code></li> <li>\ud83d\udce6 Clients - Added Cloud Client section and updated client examples - \ud83d\udcc5<code>17-Feb-2026</code></li> <li>\ud83d\udcda Collections - Updated to current APIs with multi-language examples - \ud83d\udcc5<code>17-Feb-2026</code></li> <li>\ud83c\udff7\ufe0f Array Metadata Filters - Chroma 1.5.0 adds support for array metadata with <code>$contains</code>/<code>$not_contains</code> operators - \ud83d\udcc5<code>17-Feb-2026</code></li> <li>\ud83d\udd11 Authentication in Chroma v1.0.x - Chroma 1.0.x does not support native Authentication, in this article we cover how to secure your Chroma 1.0.x instance - \ud83d\udcc5<code>28-May-2025</code></li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>We suggest you first head to the Concepts section. It now has two tracks:</p> <ul> <li>For General Users - tenancy, collections, metadata, embeddings, and cloud data-flow basics</li> <li>For Power Users - local SQLite + HNSW path, distributed frontend dispatch path, and core internals</li> </ul> <p>Once you're comfortable with the concepts, you can jump to the Installation section to install ChromaDB.</p> <p>Core Topics:</p> <ul> <li>Filters - Learn to filter data in ChromaDB using metadata and document filters</li> <li>Resource Requirements - Understand the resource requirements for running ChromaDB</li> <li>\u2728Multi-Tenancy - Learn how to implement multi-tenancy in ChromaDB</li> </ul>"},{"location":"#running-chromadb","title":"Running ChromaDB","text":"<ul> <li>Deployment Patterns - Decide between embedded (<code>PersistentClient</code>) and standalone server (<code>HttpClient</code>) with copy/paste examples</li> <li>CLI - Running ChromaDB via the CLI</li> <li>Docker - Running ChromaDB in Docker</li> <li>Docker Compose - Running ChromaDB in Docker Compose</li> <li>Kubernetes - Running ChromaDB in Kubernetes (Minikube)</li> </ul>"},{"location":"#integrations","title":"Integrations","text":"<ul> <li>\u2728LangChain - Integrating ChromaDB with LangChain</li> <li>\u2728LlamaIndex - Integrating ChromaDB with LlamaIndex</li> <li>\u2728Ollama - Integrating ChromaDB with Ollama</li> </ul>"},{"location":"#the-ecosystem","title":"The Ecosystem","text":""},{"location":"#clients","title":"Clients","text":"<p>Below is a list of available clients for ChromaDB.</p> <ul> <li>Python Client (Official Chroma client)</li> <li>JavaScript Client (Official Chroma client)</li> <li>Ruby Client (Community maintained)</li> <li>Java Client (Community maintained)</li> <li>Go Client (Community maintained)</li> <li>C# Client (Microsoft maintained)</li> <li>Rust Client (Community maintained)</li> <li>Elixir Client (Community maintained)</li> <li>Dart Client (Community maintained)</li> <li>PHP Client (Community maintained)</li> <li>PHP (Laravel) Client (Community maintained)</li> </ul>"},{"location":"#user-interfaces","title":"User Interfaces","text":"<ul> <li>VectorAdmin (MintPlex Labs) - An open-source web-based admin   interface for vector databases, including ChromaDB</li> <li>ChromaDB UI (Community maintained) - A web-based UI for ChromaDB</li> <li>phpMyChroma (Community maintained) - A tiny PHP 8+ web client that allows you to browse Chroma and perform semantic search</li> </ul>"},{"location":"#cli-tooling","title":"CLI Tooling","text":"<ul> <li>Chroma CLI (Community maintained) - Early Alpha</li> <li>Chroma Data Pipes (Community maintained) - A CLI tool for   importing and exporting data from ChromaDB</li> <li>Chroma Ops (Community maintained) - A maintenance CLI tool for ChromaDB</li> </ul>"},{"location":"#strategies","title":"Strategies","text":"<ul> <li>Backup - Backing up ChromaDB data</li> <li>Batch Imports - Importing data in batches</li> <li>Multi-Tenancy - Running multiple ChromaDB instances</li> <li>Keyword Search - Searching for keywords in ChromaDB</li> <li>Memory Management - Managing memory in ChromaDB</li> <li>Time-based Queries - Querying data based on timestamps</li> <li>\u2728 <code>Coming Soon</code> Testing with Chroma - learn how to test your GenAI apps that include Chroma.</li> <li>\u2728 <code>Coming Soon</code> Monitoring Chroma - learn how to monitor your Chroma instance.</li> <li>\u2728 <code>Coming Soon</code> Building Chroma clients - learn how to build clients for Chroma.</li> <li>\u2728 <code>Coming Soon</code> Creating the perfect Embedding Function (wrapper) - learn the best practices for creating your own   embedding function.</li> <li>\u2728 Multi-User Basic Auth Plugin - learn how to build a multi-user   basic authentication plugin for Chroma.</li> <li>\u2728 CORS Configuration For JS Browser apps - learn how to configure CORS for Chroma.</li> <li>\u2728 Running Chroma with SystemD - learn how to start Chroma upon system boot.</li> </ul>"},{"location":"#get-help","title":"Get Help","text":"<p>Missing something? Let us know by opening an issue, reach out on Discord (look for <code>@taz</code>), or message us on Twitter.</p>"},{"location":"contributing/getting-started/","title":"Getting Started with Contributing to Chroma","text":""},{"location":"contributing/getting-started/#overview","title":"Overview","text":"<p>Here are some steps to follow:</p> <ul> <li>Fork the repository (if you are part of an organization to which you cannot grant permissions it might be advisable to fork under your own user account to allow other community members to contribute by granting them permissions, something that is a bit more difficult at organizational level)</li> <li>Clone your forked repo locally (git clone ...) under a dir with an apt name for the change you want to make e.g. <code>my_awesome_feature</code></li> <li>Create a branch for your change (git checkout -b my_awesome_feature)</li> <li>Make your changes</li> <li>Test (see Testing)</li> <li>Lint (see Linting)</li> <li>Commit your changes (git commit -am 'Added some feature')</li> <li>Push to the branch (git push origin my_awesome_feature)</li> <li>Create a new Pull Request (PR) from your forked repository to the main Chroma repository</li> </ul>"},{"location":"contributing/getting-started/#testing","title":"Testing","text":"<p>It is generally good to test your changes before submitting a PR.</p> <p>To run the full test suite:</p> <pre><code>pip install -r requirements_dev.txt\npytest\n</code></pre> <p>To run a specific test:</p> <pre><code>pytest chromadb/tests/test_api.py::test_get_collection\n</code></pre> <p>If you want to see the output of print statements in the tests, you can run:</p> <pre><code>pytest -s\n</code></pre> <p>If you want your pytest to stop on first failure, you can run:</p> <pre><code>pytest -x\n</code></pre>"},{"location":"contributing/getting-started/#integration-tests","title":"Integration Tests","text":"<p>You can only run the integration tests by running:</p> <pre><code>sh bin/bin/integration-test\n</code></pre> <p>The above will create a docker container and will run the integration tests against it. This will also include JS client.</p>"},{"location":"contributing/getting-started/#linting","title":"Linting","text":""},{"location":"contributing/useful-shortcuts/","title":"Useful Shortcuts for Contributors","text":""},{"location":"contributing/useful-shortcuts/#git","title":"Git","text":""},{"location":"contributing/useful-shortcuts/#aliases","title":"Aliases","text":""},{"location":"contributing/useful-shortcuts/#create-venv-and-install-dependencies","title":"Create venv and install dependencies","text":"<p>Add the following to your <code>.bashrc</code>, <code>.zshrc</code> or <code>.profile</code>:</p> <pre><code>alias chroma-init='python -m virtualenv venv &amp;&amp; source venv/bin/activate &amp;&amp; pip install -r requirements.txt &amp;&amp; pip install -r requirements_dev.txt'\n</code></pre>"},{"location":"core/","title":"Chroma Core: Concepts and APIs","text":"<p>This section is the fastest way to understand Chroma's core data model, client choices, and day-to-day APIs.</p> <p>If you are new to Chroma, use the path below in order. If you are already building, jump to the API map.</p>"},{"location":"core/#recommended-learning-path","title":"Recommended Learning Path","text":"<ol> <li>Concepts: Understand tenants, databases, collections, documents, metadata, embeddings, and query flow.</li> <li>Installation: Install Chroma for local development, server usage, or cloud-connected workflows.</li> <li>Clients: Pick the right client (<code>PersistentClient</code>, <code>HttpClient</code>, <code>AsyncHttpClient</code>, <code>CloudClient</code>) for your architecture.</li> <li>Collections: Learn collection lifecycle and core CRUD/query operations.</li> <li>Filters: Add precise metadata and document filtering to retrieval.</li> <li>Configuration: Tune index, runtime, and environment options.</li> <li>Resources: Estimate memory/CPU/disk needs before scaling.</li> <li>Advanced Search: Learn query-stage semantics, ranking, and execution tradeoffs.</li> </ol>"},{"location":"core/#api-map","title":"API Map","text":""},{"location":"core/#client-level-apis","title":"Client-Level APIs","text":"<ul> <li>Connect to Chroma: <code>PersistentClient</code>, <code>HttpClient</code>, <code>AsyncHttpClient</code>, <code>CloudClient</code></li> <li>Scope data isolation: tenant + database selection</li> <li>Perform admin/list operations (for example collection listing and lifecycle actions)</li> </ul> <p>Start in Clients, then use Tenants and Databases for multi-tenant setups.</p>"},{"location":"core/#collection-level-apis","title":"Collection-Level APIs","text":"<ul> <li>Create/get collections: <code>create_collection</code>, <code>get_or_create_collection</code>, <code>get_collection</code></li> <li>Write data: <code>add</code>, <code>upsert</code>, <code>update</code>, <code>delete</code></li> <li>Read data: <code>get</code>, <code>query</code>, <code>count</code></li> <li>Manage collection settings: metadata, index configuration, and cloning/forking patterns</li> </ul> <p>Start in Collections, then pair with Filters and Document IDs.</p>"},{"location":"core/#httpopenapi-surface","title":"HTTP/OpenAPI Surface","text":"<ul> <li>Explore server endpoints and OpenAPI: API</li> <li>Generate custom clients when needed</li> </ul>"},{"location":"core/#minimal-end-to-end-flow-python","title":"Minimal End-to-End Flow (Python)","text":"<pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"./chroma\")\ncollection = client.get_or_create_collection(\"quickstart\")\n\ncollection.upsert(\n    ids=[\"doc-1\", \"doc-2\"],\n    documents=[\"Chroma stores vectors and metadata.\", \"Filters narrow candidate sets before ranking.\"],\n    metadatas=[{\"topic\": \"basics\"}, {\"topic\": \"search\"}],\n)\n\nresult = collection.query(\n    query_texts=[\"How do filters affect retrieval?\"],\n    where={\"topic\": \"search\"},\n    n_results=2,\n)\n\nprint(result[\"ids\"])\n</code></pre>"},{"location":"core/#where-to-go-next","title":"Where to Go Next","text":"<ul> <li>Operating local persistent deployments: Storage Layout, WAL, WAL Pruning</li> <li>Production guardrails and limits: System Constraints, Resources</li> </ul>"},{"location":"core/api/","title":"Chroma API","text":"<p>In this article we will cover the Chroma API in an indepth details.</p>"},{"location":"core/api/#accessing-the-api","title":"Accessing the API","text":"<p>If you are running a Chroma server you can access its API at - <code>http://&lt;chroma_server_host&gt;:&lt;chroma_server_port&gt;/docs</code> ( e.g. <code>http://localhost:8000/docs</code>).</p> <p>Alternatively you can take a peek at the latest API from Chroma Cloud - https://api.trychroma.com:8000/docs</p>"},{"location":"core/api/#api-endpoints","title":"API Endpoints","text":"<p>TBD</p>"},{"location":"core/api/#generating-clients","title":"Generating Clients","text":"<p>While Chroma ecosystem has client implementations for many languages, it may be the case you want to roll out your own. Below we explain some of the options available to you:</p>"},{"location":"core/api/#using-openapi-generator","title":"Using OpenAPI Generator","text":"<p>The fastest way to build a client is to use the OpenAPI Generator with the API spec. Chroma provides an OpenAPI specification that can be used to generate clients in various programming languages.</p>"},{"location":"core/api/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install the OpenAPI Generator CLI:    <pre><code># Using npm\nnpm install @openapitools/openapi-generator-cli -g\n\n# Using Docker\ndocker pull openapitools/openapi-generator-cli\n</code></pre></p> </li> <li> <p>Get the OpenAPI specification:</p> </li> <li>From a running Chroma server: <code>http://&lt;chroma_server_host&gt;:&lt;chroma_server_port&gt;/openapi.json</code></li> <li>From Chroma Cloud: <code>https://api.trychroma.com:8000/openapi.json</code></li> </ol>"},{"location":"core/api/#generating-clients_1","title":"Generating Clients","text":"<p>Here are examples for generating clients in different languages:</p> <p>Python Client: <pre><code># Using npm CLI\nopenapi-generator-cli generate \\\n  -i https://api.trychroma.com:8000/openapi.json \\\n  -g python \\\n  -o ./chroma-python-client \\\n  --additional-properties=packageName=chroma_client,packageVersion=1.0.0\n\n# Using Docker\ndocker run --rm \\\n  -v ${PWD}:/local openapitools/openapi-generator-cli generate \\\n  -i https://api.trychroma.com:8000/openapi.json \\\n  -g python \\\n  -o /local/chroma-python-client \\\n  --additional-properties=packageName=chroma_client,packageVersion=1.0.0\n</code></pre></p> <p>TypeScript/JavaScript Client: <pre><code># Using npm CLI\nopenapi-generator-cli generate \\\n  -i https://api.trychroma.com:8000/openapi.json \\\n  -g typescript-fetch \\\n  -o ./chroma-typescript-client \\\n  --additional-properties=npmName=@chroma/client,npmVersion=1.0.0\n\n# Using Docker\ndocker run --rm \\\n  -v ${PWD}:/local openapitools/openapi-generator-cli generate \\\n  -i http://localhost:8000/openapi.json \\\n  -g typescript-fetch \\\n  -o /local/chroma-typescript-client \\\n  --additional-properties=npmName=@chroma/client,npmVersion=1.0.0\n</code></pre></p> <p>Java Client: <pre><code># Using npm CLI\nopenapi-generator-cli generate \\\n  -i http://localhost:8000/openapi.json \\\n  -g java \\\n  -o ./chroma-java-client \\\n  --additional-properties=groupId=com.chroma,artifactId=chroma-client,artifactVersion=1.0.0\n\n# Using Docker\ndocker run --rm \\\n  -v ${PWD}:/local openapitools/openapi-generator-cli generate \\\n  -i http://localhost:8000/openapi.json \\\n  -g java \\\n  -o /local/chroma-java-client \\\n  --additional-properties=groupId=com.chroma,artifactId=chroma-client,artifactVersion=1.0.0\n</code></pre></p> <p>Go Client: <pre><code># Using npm CLI\nopenapi-generator-cli generate \\\n  -i http://localhost:8000/openapi.json \\\n  -g go \\\n  -o ./chroma-go-client \\\n  --additional-properties=packageName=chroma,packageVersion=1.0.0\n\n# Using Docker\ndocker run --rm \\\n  -v ${PWD}:/local openapitools/openapi-generator-cli generate \\\n  -i http://localhost:8000/openapi.json \\\n  -g go \\\n  -o /local/chroma-go-client \\\n  --additional-properties=packageName=chroma,packageVersion=1.0.0\n</code></pre></p>"},{"location":"core/api/#using-the-generated-client","title":"Using the Generated Client","text":"<p>After generating the client, you can use it in your code. Here's an example with the Python client:</p> <pre><code># Install the generated client\ncd chroma-python-client\npip install -e .\n\n# Use the client\nfrom chroma_client import ApiClient, DefaultApi\n\n# Create API client\nclient = ApiClient(host=\"http://localhost:8000\")\napi = DefaultApi(client)\n\n# List collections\ncollections = api.list_collections()\nprint(f\"Found {len(collections)} collections\")\n</code></pre>"},{"location":"core/api/#available-generators","title":"Available Generators","text":"<p>The OpenAPI Generator supports many languages and frameworks. Some popular options include: - <code>python</code> - Python client - <code>typescript-fetch</code> - TypeScript client using fetch - <code>typescript-axios</code> - TypeScript client using axios - <code>java</code> - Java client - <code>go</code> - Go client - <code>csharp</code> - C# client - <code>php</code> - PHP client - <code>ruby</code> - Ruby client - <code>rust</code> - Rust client</p> <p>For a complete list of available generators, run: <pre><code>openapi-generator-cli list\n</code></pre></p>"},{"location":"core/api/#manually-creating-a-client","title":"Manually Creating a Client","text":"<p>If you more control over things, you can create your own client by using the API spec as guideline.</p>"},{"location":"core/api/#python","title":"Python","text":""},{"location":"core/api/#typescript","title":"Typescript","text":""},{"location":"core/api/#golang","title":"Golang","text":""},{"location":"core/api/#java","title":"Java","text":""},{"location":"core/api/#rust","title":"Rust","text":""},{"location":"core/api/#elixir","title":"Elixir","text":""},{"location":"core/clients/","title":"Chroma Clients","text":"<p>Chroma Settings Object</p> <p>The below is only a partial list of Chroma configuration options. For full list check the code <code>chromadb.config.Settings</code> or the ChromaDB Configuration page.</p>"},{"location":"core/clients/#client-implementations-and-source-repos","title":"Client implementations and source repos","text":"Language Constructors covered on this page Source repository Python <code>PersistentClient</code>, <code>HttpClient</code>, <code>AsyncHttpClient</code>, <code>CloudClient</code>, <code>EphemeralClient</code>, <code>Client</code> <code>chroma-core/chroma</code> TypeScript <code>ChromaClient</code>, <code>CloudClient</code> <code>chroma-core/chroma</code> (JS client) Go <code>NewHTTPClient</code>, <code>NewCloudClient</code> <code>amikos-tech/chroma-go</code> Rust <code>ChromaHttpClient</code> <code>chroma-core/chroma</code> (Rust crate)"},{"location":"core/clients/#persistent-client","title":"Persistent Client","text":"<p>To create a local persistent client, use the <code>PersistentClient</code> class. This client stores data locally in a directory on your machine at the path you specify.</p> <p>Authentication</p> <p>For authentication details see the Chroma-native Authentication section.</p> <pre><code>import chromadb\nfrom chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n\nclient = chromadb.PersistentClient(\n    path=\"test\",\n    settings=Settings(),\n    tenant=DEFAULT_TENANT,\n    database=DEFAULT_DATABASE,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Description Default / Allowed values <code>path</code> <code>str \\| Path</code> Local path on the machine where Chroma runs. Created if it does not exist. Can be relative or absolute. <code>./chroma</code> <code>settings</code> <code>Settings \\| None</code> Chroma settings object. <code>None</code> (uses <code>Settings()</code>) <code>tenant</code> <code>str</code> Tenant to use. <code>default_tenant</code> <code>database</code> <code>str</code> Database to use. <code>default_database</code> <p>Positional Parameters</p> <p>Chroma <code>PersistentClient</code> parameters are positional, unless keyword arguments are used.</p>"},{"location":"core/clients/#uses-of-persistent-client","title":"Uses of Persistent Client","text":"<p>The persistent client is useful for:</p> <ul> <li>Local development: You can use the persistent client to develop locally and test out ChromaDB.</li> <li>Embedded applications: You can use the persistent client to embed ChromaDB in your application. This means that   you can ship Chroma bundled with your product or services, thus simplifying the deployment process.</li> <li>Simplicity: If you do not wish to incur the complexities associated with setting up and operating a Chroma   server (arguably Hosted-Chroma will resolve this).</li> <li>Data privacy: If you are working with sensitive data and do not want to store it on a remote server.</li> <li>Optimize performance: If you want to reduce latency.</li> </ul> <p>The right tool for the job</p> <p>When evaluating the use of local <code>PersistentClient</code> one should always factor in the scale of the application.  Similar to SQLite vs Postgres/MySQL, <code>PersistentClient</code> vs <code>HTTPClient</code> with Chroma server, application architectural characteristics (such as complexity, scale, performance etc) should be considered when deciding to use one or the other.</p>"},{"location":"core/clients/#http-client","title":"HTTP Client","text":"<p>Chroma also provides HTTP Client, suitable for use in a client-server mode. This client can be used to connect to a remote ChromaDB server. The HTTP client can operate in synchronous or asynchronous mode (see examples below).</p> <p>Authentication</p> <p>For authentication details see the Chroma-native Authentication section.</p> Python SyncPython AsyncTypeScriptGoRust <pre><code>import chromadb\nfrom chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n\nclient = chromadb.HttpClient(\n    host=\"localhost\",\n    port=8000,\n    ssl=False,\n    headers=None,\n    settings=Settings(),\n    tenant=DEFAULT_TENANT,\n    database=DEFAULT_DATABASE,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Description Default / Allowed values <code>host</code> <code>str</code> Hostname of the remote server. You can also pass a full URL (including a path prefix). <code>localhost</code> <code>port</code> <code>int</code> Port of the remote server. <code>8000</code> <code>ssl</code> <code>bool</code> Uses HTTPS when <code>True</code>. <code>False</code> <code>headers</code> <code>dict[str, str] \\| None</code> Additional headers sent with each request (for example auth headers). <code>None</code> <code>settings</code> <code>Settings \\| None</code> Chroma settings object. <code>None</code> (uses <code>Settings()</code>) <code>tenant</code> <code>str</code> Tenant to use. <code>default_tenant</code> <code>database</code> <code>str</code> Database to use. <code>default_database</code> <p>Positional Parameters</p> <p>Chroma <code>HttpClient</code> parameters are positional, unless keyword arguments are used.</p> <pre><code>import asyncio\nimport chromadb\nfrom chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n# Apply nest_asyncio to allow running nested event loops in jupyter notebook\n# import nest_asyncio # import this if running in jupyter notebook\n# nest_asyncio.apply() # apply this if running in jupyter notebook\n\nasync def list_collections():\n    client = await chromadb.AsyncHttpClient(\n        host=\"localhost\",\n        port=8000,\n        ssl=False,\n        headers=None,\n        settings=Settings(),\n        tenant=DEFAULT_TENANT,\n        database=DEFAULT_DATABASE,\n    )\n    return await client.list_collections()\n\nresult = asyncio.run(list_collections())\nprint(result)\n</code></pre> <p>Parameters:</p> Parameter Type Description Default / Allowed values <code>host</code> <code>str</code> Hostname of the remote server. You can also pass a full URL (including a path prefix). <code>localhost</code> <code>port</code> <code>int</code> Port of the remote server. <code>8000</code> <code>ssl</code> <code>bool</code> Uses HTTPS when <code>True</code>. <code>False</code> <code>headers</code> <code>dict[str, str] \\| None</code> Additional headers sent with each request (for example auth headers). <code>None</code> <code>settings</code> <code>Settings \\| None</code> Chroma settings object. <code>None</code> (uses <code>Settings()</code>) <code>tenant</code> <code>str</code> Tenant to use. <code>default_tenant</code> <code>database</code> <code>str</code> Database to use. <code>default_database</code> <p>Positional Parameters</p> <p>Chroma <code>AsyncHttpClient</code> parameters are positional, unless keyword arguments are used.</p> <pre><code>import { ChromaClient } from \"chromadb\";\n\nconst client = new ChromaClient({\n    path: \"http://localhost:8000\",\n    auth: {\n        provider: \"token\",\n        credentials: \"your_token_here\",\n        tokenHeaderType: \"X_CHROMA_TOKEN\",\n    },\n    tenant: \"default_tenant\",\n    database: \"default_database\",\n});\n</code></pre> <p>Parameters:</p> Parameter Type Description Default / Allowed values <code>path</code> <code>string</code> Base URL for the Chroma API. <code>http://localhost:8000</code> <code>auth</code> <code>AuthOptions</code> Authentication config. Optional. <code>provider</code> values: <code>\"basic\"</code> or <code>\"token\"</code> <code>fetchOptions</code> <code>RequestInit</code> Fetch options passed to HTTP calls (for example custom headers). Optional <code>tenant</code> <code>string</code> Tenant to use. <code>default_tenant</code> <code>database</code> <code>string</code> Database to use. <code>default_database</code> <p><pre><code>go get github.com/amikos-tech/chroma-go@latest\n</code></pre> <pre><code>package main\n\nimport (\n    \"context\"\n    \"log\"\n\n    chroma \"github.com/amikos-tech/chroma-go/pkg/api/v2\"\n)\n\nfunc main() {\n    client, err := chroma.NewHTTPClient(\n        chroma.WithBaseURL(\"http://localhost:8000\"),\n        chroma.WithDefaultDatabaseAndTenant(),\n    )\n    if err != nil {\n        log.Fatalf(\"Error creating client: %s \\n\", err)\n    }\n    if err := client.Heartbeat(context.TODO()); err != nil {\n        log.Fatalf(\"Error connecting: %s \\n\", err)\n    }\n}\n</code></pre></p> <p>Parameters:</p> Option Type Description Default / Allowed values <code>WithBaseURL()</code> <code>func(string) ClientOption</code> Sets Chroma endpoint URL. <code>/api/v2</code> is appended if missing. Default base URL is <code>http://localhost:8000/api/v2</code> <code>WithAuth()</code> <code>func(CredentialsProvider) ClientOption</code> Sets auth provider (see Go auth docs). Optional <code>WithDatabaseAndTenant()</code> <code>func(database string, tenant string) ClientOption</code> Sets database and tenant explicitly. Optional <code>WithDatabaseAndTenantFromEnv()</code> <code>func() ClientOption</code> Reads <code>CHROMA_DATABASE</code> and <code>CHROMA_TENANT</code> when present. Applied by default in <code>NewHTTPClient</code> <code>WithDefaultDatabaseAndTenant()</code> <code>func() ClientOption</code> Fills missing values with defaults. <code>default_database</code> and <code>default_tenant</code> <code>WithTimeout()</code> <code>func(time.Duration) ClientOption</code> Sets request timeout. Optional <p><pre><code>cargo add chroma\n</code></pre> <pre><code>use chroma::{ChromaHttpClient, ChromaHttpClientOptions};\n\n#[tokio::main]\nasync fn main() -&gt; anyhow::Result&lt;()&gt; {\n    let client = ChromaHttpClient::new(ChromaHttpClientOptions::default());\n    let heartbeat = client.heartbeat().await?;\n    println!(\"Server timestamp: {}\", heartbeat);\n    Ok(())\n}\n</code></pre></p> <p>Parameters (<code>ChromaHttpClientOptions</code>):</p> Parameter Type Description Default / Allowed values <code>endpoint</code> <code>reqwest::Url</code> Server base URL. <code>http://localhost:8000</code> <code>auth_method</code> <code>ChromaAuthMethod</code> Authentication strategy. <code>ChromaAuthMethod::None</code> <code>retry_options</code> <code>ChromaRetryOptions</code> Retry/backoff behavior for failed requests. <code>max_retries=3</code>, <code>min_delay=200ms</code>, <code>max_delay=5s</code>, <code>jitter=true</code> <code>tenant_id</code> <code>Option&lt;String&gt;</code> Tenant identifier override. <code>None</code> (resolved from identity when possible) <code>database_name</code> <code>Option&lt;String&gt;</code> Database name override. <code>None</code> (resolved when possible; explicit value recommended if multiple DBs are accessible) <p>You can also construct a client from environment variables (<code>CHROMA_ENDPOINT</code>, <code>CHROMA_TENANT</code>, <code>CHROMA_DATABASE</code>):</p> <pre><code>let client = ChromaHttpClient::from_env()?;\n</code></pre>"},{"location":"core/clients/#uses-of-http-client","title":"Uses of HTTP Client","text":"<p>The HTTP client is ideal for when you want to scale your application or move off of local machine storage. It is important to note that there are trade-offs associated with using HTTP client:</p> <ul> <li>Network latency - The time it takes to send a request to the server and receive a response.</li> <li>Serialization and deserialization overhead - The time it takes to convert data to a format that can be sent over the   network and then convert it back to its original format.</li> <li>Security - The data is sent over the network, so it is important to ensure that the connection is secure (we recommend   using both HTTPS and authentication).</li> <li>Availability - The server must be available for the client to connect to it.</li> <li>Bandwidth usage - The amount of data sent over the network.</li> <li>Data privacy and compliance - Storing data on a remote server may require compliance with data protection laws and   regulations.</li> <li>Difficulty in debugging - Debugging network issues can be more difficult than debugging local issues. The same applies   to server-side issues.</li> </ul>"},{"location":"core/clients/#host-parameter-special-cases-python-only","title":"Host parameter special cases (Python-only)","text":"<p>The <code>host</code> parameter supports a more advanced syntax than just the hostname. You can specify the whole endpoint URL ( without the API paths), e.g. <code>https://chromadb.example.com:8000/my_server/path/</code>. This is useful when you want to use a reverse proxy or load balancer in front of your ChromaDB server.</p>"},{"location":"core/clients/#cloud-client","title":"Cloud Client","text":"<p>The <code>CloudClient</code> connects to Chroma Cloud. It handles authentication and endpoint configuration automatically.</p> <p>Environment Variables</p> <p>Cloud environment variable handling differs by language:</p> Language API key Tenant / database Python <code>api_key</code> arg or <code>CHROMA_API_KEY</code> <code>tenant</code>/<code>database</code> args, or <code>CHROMA_TENANT</code>/<code>CHROMA_DATABASE</code>, or auto-resolved from scoped credentials TypeScript <code>apiKey</code> arg or <code>CHROMA_API_KEY</code> Constructor args (<code>tenant</code>, <code>database</code>) Go <code>WithCloudAPIKey()</code> or <code>CHROMA_API_KEY</code> <code>WithDatabaseAndTenant()</code> or <code>CHROMA_TENANT</code> + <code>CHROMA_DATABASE</code> Rust <code>ChromaHttpClientOptions::cloud(...)</code> or <code>CHROMA_API_KEY</code> Explicit options or <code>CHROMA_TENANT</code>/<code>CHROMA_DATABASE</code>; otherwise resolved from identity when possible PythonTypeScriptGoRust <pre><code>import chromadb\n\n# Minimal \u2014 auto-resolves tenant/database from API key\nclient = chromadb.CloudClient(api_key=\"ck-your-api-key\")\n\n# Explicit tenant and database\nclient = chromadb.CloudClient(\n    tenant=\"your-tenant-id\",\n    database=\"your-database-name\",\n    api_key=\"ck-your-api-key\",\n)\n</code></pre> <p>Parameters:</p> Parameter Type Description Default / Allowed values <code>api_key</code> <code>str \\| None</code> Chroma Cloud API key. Required. Falls back to <code>CHROMA_API_KEY</code> <code>tenant</code> <code>str \\| None</code> Tenant identifier. Falls back to <code>CHROMA_TENANT</code>, then auth-based resolution <code>database</code> <code>str \\| None</code> Database name. Falls back to <code>CHROMA_DATABASE</code>, then auth-based resolution <code>settings</code> <code>Settings \\| None</code> Settings override. <code>None</code> (uses <code>Settings()</code>) <code>cloud_host</code> <code>str</code> Cloud API hostname (keyword-only; primarily for testing). <code>api.trychroma.com</code> <code>cloud_port</code> <code>int</code> Cloud API port (keyword-only; primarily for testing). <code>443</code> <code>enable_ssl</code> <code>bool</code> Enables TLS (keyword-only; primarily for testing). <code>True</code> <pre><code>import { CloudClient } from \"chromadb\";\n\nconst client = new CloudClient({\n    apiKey: \"ck-your-api-key\",\n    tenant: \"your-tenant-id\",\n    database: \"your-database-name\",\n});\n</code></pre> <p>Parameters:</p> Parameter Type Description Default / Allowed values <code>apiKey</code> <code>string \\| undefined</code> Chroma Cloud API key. Required. Falls back to <code>CHROMA_API_KEY</code> <code>tenant</code> <code>string \\| undefined</code> Tenant identifier. Optional. Defaults to <code>default_tenant</code> in underlying <code>ChromaClient</code> <code>database</code> <code>string \\| undefined</code> Database name. Optional. Defaults to <code>default_database</code> in underlying <code>ChromaClient</code> <code>cloudHost</code> <code>string \\| undefined</code> Cloud host prefix. <code>https://api.trychroma.com</code> <code>cloudPort</code> <code>string \\| undefined</code> Cloud port suffix. <code>8000</code> <pre><code>package main\n\nimport (\n    \"context\"\n    \"log\"\n\n    chroma \"github.com/amikos-tech/chroma-go/pkg/api/v2\"\n)\n\nfunc main() {\n    client, err := chroma.NewCloudClient(\n        chroma.WithCloudAPIKey(\"ck-your-api-key\"),\n        chroma.WithDatabaseAndTenant(\"your-database\", \"your-tenant-id\"),\n    )\n    if err != nil {\n        log.Fatalf(\"Error creating cloud client: %s \\n\", err)\n    }\n    if err := client.Heartbeat(context.TODO()); err != nil {\n        log.Fatalf(\"Error connecting: %s \\n\", err)\n    }\n}\n</code></pre> <p>Parameters:</p> Option Type Description Default / Allowed values <code>WithCloudAPIKey()</code> <code>func(string) ClientOption</code> Sets Chroma Cloud API key. Falls back to <code>CHROMA_API_KEY</code> <code>WithDatabaseAndTenant()</code> <code>func(database string, tenant string) ClientOption</code> Sets database and tenant explicitly. Required unless both env vars are set <code>WithDatabaseAndTenantFromEnv()</code> <code>func() ClientOption</code> Reads <code>CHROMA_DATABASE</code> and <code>CHROMA_TENANT</code>. Applied by default in <code>NewCloudClient</code> <code>WithTimeout()</code> <code>func(time.Duration) ClientOption</code> Sets request timeout. Optional <p><code>NewCloudClient</code> requires non-default tenant and database values and also requires an API key.</p> Go Client Package <p>The Go client is maintained at <code>amikos-tech/chroma-go</code> and has not yet been moved to <code>chroma-core</code>. Use the <code>github.com/amikos-tech/chroma-go/pkg/api/v2</code> import path.</p> <pre><code>use chroma::{ChromaHttpClient, ChromaHttpClientOptions};\n\n#[tokio::main]\nasync fn main() -&gt; anyhow::Result&lt;()&gt; {\n    // Explicit API key and database\n    let options = ChromaHttpClientOptions::cloud(\n        \"ck-your-api-key\",\n        \"your-database-name\",\n    )?;\n    let client = ChromaHttpClient::new(options);\n\n    // Or from environment variables (CHROMA_API_KEY, CHROMA_DATABASE, etc.)\n    let client = ChromaHttpClient::cloud()?;\n\n    let heartbeat = client.heartbeat().await?;\n    println!(\"Server timestamp: {}\", heartbeat);\n    Ok(())\n}\n</code></pre> <p>Parameters (<code>ChromaHttpClientOptions::cloud()</code>):</p> Parameter Type Description Default / Allowed values <code>api_key</code> <code>impl Into&lt;String&gt;</code> Chroma Cloud API key. Required <code>database_name</code> <code>impl Into&lt;String&gt;</code> Database name used for collection operations. Required <p>Environment-based (<code>ChromaHttpClient::cloud()</code>):</p> Variable Required Description Default / Allowed values <code>CHROMA_API_KEY</code> Yes Cloud API key. None <code>CHROMA_ENDPOINT</code> No Cloud endpoint override. <code>https://api.trychroma.com</code> <code>CHROMA_TENANT</code> No Tenant override. If omitted, resolved from identity when possible <code>CHROMA_DATABASE</code> No Database override. If omitted, resolved from identity when possible"},{"location":"core/clients/#ephemeral-client","title":"Ephemeral Client","text":"<p>Ephemeral client is a client that does not store any data on disk. It is useful for fast prototyping and testing. To get started with an ephemeral client, use the <code>EphemeralClient</code> class.</p> <pre><code>import chromadb\nfrom chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n\nclient = chromadb.EphemeralClient(\n    settings=Settings(),\n    tenant=DEFAULT_TENANT,\n    database=DEFAULT_DATABASE,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Description Default / Allowed values <code>settings</code> <code>Settings \\| None</code> Chroma settings object. <code>None</code> (uses <code>Settings()</code>) <code>tenant</code> <code>str</code> Tenant to use. <code>default_tenant</code> <code>database</code> <code>str</code> Database to use. <code>default_database</code> <p>Positional Parameters</p> <p>Chroma <code>EphemeralClient</code> parameters are positional, unless keyword arguments are used.</p>"},{"location":"core/clients/#environment-variable-configured-client","title":"Environment Variable Configured Client","text":"<p>You can also configure the client using environment variables. This is useful when you want to configure any of the client options listed above via environment variables.</p> <pre><code>import chromadb\n\n# Uses configured defaults from environment/.env/settings.\nclient = chromadb.Client()\n</code></pre> <p>Parameters:</p> Parameter Type Description Default / Allowed values <code>settings</code> <code>Settings</code> Settings object for environment and runtime configuration. Current global settings (<code>chromadb.get_settings()</code>) <code>tenant</code> <code>str</code> Tenant to use. <code>default_tenant</code> <code>database</code> <code>str</code> Database to use. <code>default_database</code> <p>Positional Parameters</p> <p>Chroma <code>Client</code> parameters are positional, unless keyword arguments are used.</p>"},{"location":"core/collections/","title":"Collections","text":"<p>Collections are the grouping mechanism for embeddings, documents, and metadata.</p> <p>Runnable Examples</p> <p>Complete, runnable collection examples for each language are available in the examples/collections directory:</p> <ul> <li>Python</li> <li>TypeScript</li> <li>Go</li> <li>Rust</li> </ul> <p>All examples require a running Chroma server: <code>docker run -p 8000:8000 chromadb/chroma</code></p>"},{"location":"core/collections/#collection-basics","title":"Collection Basics","text":""},{"location":"core/collections/#collection-properties","title":"Collection Properties","text":"<p>Each collection is characterized by the following properties:</p> <ul> <li><code>name</code>: The name of the collection. The name can be changed as long as it is unique within the database (   use <code>collection.modify(name=\"new_name\")</code> to change the name of the collection</li> <li><code>metadata</code>: A dictionary of metadata associated with the collection. The metadata is a dictionary of key-value pairs.   Keys can be strings, values can be strings, integers, floats, or booleans. Metadata can be changed   using <code>collection.modify(metadata={\"key\": \"value\"})</code> (Note: Metadata is always overwritten when modified)</li> <li><code>configuration</code>: A dictionary of HNSW index configuration options. Configuration is set at collection creation time via the <code>configuration</code> parameter. See the example below.</li> <li><code>embedding_function</code>: The embedding function used to embed documents in the collection.</li> </ul> <p>Defaults:</p> <ul> <li>Embedding Function - by default if <code>embedding_function</code> parameter is not provided at <code>create_collection()</code>   or <code>get_or_create_collection()</code> time, Chroma uses <code>chromadb.utils.embedding_functions.DefaultEmbeddingFunction</code> to embed documents. The default embedding   function uses Onnx Runtime   with <code>all-MiniLM-L6-v2</code> model.</li> <li>Distance metric - by default Chroma uses L2 (Euclidean Distance Squared) distance metric for newly created collections.   You can change it at creation time using the <code>configuration</code> parameter:   <code>configuration={\"hnsw\": {\"space\": \"cosine\"}}</code>. Possible values are <code>l2</code>, <code>cosine</code>, and <code>ip</code> (inner product). (Note: <code>cosine</code> value returns <code>cosine distance</code> rather than <code>cosine similarity</code>. I.e. values close to 0 means the embeddings are more similar.)</li> <li>Batch size, defined by <code>configuration={\"hnsw\": {\"batch_size\": 100}}</code>. Default is 100. The batch size defines the size of the   in-memory bruteforce index. Once the threshold is reached, vectors are added to the HNSW index and the bruteforce   index is cleared. Greater values may improve ingest performance. When updating also consider changing sync threshold.</li> <li>Sync threshold, defined by <code>configuration={\"hnsw\": {\"sync_threshold\": 1000}}</code>. Default is 1000. The sync threshold defines the limit at   which the HNSW index is synced to disk. This limit only applies to newly added vectors.</li> </ul> <p>Keep in Mind</p> <p>Collection distance metric cannot be changed after the collection is created. To change the distance metric see Cloning a Collection.</p> <p>Embedding Function Persistence</p> <p>Since Chroma v1.1.13, the embedding function configuration (EF) is persisted server-side. You no longer need to pass <code>embedding_function</code> when calling <code>get_collection</code> \u2014 Chroma will use the EF that was set at collection creation time.</p> <p>Name Restrictions</p> <p>Collection names in Chroma must adhere to the following restrictions:</p> <p>(1) contains 3-512 characters (2) starts and ends with a lowercase letter or a digit (3) can contain dots, dashes, and underscores in between (4) cannot contain two consecutive periods (<code>..</code>) (5) is not a valid IPv4 address</p>"},{"location":"core/collections/#creating-a-collection","title":"Creating a collection","text":"<p>Official Docs</p> <p>For more information on the <code>create_collection</code> or <code>get_or_create_collection</code> methods, see the official ChromaDB documentation.</p> <p>Parameters:</p> Name Description Default Value Type <code>name</code> Name of the collection to create. Parameter is required N/A String <code>metadata</code> Metadata associated with the collection. This is an optional parameter <code>None</code> Dictionary <code>configuration</code> HNSW index configuration for the collection. This is an optional parameter <code>None</code> Dictionary <code>embedding_function</code> Embedding function to use for the collection. This is an optional parameter <code>chromadb.utils.embedding_functions.DefaultEmbeddingFunction</code> EmbeddingFunction PythonTypeScriptGoRust <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncol = client.create_collection(\"test\")\n</code></pre> <pre><code>import { ChromaClient } from \"chromadb\";\n\nconst client = new ChromaClient();\nconst collection = await client.createCollection({ name: \"test\" });\n</code></pre> <pre><code>package main\n\nimport (\n    \"context\"\n    chroma \"github.com/amikos-tech/chroma-go\"\n)\n\nfunc main() {\n    ctx := context.Background()\n    client, _ := chroma.NewHTTPClient(ctx, chroma.WithDefaultDatabase(\"default_database\"), chroma.WithDefaultTenant(\"default_tenant\"))\n    col, _ := client.CreateCollection(ctx, \"test\", false)\n}\n</code></pre> <pre><code>use chromadb::v2::ChromaClient;\n\n#[tokio::main]\nasync fn main() {\n    let client = ChromaClient::new(Default::default()).await.unwrap();\n    let collection = client.create_collection(\"test\", None, None).await.unwrap();\n}\n</code></pre> <p>Alternatively you can use the <code>get_or_create_collection</code> method to create a collection if it doesn't exist already.</p> PythonTypeScriptGoRust <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncol = client.get_or_create_collection(\"test\", metadata={\"key\": \"value\"})\n</code></pre> <pre><code>import { ChromaClient } from \"chromadb\";\n\nconst client = new ChromaClient();\nconst collection = await client.getOrCreateCollection({\n    name: \"test\",\n    metadata: { key: \"value\" },\n});\n</code></pre> <pre><code>col, _ := client.GetOrCreateCollection(ctx, \"test\")\n</code></pre> <pre><code>let collection = client.get_or_create_collection(\"test\", None, None).await.unwrap();\n</code></pre> <p>Creating a collection with custom HNSW configuration:</p> PythonTypeScript <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncol = client.create_collection(\n    \"test\",\n    configuration={\n        \"hnsw\": {\n            \"space\": \"cosine\",\n            \"ef_construction\": 200,\n            \"max_neighbors\": 32,\n        }\n    },\n)\n</code></pre> <pre><code>import { ChromaClient } from \"chromadb\";\n\nconst client = new ChromaClient();\nconst collection = await client.createCollection({\n    name: \"test\",\n    configuration: {\n        hnsw: {\n            space: \"cosine\",\n            ef_construction: 200,\n            max_neighbors: 32,\n        },\n    },\n});\n</code></pre>"},{"location":"core/collections/#embedding-function-configuration-and-persistence","title":"Embedding Function Configuration and Persistence","text":"<p>Starting with Chroma v1.1.13, embedding functions are persisted server-side in the collection configuration. After you create a collection, later <code>get_collection</code> / <code>getCollection</code> calls will auto-resolve the persisted embedding function.</p> <p>You can configure embedding functions in two ways:</p> <ol> <li>Pass <code>embedding_function</code> when creating a collection</li> <li>Set <code>configuration.embedding_function</code> with <code>name</code> and <code>config</code></li> </ol> <p>API keys are auto-discovered from provider standard environment variables (for example <code>OPENAI_API_KEY</code>). If you use a non-standard variable, set <code>api_key_env_var</code> (Python) or <code>apiKeyEnvVar</code> (TypeScript).</p> <p>The persisted <code>embedding_function</code> payload follows provider schemas in the upstream Chroma registry:</p> <ul> <li>Embedding Function Schemas</li> <li>OpenAI Schema Example</li> <li>Schema README</li> </ul> <p>Cross-checked dense provider/package mapping:</p> Provider Python TypeScript (NPM \u00b7 GitHub) Go (pkg.go.dev \u00b7 GitHub) OpenAI \u2705 \u2705 npm \u00b7 src \u2705 pkg \u00b7 src Google Gemini \u2705 \u2705 npm \u00b7 src \u2705 pkg \u00b7 src Cohere \u2705 \u2705 npm \u00b7 src \u2705 pkg \u00b7 src Cloudflare Workers AI \u2705 \u2705 npm \u00b7 src \u2705 pkg \u00b7 src Hugging Face \u2705 - \u2705 pkg \u00b7 src Hugging Face Embedding Server \u2705 \u2705 npm \u00b7 src \u2705 pkg \u00b7 src Instructor \u2705 - - Jina AI \u2705 \u2705 npm \u00b7 src \u2705 pkg \u00b7 src Mistral \u2705 \u2705 npm \u00b7 src \u2705 pkg \u00b7 src Morph \u2705 \u2705 npm \u00b7 src \u2705 pkg \u00b7 src Ollama \u2705 \u2705 npm \u00b7 src \u2705 pkg \u00b7 src Nomic \u2705 - \u2705 pkg \u00b7 src OpenCLIP (Multimodal) \u2705 - - Roboflow (Multimodal) \u2705 - \u2705 pkg \u00b7 src Sentence Transformers \u2705 \u2705 npm \u00b7 src - Text2Vec \u2705 - - Together AI \u2705 \u2705 npm \u00b7 src \u2705 pkg \u00b7 src VoyageAI \u2705 \u2705 npm \u00b7 src \u2705 pkg \u00b7 src Amazon Bedrock \u2705 - \u2705 pkg \u00b7 src Baseten \u2705 - \u2705 pkg \u00b7 src Chroma Cloud Qwen \u2705 \u2705 npm \u00b7 src - <p>Sparse embedding function integrations include:</p> <ul> <li>Chroma BM25</li> <li>Chroma Cloud Splade</li> <li>Hugging Face sparse</li> </ul> <p>For broader language/provider support, see:</p> <ul> <li>Chroma Ecosystem Clients</li> <li>Chroma Integrations</li> <li>Upstream embedding functions reference</li> <li>Upstream collection configuration reference</li> </ul> <p>Cross-check Scope</p> <p>Python/TypeScript support was cross-checked against Chroma Docs integrations and embedding functions pages. Go package mappings were cross-checked against <code>github.com/amikos-tech/chroma-go/v2/pkg/embeddings/*</code>.</p> <p>Configure a persisted EF at collection creation:</p> PythonTypeScriptGo <pre><code>import chromadb\nfrom chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n\nclient = chromadb.HttpClient()\n\n# 1) Set via embedding_function argument\nef = OpenAIEmbeddingFunction(model_name=\"text-embedding-3-small\")\ncol = client.create_collection(\"with_openai_ef\", embedding_function=ef)\n\n# 2) Later calls auto-resolve persisted EF (no ef needed here)\nsame_col = client.get_collection(\"with_openai_ef\")\n</code></pre> <pre><code>import { ChromaClient } from \"chromadb\";\n\nconst client = new ChromaClient();\n\nconst collection = await client.createCollection({\n    name: \"with_openai_ef\",\n    configuration: {\n        embedding_function: {\n            name: \"openai\",\n            config: {\n                model_name: \"text-embedding-3-small\",\n                apiKeyEnvVar: \"OPENAI_API_KEY\",\n            },\n        },\n    },\n});\n\nconst sameCollection = await client.getCollection({ name: \"with_openai_ef\" });\n</code></pre> <pre><code>package main\n\nimport (\n    \"context\"\n    \"os\"\n\n    chroma \"github.com/amikos-tech/chroma-go/v2\"\n    v2 \"github.com/amikos-tech/chroma-go/v2/pkg/api/v2\"\n    openai \"github.com/amikos-tech/chroma-go/v2/pkg/embeddings/openai\"\n)\n\nfunc main() {\n    ctx := context.Background()\n    client, _ := chroma.NewHTTPClient(ctx,\n        chroma.WithDefaultDatabase(\"default_database\"),\n        chroma.WithDefaultTenant(\"default_tenant\"),\n    )\n\n    ef, _ := openai.NewOpenAIEmbeddingFunction(os.Getenv(\"OPENAI_API_KEY\"))\n    _, _ = client.CreateCollection(ctx, \"with_openai_ef\", v2.WithEmbeddingFunctionCreate(ef))\n\n    // Persisted EF is auto-resolved server-side\n    _, _ = client.GetCollection(ctx, \"with_openai_ef\")\n}\n</code></pre> <p>Custom API key environment variable names:</p> PythonTypeScript <pre><code>from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n\nef = OpenAIEmbeddingFunction(\n    model_name=\"text-embedding-3-small\",\n    api_key_env_var=\"MY_CUSTOM_OPENAI_KEY\",\n)\n</code></pre> <pre><code>import { OpenAIEmbeddingFunction } from \"@chroma-core/openai\";\n\nconst ef = new OpenAIEmbeddingFunction({\n    modelName: \"text-embedding-3-small\",\n    apiKeyEnvVar: \"MY_CUSTOM_OPENAI_KEY\",\n});\n</code></pre> <p>Custom embedding function patterns:</p> PythonTypeScriptGo <pre><code>from typing import Any, Dict\nfrom chromadb import Documents, EmbeddingFunction, Embeddings\nfrom chromadb.utils.embedding_functions import register_embedding_function\n\n@register_embedding_function\nclass MyEmbeddingFunction(EmbeddingFunction):\n    def __call__(self, input: Documents) -&gt; Embeddings:\n        # Produce embeddings for input documents\n        return [[0.0] * 3 for _ in input]\n\n    @staticmethod\n    def name() -&gt; str:\n        return \"my-embedding-function\"\n\n    def get_config(self) -&gt; Dict[str, Any]:\n        return {\"model\": \"my-model-v1\"}\n\n    @staticmethod\n    def build_from_config(config: Dict[str, Any]) -&gt; \"MyEmbeddingFunction\":\n        return MyEmbeddingFunction()\n</code></pre> <pre><code>import type { ChromaClient, EmbeddingFunction } from \"chromadb\";\n\ntype MyConfig = { model: string };\n\nclass MyEmbeddingFunction implements EmbeddingFunction {\n    public readonly name = \"my-embedding-function\";\n\n    constructor(private readonly config: MyConfig) {}\n\n    async generate(texts: string[]): Promise&lt;number[][]&gt; {\n        return texts.map(() =&gt; [0, 0, 0]);\n    }\n\n    getConfig(): MyConfig {\n        return this.config;\n    }\n\n    validateConfigUpdate(next: Record&lt;string, unknown&gt;) {\n        if (\"model\" in next) {\n            throw new Error(\"Model cannot be updated\");\n        }\n    }\n\n    static buildFromConfig(config: MyConfig, _client?: ChromaClient): MyEmbeddingFunction {\n        return new MyEmbeddingFunction(config);\n    }\n}\n</code></pre> <pre><code>package myef\n\nimport (\n    \"context\"\n\n    \"github.com/amikos-tech/chroma-go/v2/pkg/embeddings\"\n)\n\ntype MyEmbeddingFunction struct{}\n\nfunc (m *MyEmbeddingFunction) EmbedDocuments(_ context.Context, texts []string) ([]embeddings.Embedding, error) {\n    out := make([]embeddings.Embedding, len(texts))\n    for i := range texts {\n        out[i] = embeddings.NewEmbeddingFromFloat32([]float32{0, 0, 0})\n    }\n    return out, nil\n}\n\nfunc (m *MyEmbeddingFunction) EmbedQuery(_ context.Context, _ string) (embeddings.Embedding, error) {\n    return embeddings.NewEmbeddingFromFloat32([]float32{0, 0, 0}), nil\n}\n\nfunc (m *MyEmbeddingFunction) Name() string { return \"my-embedding-function\" }\n\nfunc (m *MyEmbeddingFunction) GetConfig() embeddings.EmbeddingFunctionConfig {\n    return embeddings.EmbeddingFunctionConfig{\"model\": \"my-model-v1\"}\n}\n\nfunc (m *MyEmbeddingFunction) DefaultSpace() embeddings.DistanceMetric { return embeddings.COSINE }\n\nfunc (m *MyEmbeddingFunction) SupportedSpaces() []embeddings.DistanceMetric {\n    return []embeddings.DistanceMetric{embeddings.COSINE}\n}\n\nfunc newMyEmbeddingFunctionFromConfig(_ embeddings.EmbeddingFunctionConfig) (embeddings.EmbeddingFunction, error) {\n    return &amp;MyEmbeddingFunction{}, nil\n}\n\nfunc init() {\n    _ = embeddings.RegisterDense(\"my-embedding-function\", newMyEmbeddingFunctionFromConfig)\n}\n</code></pre> <p>Metadata with <code>get_or_create_collection()</code></p> <p>If the collection exists and metadata is provided in the method it will attempt to overwrite the existing metadata.</p>"},{"location":"core/collections/#deleting-a-collection","title":"Deleting a collection","text":"<p>Official Docs</p> <p>For more information on the <code>delete_collection</code> method, see the official ChromaDB documentation.</p> <p>Destructive Operation</p> <p>Deleting a collection permanently removes all its data (embeddings, documents, and metadata). This action cannot be undone.</p> <p>Parameters:</p> Name Description Default Value Type <code>name</code> Name of the collection to delete. Parameter is required N/A String PythonTypeScriptGoRust <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\nclient.delete_collection(\"test\")\n</code></pre> <pre><code>import { ChromaClient } from \"chromadb\";\n\nconst client = new ChromaClient();\nawait client.deleteCollection({ name: \"test\" });\n</code></pre> <pre><code>_, err := client.DeleteCollection(ctx, \"test\")\n</code></pre> <pre><code>client.delete_collection(\"test\").await.unwrap();\n</code></pre>"},{"location":"core/collections/#listing-all-collections","title":"Listing all collections","text":"<p>Official Docs</p> <p>For more information on the <code>list_collections</code> method, see the official ChromaDB documentation.</p> <p>The <code>list_collections</code> method returns <code>Collection</code> objects (name, metadata, configuration, and counts). Use <code>offset</code> and <code>limit</code> to paginate through large tenants or databases.</p> <p>Parameters:</p> Name Description Default Value Type <code>offset</code> The starting offset for listing collections. This is an optional parameter <code>None</code> Positive Integer <code>limit</code> The number of collections to return. If the remaining collections from <code>offset</code> are fewer than this number then returned collection will also be fewer. This is an optional parameter <code>None</code> Positive Integer PythonTypeScriptGoRust <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncollections = client.list_collections()  # returns list of collection names\n\n# with pagination\ncollections = client.list_collections(limit=10, offset=0)\n</code></pre> <pre><code>import { ChromaClient } from \"chromadb\";\n\nconst client = new ChromaClient();\nconst collections = await client.listCollections({ limit: 10, offset: 0 });\n\n// fetch the next page by advancing the offset\nconst nextPage = await client.listCollections({ limit: 10, offset: 10 });\n</code></pre> <pre><code>collections, _ := client.ListCollections(ctx)\n\n// with pagination\ncollections, _ = client.ListCollections(ctx, chroma.ListWithLimit(10), chroma.ListWithOffset(0))\n</code></pre> <pre><code>let collections = client.list_collections(100, None).await.unwrap();\n\n// with pagination\nlet collections = client.list_collections(10, Some(0)).await.unwrap();\n</code></pre>"},{"location":"core/collections/#getting-a-collection","title":"Getting a collection","text":"<p>Official Docs</p> <p>For more information on the <code>get_collection</code> method, see the official ChromaDB documentation.</p> <p>Embedding Function Persistence</p> <p>Since Chroma v1.1.13, the embedding function is persisted server-side. You no longer need to pass <code>embedding_function</code> when calling <code>get_collection</code>. If you do pass one, it will override the persisted configuration for that client session.</p> <p>Parameters:</p> Name Description Default Value Type <code>name</code> Name of the collection to get. Parameter is required N/A String <code>embedding_function</code> Embedding function override for the collection. Optional \u2014 uses the persisted EF if not provided <code>None</code> EmbeddingFunction PythonTypeScriptGoRust <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncol = client.get_collection(\"test\")\n</code></pre> <pre><code>import { ChromaClient } from \"chromadb\";\n\nconst client = new ChromaClient();\nconst collection = await client.getCollection({ name: \"test\" });\n</code></pre> <pre><code>col, _ := client.GetCollection(ctx, \"test\")\n</code></pre> <pre><code>let collection = client.get_collection(\"test\").await.unwrap();\n</code></pre>"},{"location":"core/collections/#modifying-a-collection","title":"Modifying a collection","text":"<p>Official Docs</p> <p>For more information on the <code>modify</code> method, see the official ChromaDB documentation.</p> <p>Modify method on collection</p> <p>The <code>modify</code> method is called on the collection and not on the client, unlike the rest of the collection lifecycle methods.</p> <p>Metadata Overwrite</p> <p>Metadata is always overwritten when modified. If you want to add a new key-value pair to the metadata, you must first get the existing metadata and then add the new key-value pair to it.</p> <p>Changing HNSW parameters</p> <p>HNSW configuration parameters (space, M, ef_construction, etc.) cannot be changed after the collection is created. To change these parameters, clone the collection \u2014 see Cloning a Collection.</p> <p>Parameters:</p> Name Description Default Value Type <code>name</code> The new name of the collection. Parameter is required N/A String <code>metadata</code> Metadata associated with the collection. This is an optional parameter <code>None</code> Dictionary <p>Both collection properties (<code>name</code> and <code>metadata</code>) can be modified, separately or together.</p> PythonTypeScript <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncol = client.get_collection(\"test\")\ncol.modify(name=\"test2\", metadata={\"key\": \"value\"})\n</code></pre> <pre><code>import { ChromaClient } from \"chromadb\";\n\nconst client = new ChromaClient();\nconst collection = await client.getCollection({ name: \"test\" });\nawait collection.modify({ name: \"test2\", metadata: { key: \"value\" } });\n</code></pre>"},{"location":"core/collections/#counting-collections","title":"Counting Collections","text":"<p>Returns the number of collections for the currently configured tenant and database.</p> <p>Official Docs</p> <p>For more information on the <code>count_collections</code> method, see the official ChromaDB documentation.</p> PythonTypeScript <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncol = client.get_or_create_collection(\"test\")  # create a new collection\n\ncollections_count = client.count_collections()  # int\n</code></pre> <pre><code>import { ChromaClient } from \"chromadb\";\n\nconst client = new ChromaClient();\nconst count = await client.countCollections();\n</code></pre>"},{"location":"core/collections/#convenience-methods","title":"Convenience Methods","text":"<p>The following methods are available on a collection instance:</p> PythonTypeScript <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncol = client.get_or_create_collection(\"test\")\ncol.add(ids=[\"1\", \"2\"], documents=[\"hello world\", \"hello chroma\"])\n\n# peek at the first N items in the collection (default 10)\ncol.peek()\ncol.peek(limit=5)\n\n# count the number of items in the collection\ncol.count()\n</code></pre> <pre><code>import { ChromaClient } from \"chromadb\";\n\nconst client = new ChromaClient();\nconst collection = await client.getOrCreateCollection({ name: \"test\" });\nawait collection.add({\n    ids: [\"1\", \"2\"],\n    documents: [\"hello world\", \"hello chroma\"],\n});\n\n// peek at the first N items in the collection (default 10)\nawait collection.peek();\nawait collection.peek({ limit: 5 });\n\n// count the number of items in the collection\nawait collection.count();\n</code></pre>"},{"location":"core/collections/#query-and-get-results","title":"Query and Get Results","text":"<p><code>collection.get()</code> and <code>collection.query()</code> return column-oriented results.</p> <ul> <li>Column values are aligned by index. For <code>get()</code>, <code>ids[i]</code> refers to the same record as <code>documents[i]</code>, <code>metadatas[i]</code>, and <code>embeddings[i]</code> (if included).</li> <li><code>query()</code> adds one level of nesting. <code>ids[q][k]</code> is the <code>k</code>-th match for query <code>q</code>, and aligns with <code>documents[q][k]</code>, <code>metadatas[q][k]</code>, and <code>distances[q][k]</code> (if included).</li> <li>Use <code>include</code> to control which optional fields are returned.</li> <li>Default <code>include</code> fields for <code>get()</code>: <code>documents</code> and <code>metadatas</code> (order may vary by client).</li> <li>Default <code>include</code> fields for <code>query()</code>: <code>documents</code>, <code>metadatas</code>, and <code>distances</code> (order may vary by client).</li> <li><code>ids</code> are always returned, even when <code>include=[]</code>.</li> </ul>"},{"location":"core/collections/#constrain-query-candidates-by-id","title":"Constrain Query Candidates By ID","text":"<p>Use the <code>ids</code> argument on <code>query()</code> to search only within a known subset of records. Provide one query input (<code>query_texts</code> or <code>query_embeddings</code>) and an <code>ids</code> list. By default, Chroma returns up to 10 results per query, capped by matching IDs.</p> PythonTypeScriptGoRust <pre><code>collection.query(\n    query_texts=[\"climate\"],\n    ids=[\"doc-1\", \"doc-2\", \"doc-3\"],\n)\n</code></pre> <pre><code>await collection.query({\n    queryTexts: [\"climate\"],\n    ids: [\"doc-1\", \"doc-2\", \"doc-3\"],\n});\n</code></pre> <pre><code>_, err := collection.Query(ctx,\n    chroma.WithQueryTexts(\"climate\"),\n    chroma.WithIDs(\"doc-1\", \"doc-2\", \"doc-3\"),\n)\nif err != nil {\n    panic(err)\n}\n</code></pre> <pre><code>let _results = collection\n    .query(\n        vec![vec![0.1, 0.2, 0.3]],\n        None,\n        None,\n        Some(vec![\n            \"doc-1\".to_string(),\n            \"doc-2\".to_string(),\n            \"doc-3\".to_string(),\n        ]),\n        None,\n    )\n    .await?;\n</code></pre>"},{"location":"core/collections/#result-type-shapes","title":"Result Type Shapes","text":"PythonTypeScriptGoRust <pre><code>class GetResult(TypedDict):\n    ids: List[ID]\n    embeddings: Optional[Union[Embeddings, PyEmbeddings, NDArray[Union[np.int32, np.float32]]]]\n    documents: Optional[List[Document]]\n    uris: Optional[URIs]\n    data: Optional[Loadable]\n    metadatas: Optional[List[Metadata]]\n    included: Include\n\nclass QueryResult(TypedDict):\n    ids: List[IDs]\n    embeddings: Optional[\n        Union[\n            List[Embeddings],\n            List[PyEmbeddings],\n            List[NDArray[Union[np.int32, np.float32]]],\n        ]\n    ]\n    documents: Optional[List[List[Document]]]\n    uris: Optional[List[List[URI]]]\n    data: Optional[List[Loadable]]\n    metadatas: Optional[List[List[Metadata]]]\n    distances: Optional[List[List[float]]]\n    included: Include\n</code></pre> <pre><code>class GetResult&lt;TMeta extends Metadata = Metadata&gt; {\n    readonly ids: string[];\n    readonly documents: (string | null)[];\n    readonly metadatas: (TMeta | null)[];\n    readonly embeddings: number[][];\n    readonly uris: (string | null)[];\n    readonly include: Include[];\n    rows(): Array&lt;{\n        id: string;\n        document?: string | null;\n        metadata?: TMeta | null;\n        embedding?: number[];\n        uri?: string | null;\n    }&gt;;\n}\n\nclass QueryResult&lt;TMeta extends Metadata = Metadata&gt; {\n    readonly ids: string[][];\n    readonly documents: (string | null)[][];\n    readonly metadatas: (TMeta | null)[][];\n    readonly embeddings: (number[] | null)[][];\n    readonly distances: (number | null)[][];\n    readonly uris: (string | null)[][];\n    readonly include: Include[];\n    rows(): QueryRowResult&lt;TMeta&gt;[][];\n}\n</code></pre> <pre><code>// Selected methods shown for brevity.\ntype GetResult interface {\n    GetIDs() DocumentIDs\n    GetDocuments() Documents\n    GetMetadatas() DocumentMetadatas\n    GetEmbeddings() embeddings.Embeddings\n}\n\ntype QueryResult interface {\n    GetIDGroups() []DocumentIDs\n    GetDocumentsGroups() []Documents\n    GetMetadatasGroups() []DocumentMetadatas\n    GetEmbeddingsGroups() []embeddings.Embeddings\n    GetDistancesGroups() []embeddings.Distances\n}\n\ntype GetResultImpl struct {\n    Ids        DocumentIDs\n    Documents  Documents\n    Metadatas  DocumentMetadatas\n    Embeddings embeddings.Embeddings\n    Include    []Include\n}\n\ntype QueryResultImpl struct {\n    IDLists         []DocumentIDs\n    DocumentsLists  []Documents\n    MetadatasLists  []DocumentMetadatas\n    EmbeddingsLists []embeddings.Embeddings\n    DistancesLists  []embeddings.Distances\n    Include         []Include\n}\n\n// Row helpers for iteration\nfunc (r *GetResultImpl) Rows() []ResultRow\nfunc (r *GetResultImpl) At(index int) (ResultRow, bool)\n// Query.Rows() returns the first query group; use RowGroups() for all groups.\nfunc (r *QueryResultImpl) Rows() []ResultRow\nfunc (r *QueryResultImpl) RowGroups() [][]ResultRow\nfunc (r *QueryResultImpl) At(group, index int) (ResultRow, bool)\n</code></pre> <pre><code>pub struct GetResponse {\n    pub ids: Vec&lt;String&gt;,\n    pub embeddings: Option&lt;Vec&lt;Vec&lt;f32&gt;&gt;&gt;,\n    pub documents: Option&lt;Vec&lt;Option&lt;String&gt;&gt;&gt;,\n    pub uris: Option&lt;Vec&lt;Option&lt;String&gt;&gt;&gt;,\n    pub metadatas: Option&lt;Vec&lt;Option&lt;Metadata&gt;&gt;&gt;,\n    pub include: Vec&lt;Include&gt;,\n}\n\npub struct QueryResponse {\n    pub ids: Vec&lt;Vec&lt;String&gt;&gt;,\n    pub embeddings: Option&lt;Vec&lt;Vec&lt;Option&lt;Vec&lt;f32&gt;&gt;&gt;&gt;&gt;,\n    pub documents: Option&lt;Vec&lt;Vec&lt;Option&lt;String&gt;&gt;&gt;&gt;,\n    pub uris: Option&lt;Vec&lt;Vec&lt;Option&lt;String&gt;&gt;&gt;&gt;,\n    pub metadatas: Option&lt;Vec&lt;Vec&lt;Option&lt;Metadata&gt;&gt;&gt;&gt;,\n    pub distances: Option&lt;Vec&lt;Vec&lt;Option&lt;f32&gt;&gt;&gt;&gt;,\n    pub include: Vec&lt;Include&gt;,\n}\n</code></pre>"},{"location":"core/collections/#iteration-patterns","title":"Iteration Patterns","text":"PythonTypeScriptGoRust <pre><code># GET: zip aligned columns\nresult = collection.get(include=[\"documents\", \"metadatas\"])\nif result[\"documents\"] is None or result[\"metadatas\"] is None:\n    raise ValueError(\"include must contain documents and metadatas\")\n\nfor doc_id, doc, meta in zip(\n    result[\"ids\"],\n    result[\"documents\"],\n    result[\"metadatas\"],\n):\n    print(doc_id, doc, meta)\n\n# QUERY: nested loop (queries -&gt; matches)\nq = collection.query(query_texts=[\"climate\"], n_results=3, include=[\"documents\", \"distances\"])\nif q[\"documents\"] is None or q[\"distances\"] is None:\n    raise ValueError(\"include must contain documents and distances\")\n\nfor q_idx, ids in enumerate(q[\"ids\"]):\n    docs = q[\"documents\"][q_idx]\n    distances = q[\"distances\"][q_idx]\n    for doc_id, doc, distance in zip(ids, docs, distances):\n        print(q_idx, doc_id, distance, doc)\n</code></pre> <pre><code>// Metadata type inference with generics\nconst getResult = await collection.get&lt;{ page: number }&gt;({\n    include: [\"documents\", \"metadatas\"],\n});\n\nfor (const row of getResult.rows()) {\n    console.log(row.id, row.metadata?.page, row.document);\n}\n\nconst queryResult = await collection.query&lt;{ page: number }&gt;({\n    queryTexts: [\"climate\"],\n    nResults: 3,\n    include: [\"documents\", \"metadatas\", \"distances\"],\n});\n\nfor (const [queryIndex, rows] of queryResult.rows().entries()) {\n    for (const row of rows) {\n        console.log(queryIndex, row.id, row.distance, row.metadata?.page);\n    }\n}\n</code></pre> <pre><code>// Keep example compact: panic on unexpected errors/types.\ngetResult, err := collection.Get(ctx, chroma.WithInclude(chroma.IncludeDocuments, chroma.IncludeMetadatas))\nif err != nil {\n    panic(err)\n}\ngetRows, ok := getResult.(*chroma.GetResultImpl)\nif !ok {\n    panic(fmt.Sprintf(\"unexpected get result type %T\", getResult))\n}\nfor _, row := range getRows.Rows() {\n    fmt.Println(row.ID, row.Document, row.Metadata)\n}\nif row, ok := getRows.At(0); ok {\n    fmt.Println(\"first get row:\", row.ID)\n}\n\nqueryResult, err := collection.Query(ctx,\n    chroma.WithQueryTexts(\"climate\"),\n    chroma.WithNResults(3),\n    chroma.WithInclude(chroma.IncludeDocuments, chroma.IncludeMetadatas, chroma.IncludeDistances),\n)\nif err != nil {\n    panic(err)\n}\nqueryRows, ok := queryResult.(*chroma.QueryResultImpl)\nif !ok {\n    panic(fmt.Sprintf(\"unexpected query result type %T\", queryResult))\n}\n// Query.Rows() gives rows for the first query group.\nfor _, row := range queryRows.Rows() {\n    fmt.Println(\"q0\", row.ID, row.Score, row.Document)\n}\nif row, ok := queryRows.At(0, 0); ok {\n    fmt.Println(\"first query row:\", row.ID)\n}\n// Query.RowGroups() gives all query groups (useful for multi-query inputs).\nfor queryIndex, rows := range queryRows.RowGroups() {\n    for _, row := range rows {\n        fmt.Println(queryIndex, row.ID, row.Score, row.Document)\n    }\n}\n</code></pre> <pre><code>// `None` include uses the Rust defaults:\n// IncludeList::default_get() and IncludeList::default_query().\nlet get_result = collection.get(None, None, Some(10), Some(0), None).await?;\nfor (i, id) in get_result.ids.iter().enumerate() {\n    let doc = get_result\n        .documents\n        .as_ref()\n        .and_then(|docs| docs.get(i))\n        .and_then(|doc| doc.as_deref());\n    println!(\"{id}: {:?}\", doc);\n}\n\nlet query_result = collection\n    .query(vec![vec![0.1, 0.2, 0.3]], Some(3), None, None, None)\n    .await?;\nfor (i, ids) in query_result.ids.iter().enumerate() {\n    println!(\"query {i} has {} neighbors\", ids.len());\n}\nfor (query_index, ids) in query_result.ids.iter().enumerate() {\n    for (rank, id) in ids.iter().enumerate() {\n        let distance = query_result\n            .distances\n            .as_ref()\n            .and_then(|groups| groups.get(query_index))\n            .and_then(|group| group.get(rank))\n            .and_then(|v| *v);\n        println!(\"query={query_index} rank={rank} id={id} distance={distance:?}\");\n    }\n}\n</code></pre>"},{"location":"core/collections/#iterating-over-a-collection","title":"Iterating over a Collection","text":"<pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"my_local_data\")  # or HttpClient()\n\ncollection = client.get_or_create_collection(\"local_collection\")\ncollection.add(\n    ids=[f\"{i}\" for i in range(1000)],\n    documents=[f\"document {i}\" for i in range(1000)],\n    metadatas=[{\"doc_id\": i} for i in range(1000)])\nexisting_count = collection.count()\nbatch_size = 10\nfor i in range(0, existing_count, batch_size):\n    batch = collection.get(\n        include=[\"metadatas\", \"documents\", \"embeddings\"],\n        limit=batch_size,\n        offset=i)\n    print(batch)  # do something with the batch\n</code></pre>"},{"location":"core/collections/#collection-utilities","title":"Collection Utilities","text":""},{"location":"core/collections/#copying-collections","title":"Copying Collections","text":"Local To RemoteLocal To Local <p>The following example demonstrates how to copy a local collection to a remote ChromaDB server. (it also works in reverse)</p> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"my_local_data\")\nremote_client = chromadb.HttpClient()\n\ncollection = client.get_or_create_collection(\"local_collection\")\ncollection.add(\n    ids=[\"1\", \"2\"],\n    documents=[\"hello world\", \"hello ChromaDB\"],\n    metadatas=[{\"a\": 1}, {\"b\": 2}])\nremote_collection = remote_client.get_or_create_collection(\"remote_collection\",\n                                                           metadata=collection.metadata)\nexisting_count = collection.count()\nbatch_size = 10\nfor i in range(0, existing_count, batch_size):\n    batch = collection.get(\n        include=[\"metadatas\", \"documents\", \"embeddings\"],\n        limit=batch_size,\n        offset=i)\n    remote_collection.add(\n        ids=batch[\"ids\"],\n        documents=batch[\"documents\"],\n        metadatas=batch[\"metadatas\"],\n        embeddings=batch[\"embeddings\"])\n</code></pre> <p>Using ChromaDB Data Pipes</p> <p>Using ChromaDB Data Pipes package you can achieve the same result.</p> <pre><code>pip install chromadb-data-pipes\ncdp export \"file://path/to_local_data/local_collection\" | \\\ncdp import \"http://remote_chromadb:port/remote_collection\" --create\n</code></pre> <p>Following shows an example of how to copy a collection from one local persistent DB to another local persistent DB.</p> <pre><code>import chromadb\n\nlocal_client = chromadb.PersistentClient(path=\"source\")\nremote_client = chromadb.PersistentClient(path=\"target\")\n\ncollection = local_client.get_or_create_collection(\"my_source_collection\")\ncollection.add(\n    ids=[\"1\", \"2\"],\n    documents=[\"hello world\", \"hello ChromaDB\"],\n    metadatas=[{\"a\": 1}, {\"b\": 2}])\nremote_collection = remote_client.get_or_create_collection(\"my_target_collection\",\n                                                           metadata=collection.metadata)\nexisting_count = collection.count()\nbatch_size = 10\nfor i in range(0, existing_count, batch_size):\n    batch = collection.get(\n        include=[\"metadatas\", \"documents\", \"embeddings\"],\n        limit=batch_size,\n        offset=i)\n    remote_collection.add(\n        ids=batch[\"ids\"],\n        documents=batch[\"documents\"],\n        metadatas=batch[\"metadatas\"],\n        embeddings=batch[\"embeddings\"])\n</code></pre> <p>Using ChromaDB Data Pipes</p> <p>You can achieve the above with ChromaDB Data Pipes package.</p> <pre><code>pip install chromadb-data-pipes\ncdp export \"file://source_persist_dir/target_collection\" | \\\ncdp import \"file://target_persist_dir/target_collection\" --create\n</code></pre>"},{"location":"core/collections/#cloning-a-collection","title":"Cloning a collection","text":"<p>Here are some reasons why you might want to clone a collection:</p> <ul> <li>Change distance function (via <code>configuration</code> \u2014 <code>hnsw.space</code>)</li> <li>Change HNSW hyper parameters (<code>max_neighbors</code>, <code>ef_construction</code>, <code>search_ef</code>)</li> </ul> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncol = client.get_or_create_collection(\"test\")  # create a new collection with L2 (default)\n\ncol.add(ids=[f\"{i}\" for i in range(1000)], documents=[f\"document {i}\" for i in range(1000)])\nnewCol = client.get_or_create_collection(\"test1\", configuration={\n    \"hnsw\": {\"space\": \"cosine\"}})  # change the distance function to cosine\n\nexisting_count = col.count()\nbatch_size = 10\nfor i in range(0, existing_count, batch_size):\n    batch = col.get(include=[\"metadatas\", \"documents\", \"embeddings\"], limit=batch_size, offset=i)\n    newCol.add(ids=batch[\"ids\"], documents=batch[\"documents\"], metadatas=batch[\"metadatas\"],\n               embeddings=batch[\"embeddings\"])\n\nprint(newCol.count())\nprint(newCol.get(offset=0, limit=10))  # get first 10 documents\n</code></pre>"},{"location":"core/collections/#changing-the-embedding-function","title":"Changing the embedding function","text":"<p>To change the embedding function of a collection, it must be cloned to a new collection with the desired embedding function.</p> <p>External API Dependency</p> <p>This example requires an OpenAI API key (<code>OPENAI_API_KEY</code> environment variable). The runnable example skips this section gracefully when the key is not set.</p> <pre><code>import os\nimport chromadb\nfrom chromadb.utils.embedding_functions import OpenAIEmbeddingFunction, DefaultEmbeddingFunction\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ndefault_ef = DefaultEmbeddingFunction()\ncol = client.create_collection(\"default_ef_collection\",embedding_function=default_ef)\nopenai_ef = OpenAIEmbeddingFunction(api_key=os.getenv(\"OPENAI_API_KEY\"), model_name=\"text-embedding-3-small\")\ncol.add(ids=[f\"{i}\" for i in range(1000)], documents=[f\"document {i}\" for i in range(1000)])\nnewCol = client.get_or_create_collection(\"openai_ef_collection\", embedding_function=openai_ef)\n\nexisting_count = col.count()\nbatch_size = 10\nfor i in range(0, existing_count, batch_size):\n    batch = col.get(include=[\"metadatas\", \"documents\"], limit=batch_size, offset=i)\n    newCol.add(ids=batch[\"ids\"], documents=batch[\"documents\"], metadatas=batch[\"metadatas\"])\n# get first 10 documents with their OpenAI embeddings\nprint(newCol.get(offset=0, limit=10,include=[\"metadatas\", \"documents\", \"embeddings\"]))\n</code></pre>"},{"location":"core/collections/#cloning-a-subset-of-a-collection-with-query","title":"Cloning a subset of a collection with query","text":"<p>The below example demonstrates how to select a slice of an existing collection by using <code>where</code> and <code>where_document</code> query and creating a new collection with the selected slice.</p> <p>Race Condition</p> <p>The below example is not atomic and if data is changed between the initial selection query (<code>select_ids = col.get(...)</code> and the subsequent insertion query (<code>batch = col.get(...)</code>) the new collection may not contain the expected data.</p> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")  # or HttpClient()\ncol = client.get_or_create_collection(\"test\")  # create a new collection with L2 (default)\n\ncol.add(ids=[f\"{i}\" for i in range(1000)], documents=[f\"document {i}\" for i in range(1000)])\nnewCol = client.get_or_create_collection(\"test1\", configuration={\n    \"hnsw\": {\"space\": \"cosine\", \"max_neighbors\": 32}})\nquery_where = {\"metadata_key\": \"value\"}\nquery_where_document = {\"$contains\": \"document\"}\nselect_ids = col.get(where_document=query_where_document, where=query_where, include=[])  # get only IDs\nbatch_size = 10\nfor i in range(0, len(select_ids[\"ids\"]), batch_size):\n    batch = col.get(include=[\"metadatas\", \"documents\", \"embeddings\"], limit=batch_size, offset=i, where=query_where,\n                    where_document=query_where_document)\n    newCol.add(ids=batch[\"ids\"], documents=batch[\"documents\"], metadatas=batch[\"metadatas\"],\n               embeddings=batch[\"embeddings\"])\n\nprint(newCol.count())\nprint(newCol.get(offset=0, limit=10))  # get first 10 documents\n</code></pre>"},{"location":"core/collections/#updating-documentrecord-metadata","title":"Updating Document/Record Metadata","text":"<p>In this example we loop through all documents of a collection and strip all metadata fields of leading and trailing whitespace. Change the <code>update_metadata</code> function to suit your needs.</p> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")\ncol = client.get_or_create_collection(\"test\")\ncount = col.count()\n\n\ndef update_metadata(metadata: dict):\n    return {k: v.strip() for k, v in metadata.items()}\n\n\nfor i in range(0, count, 10):\n    batch = col.get(include=[\"metadatas\"], limit=10, offset=i)\n    col.update(ids=batch[\"ids\"], metadatas=[update_metadata(metadata) for metadata in batch[\"metadatas\"]])\n</code></pre>"},{"location":"core/collections/#tips-and-tricks","title":"Tips and Tricks","text":""},{"location":"core/collections/#getting-ids-only","title":"Getting IDs Only","text":"<p>The below example demonstrates how to get only the IDs of a collection. This is useful if you need to work with IDs without the need to fetch any additional data. Chroma will accept and empty <code>include</code> array indicating that no other data than the IDs is returned.</p> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")\ncol = client.get_or_create_collection(\"my_collection\")\nids_only_result = col.get(include=[])\nprint(ids_only_result['ids'])\n</code></pre>"},{"location":"core/concepts/","title":"Concepts","text":"<p>This page has two tracks:</p> <ul> <li>For General Users</li> <li>For Power Users</li> </ul> <p>If you're new to Chroma, start with For General Users.</p>"},{"location":"core/concepts/#for-general-users","title":"For General Users","text":""},{"location":"core/concepts/#tenancy-and-db-hierarchies","title":"Tenancy and DB Hierarchies","text":"<p>The following picture illustrates the tenancy and DB hierarchy in Chroma:</p> <p></p> <p>Quick mental model:</p> <ul> <li>Tenant = the top-level account or organization boundary</li> <li>Database = a project/app space inside that tenant</li> <li>Collection = a dataset (your searchable records) inside that database</li> </ul> <p>Storage</p> <p>In Chroma single-node, all data about tenancy, databases, collections and documents is stored in a single SQLite database.</p>"},{"location":"core/concepts/#tenants","title":"Tenants","text":"<p>A tenant is the top-level container for data isolation. In practice, this is usually one team, company, or app owner.</p> <p>Example: <code>acme-inc</code> as one tenant.</p>"},{"location":"core/concepts/#databases","title":"Databases","text":"<p>A database is a project space inside a tenant. One database can hold many collections.</p> <p>Example: inside tenant <code>acme-inc</code>, you might have databases <code>support-bot</code>, <code>website-search</code>, and <code>analytics-rag</code>.</p>"},{"location":"core/concepts/#collections","title":"Collections","text":"<p>A collection is the dataset you query. It stores records (IDs, documents, metadata, embeddings) together.</p> <p>Example: <code>support_articles_v1</code>.</p>"},{"location":"core/concepts/#documents","title":"Documents","text":"<p>Chunks of text</p> <p>Documents in ChromaDB lingo are chunks of text that fit within the embedding model's context window. Unlike other frameworks that use the term \"document\" to mean a file, ChromaDB uses the term \"document\" to mean a chunk of text.</p> <p>Documents are raw chunks of text that are associated with an embedding. Documents are stored in the database and can be queried.</p> <p>Example document:</p> <pre><code>\"To reset SSO, rotate your IdP certificate and re-run domain verification.\"\n</code></pre>"},{"location":"core/concepts/#metadata","title":"Metadata","text":"<p>Metadata is a dictionary of key-value pairs associated with an embedding.</p> <p>Example metadata:</p> <pre><code>{\n  \"product_area\": \"auth\",\n  \"status\": \"published\",\n  \"year\": 2025\n}\n</code></pre> <p>Metadata values can be:</p> <ul> <li>strings</li> <li>integers</li> <li>floats (<code>float32</code>)</li> <li>booleans</li> <li>arrays of strings, integers, floats, or booleans (<code>Chroma &gt;= 1.5.0</code>)</li> </ul> <p>Array metadata constraints:</p> <ul> <li>all elements must be the same type</li> <li>empty arrays are not allowed</li> <li>nested arrays are not supported</li> </ul> <p>See Array Metadata for examples with <code>$contains</code> / <code>$not_contains</code>.</p> <p>Runnable filter examples:</p> <ul> <li>Python</li> <li>TypeScript</li> <li>Rust</li> </ul>"},{"location":"core/concepts/#embedding-function","title":"Embedding Function","text":"<p>An embedding function is the model/API that turns text into vectors.</p> <p>You run it on:</p> <ul> <li>documents (when writing data)</li> <li>queries (when searching)</li> </ul> <p>See Chroma's official embedding integrations.</p>"},{"location":"core/concepts/#embeddings","title":"Embeddings","text":"<p>An embedding is the numeric vector representation of text, typically a list of <code>float32</code> values.</p> <p>You can think of it as a machine-friendly fingerprint of meaning.</p>"},{"location":"core/concepts/#distance-function","title":"Distance Function","text":"<p>Distance functions define similarity between vectors:</p> <ul> <li>Cosine: common for semantic text similarity</li> <li>Euclidean (<code>l2</code>): geometric distance</li> <li>Inner Product (<code>ip</code>): common in recommendation-like scenarios</li> </ul> <p>In most query outputs, lower distance means closer match.</p>"},{"location":"core/concepts/#search-concepts","title":"Search Concepts","text":"<p>Most search systems follow this flow: eligible pool -&gt; ranked list -&gt; optional fusion -&gt; optional grouping -&gt; returned page.</p> <p>Use this running example:</p> <ul> <li>Query intent: \"Find troubleshooting docs about SSO login failures.\"</li> <li>Constraints: only <code>status=published</code> from <code>year &gt;= 2024</code>.</li> <li>Output goal: top 20 results (<code>title</code> + <code>score</code>), without one product area dominating.</li> </ul> Stage What it decides Chroma concept 1. Candidate selection Which records are allowed to compete <code>where</code>, <code>where_document</code> 2. Relevance ranking Which eligible records appear first <code>Knn</code> / ranking expressions 3. Hybrid fusion (optional) How multiple ranked lists are combined <code>Rrf</code> 4. Diversity / dedup (optional) How many records to keep per bucket <code>GroupBy</code> + <code>MinK</code> / <code>MaxK</code> 5. Response shaping How much and which fields to return <code>limit</code>, pagination, <code>select</code>"},{"location":"core/concepts/#1-filters-decide-eligibility-not-relevance","title":"1) Filters decide eligibility (not relevance)","text":"<p>Filters answer: \"Can this record be considered?\"</p> <ul> <li><code>where</code> filters metadata (for example <code>status=published</code>, <code>year &gt;= 2024</code>).</li> <li><code>where_document</code> filters document text content.</li> <li>Filters remove non-matching records, but they do not define final ranking order.</li> </ul>"},{"location":"core/concepts/#2-ranking-decides-order-among-eligible-records","title":"2) Ranking decides order among eligible records","text":"<p>Ranking answers: \"Among the records that passed filters, which are most relevant?\"</p> <ul> <li><code>Knn</code> computes similarity/distance ordering for eligible records.</li> <li>In Search API scoring, lower scores represent better matches.</li> </ul> <p>For the running example, ranking pushes SSO/login-related incidents closest to the top after the eligibility filters are applied.</p>"},{"location":"core/concepts/#3-rrf-fuses-multiple-rankings-when-score-scales-differ","title":"3) RRF fuses multiple rankings when score scales differ","text":"<p>Hybrid fusion answers: \"How do we merge strong semantic matches with strong keyword matches?\"</p> <ul> <li><code>Rrf</code> combines rankings by position, not raw score magnitude.</li> <li>This is useful when dense and sparse ranking scores are on different scales.</li> </ul> <p>In the running example, dense retrieval might catch \"authentication outage\", while sparse retrieval catches exact tokens like \"SSO\" and \"SAML\"; <code>Rrf</code> blends both lists.</p>"},{"location":"core/concepts/#4-groupingaggregation-shapes-the-final-mix","title":"4) Grouping/aggregation shapes the final mix","text":"<p>Grouping answers: \"How do we avoid one category dominating the top results?\"</p> <ul> <li><code>GroupBy</code> partitions ranked results by one or more keys.</li> <li><code>MinK</code> / <code>MaxK</code> keep top-k rows per group before flattening.</li> </ul> <p>In the running example, grouping by <code>product_area</code> with <code>k=2</code> can prevent ten near-duplicate auth incidents from crowding out other useful categories.</p>"},{"location":"core/concepts/#5-response-shaping-controls-what-comes-back","title":"5) Response shaping controls what comes back","text":"<p>Response shaping answers: \"How much should the API return, and which fields do you actually need?\"</p> <ul> <li>Pagination controls page size and offset.</li> <li><code>select</code> controls the returned payload.</li> </ul> <p>This keeps responses smaller and focused for downstream UI or agent use.</p> <p>Example: if search found 2,000 matches, you might return only the first 20 IDs, titles, and scores.</p> <p>See:</p> <ul> <li>Filters for <code>where</code> / <code>where_document</code> syntax and operators.</li> <li>Advanced Search Semantics for execution-stage behavior and tradeoffs.</li> <li>Search API Overview for the full query model.</li> <li>Ranking and Scoring, Hybrid Search with RRF, and Group By &amp; Aggregation for ranking primitives and grouping concepts.</li> <li>Examples &amp; Patterns for concrete Search API implementations.</li> </ul>"},{"location":"core/concepts/#how-data-flows-through-chroma-cloud-distributed-chroma","title":"How Data Flows Through Chroma Cloud (Distributed Chroma)","text":"<p>The animated flows below model Chroma Cloud / distributed Chroma, where gateway, WAL, compaction, and query execution are separate services. In local or single-node deployments, the same logical stages still apply but are often co-located in one process.</p> <p>What this means:</p> <ul> <li>On writes: Chroma saves changes durably first, then updates indexes in the background.</li> <li>On reads: Chroma combines indexed data and recent log data so results stay up to date.</li> </ul>"},{"location":"core/concepts/#write-path-add-update-upsert-delete","title":"Write Path (Add / Update / Upsert / Delete)","text":"Write Path Cloud/distributed: durable first, indexed asynchronously Client Gateway WAL Service Compactor Indexes <p>In distributed Chroma, writes are acknowledged after WAL durability. Compaction materializes new index versions in the background.</p>"},{"location":"core/concepts/#query-path-get-query-search","title":"Query Path (Get / Query / Search)","text":"Query Path Cloud/distributed: index + WAL consistency Client Gateway Query Engine Filter + Plan Vector / FTS / Metadata Ranked Results <p>In distributed Chroma, strongly consistent reads combine indexed state with recent WAL state.</p> <p>The detailed query pipeline is described in Advanced Queries.</p> <p>Implementation-level (code-backed) local-vs-distributed query diagrams are in the For Power Users section below.</p>"},{"location":"core/concepts/#for-power-users","title":"For Power Users","text":"<p>This section is a code-oriented map of distributed Chroma, based on the Rust workspace (<code>rust/</code>) and the distributed architecture docs.</p> <p>If you mostly care about product behavior, you can skip this section. This part is for readers who want to connect concepts to Rust implementation details.</p>"},{"location":"core/concepts/#execution-paths-code-backed","title":"Execution Paths (Code-Backed)","text":"<p>These diagrams are traced from the Rust frontend/segment/log implementation (<code>rust/frontend</code>, <code>rust/segment</code>, <code>rust/log</code>).</p> <p>Tip: treat these as \"what service does what\" maps, not required reading for everyday app development.</p>"},{"location":"core/concepts/#interactive-local-query-path-single-node-sqlite-hnsw","title":"Interactive Local Query Path (Single-Node SQLite + HNSW)","text":"Interactive Local Query Pipeline Single-node: SQLite + HNSW Local <p>Click any stage to inspect what happens in the local executor path. First read triggers backfill/purge into local metadata + HNSW segments.</p> Validation + Resolve Metadata Pre-Filter get SQLite Metadata KNN Search query HNSW Local Metadata Fetch get SQLite Metadata Result Aggregation <p>Selected stage</p> <p>Validation + Segment Resolve</p> <p>Service: FrontendServer + ServiceBasedFrontend</p> <p>Request validation/auth happens in frontend, then collection + segment ids are resolved before plan execution.</p>"},{"location":"core/concepts/#distributed-frontend-dispatch-path-cloud","title":"Distributed Frontend Dispatch Path (Cloud)","text":"Distributed Query Path in Frontend Cloud: frontend dispatches, workers execute Client Frontend API Resolver + Executor gRPC Query Worker Worker Indexes Results <p>In distributed mode, frontend routes Knn/Get/Search plans to query workers over gRPC. Worker-side execution uses distributed segment/index types (HnswDistributed or Spann with blockfile segments), not frontend-local SQLite + HNSW providers.</p> <p>Code references for the two paths:</p> <ul> <li>Local segment types on collection create: <code>Executor::Local</code> creates <code>HnswLocalPersisted</code> + <code>Sqlite</code></li> <li>Local query execution: <code>LocalExecutor</code> uses <code>SqliteMetadataReader</code> + <code>LocalSegmentManager::get_hnsw_reader</code></li> <li>Distributed query execution: <code>DistributedExecutor</code> dispatches <code>knn/get/search</code> via gRPC query clients</li> </ul>"},{"location":"core/concepts/#distributed-architecture-main-services","title":"Distributed Architecture (Main Services)","text":"<ul> <li>Gateway / frontend API service: <code>rust/frontend</code> (receives API calls and dispatches work) (server)</li> <li>Query executor service: <code>rust/worker</code> (runs query operators and orchestrators) (query entrypoint, query server)</li> <li>Compaction service: <code>rust/worker</code> (turns WAL/log history into read-optimized segment versions) (compaction orchestrator)</li> <li>Write-ahead log: <code>rust/wal3</code> (durable append-only change log) (design README)</li> <li>Garbage collector service: <code>rust/garbage_collector</code> (cleans old index/log artifacts safely) (orchestrator)</li> </ul> <p>See also the official architecture doc: Distributed Chroma Architecture.</p>"},{"location":"core/concepts/#main-primitives-and-index-families","title":"Main Primitives and Index Families","text":"<p>At the segment/type level (<code>rust/types/src/segment.rs</code>), distributed Chroma uses segment types such as:</p> <ul> <li><code>BlockfileMetadata</code>, <code>BlockfileRecord</code></li> <li><code>HnswDistributed</code></li> <li><code>Spann</code>, <code>QuantizedSpann</code></li> <li><code>Sqlite</code></li> </ul> <p>In simple terms:</p> <ul> <li><code>Blockfile*</code> segments store compacted record/metadata data.</li> <li><code>HnswDistributed</code> / <code>Spann</code> / <code>QuantizedSpann</code> are vector-search structures.</li> <li><code>Sqlite</code> is still used for specific metadata/system concerns.</li> </ul> <p>And at the index crate level (<code>rust/index/src</code>), major families include:</p> <ul> <li>Vector ANN: <code>hnsw</code>, <code>spann</code>, <code>quantized_spann</code></li> <li>Full text: <code>fulltext</code></li> <li>Metadata: <code>metadata</code></li> <li>Sparse retrieval support: <code>sparse</code></li> </ul>"},{"location":"core/concepts/#spann-in-distributed-chroma","title":"SPANN in Distributed Chroma","text":"<p>Core implementation: <code>rust/index/src/spann/types.rs</code>.</p> <p>Quick intuition: instead of searching one giant graph, SPANN first searches cluster centers, then looks inside the best matching posting lists.</p> <p>Operationally, SPANN combines:</p> <ul> <li>a head/center ANN structure (HNSW over centers)</li> <li>posting lists keyed by center/head id (blockfile-backed)</li> <li>a versions map (<code>doc_offset_id -&gt; version</code>) to filter stale entries</li> <li>a persisted <code>max_head_id</code> for deterministic head allocation across compactions</li> </ul> <p>The write-side behavior includes:</p> <ul> <li>add/update/delete on posting lists and versions map</li> <li>splitting oversized posting lists into new heads</li> <li>reassigning points to nearby heads after split/merge operations</li> <li>optional garbage collection policies:</li> <li>posting-list random-sample cleanup</li> <li>HNSW full rebuild or delete-percentage-triggered rebuild</li> </ul>"},{"location":"core/concepts/#blockfile-format-and-update-model","title":"Blockfile Format and Update Model","text":"<p>Core implementation: <code>rust/blockstore/src/arrow</code>.</p> <p>Quick intuition: blockfiles are immutable Arrow-backed data blocks with copy-on-write updates, so reads stay stable while writes build new versions.</p> <p>Production blockfiles are Arrow-backed and use:</p> <ul> <li>immutable blocks for persisted data</li> <li>an in-memory sparse index mapping key ranges to block ids</li> <li>writer-side deltas for mutation batching (<code>set</code>/<code>delete</code>)</li> <li>copy-on-write for updates via <code>fork(...)</code></li> </ul> <p>Update lifecycle:</p> <ol> <li>Writer mutates deltas and may split blocks when over target block size.</li> <li>Sparse index is updated to point at new block ids.</li> <li><code>commit()</code> converts deltas into immutable blocks and prepares a flusher.</li> <li><code>flush()</code> persists blocks, then atomically persists root metadata/sparse index.</li> </ol> <p>Relevant code:</p> <ul> <li>provider</li> <li>blockfile writer</li> <li>flusher</li> <li>root + sparse index</li> </ul>"},{"location":"core/concepts/#compaction-and-registration","title":"Compaction and Registration","text":"<p>Compaction in distributed mode (<code>rust/worker/src/execution/orchestration/compact.rs</code>) follows an explicit staged flow:</p> <ul> <li>fetch and materialize logs</li> <li>apply to segment writers (record/metadata/vector)</li> <li>commit and flush segment artifacts</li> <li>register new segment metadata and offsets in sysdb/log metadata</li> </ul> <p>This is the core bridge from WAL durability to read-optimized segment versions.</p> <p>What this means: compaction is the \"make recent writes fast to read\" job.</p>"},{"location":"core/concepts/#garbage-collection-index-files-wal","title":"Garbage Collection (Index Files + WAL)","text":"<p>Two GC tracks run in the Rust implementation:</p> <ul> <li>Segment/index artifact GC in <code>rust/garbage_collector</code>:</li> <li>construct collection version graph (including fork dependencies)</li> <li>compute versions to delete using cutoff + min-versions retention</li> <li>compute unreferenced files and clean up (dry-run / rename / delete)</li> <li>WAL GC in <code>wal3</code>:</li> <li>three-phase GC flow (compute garbage, manifest synchronization, delete)</li> <li>cursor-driven safety so required log ranges remain pinned</li> </ul> <p>What this means: GC removes storage that is no longer needed, but only after safety checks confirm active readers won't break.</p> <p>Relevant code:</p> <ul> <li>version graph construction</li> <li>version deletion policy</li> <li>unused file cleanup</li> <li>unused WAL cleanup</li> </ul>"},{"location":"core/configuration/","title":"Configuration","text":""},{"location":"core/configuration/#10-configuration-current","title":"1.0 Configuration (Current)","text":"<p>Starting with Chroma 1.0, collection index settings are configured via the <code>configuration</code> dict parameter at collection creation time. This replaces the legacy <code>metadata</code>-based approach.</p> <pre><code>collection = client.create_collection(\n    \"my_collection\",\n    configuration={\n        \"hnsw\": {\n            \"space\": \"cosine\",\n            \"ef_construction\": 200,\n            \"max_neighbors\": 32,\n        }\n    },\n)\n</code></pre> <p>Configuration vs Metadata</p> <p>The <code>configuration</code> dict is separate from <code>metadata</code>. Metadata is for user-defined key-value pairs. Configuration controls the vector index behavior. You cannot specify both <code>hnsw</code> and <code>spann</code> in the same configuration - only one index type is allowed per collection.</p>"},{"location":"core/configuration/#hnsw-index-configuration","title":"HNSW Index Configuration","text":"<p>HNSW (Hierarchical Navigable Small World) is the default vector index for Chroma. It provides fast approximate nearest neighbor search for single-node and self-hosted deployments.</p>"},{"location":"core/configuration/#parameters","title":"Parameters","text":"Parameter Description Default Constraints Mutable <code>space</code> Distance metric <code>l2</code> <code>l2</code>, <code>cosine</code>, <code>ip</code> No <code>ef_construction</code> Neighbors explored during index build <code>100</code> Positive integer No <code>ef_search</code> Neighbors explored during search <code>100</code> Positive integer Yes <code>max_neighbors</code> Max connections per node (M parameter) <code>16</code> Positive integer No <code>num_threads</code> Threads used by HNSW CPU cores Positive integer Yes <code>resize_factor</code> Graph growth rate when capacity is reached <code>1.2</code> Positive float Yes <code>batch_size</code> In-memory bruteforce index size before HNSW flush <code>100</code> &gt;= 2 Yes <code>sync_threshold</code> Threshold for syncing HNSW index to disk <code>1000</code> &gt;= 2 Yes"},{"location":"core/configuration/#examples","title":"Examples","text":"PythonTypeScriptGo <p>Create with configuration:</p> <pre><code>import chromadb\n\nclient = chromadb.HttpClient()  # or PersistentClient()\ncollection = client.create_collection(\n    \"my_collection\",\n    configuration={\n        \"hnsw\": {\n            \"space\": \"cosine\",\n            \"ef_construction\": 200,\n            \"ef_search\": 100,\n            \"max_neighbors\": 32,\n            \"num_threads\": 4,\n            \"resize_factor\": 1.2,\n            \"batch_size\": 100,\n            \"sync_threshold\": 1000,\n        }\n    },\n)\n</code></pre> <p>Update mutable parameters after creation:</p> <pre><code>collection.modify(\n    configuration={\n        \"hnsw\": {\n            \"ef_search\": 200,\n            \"num_threads\": 8,\n        }\n    }\n)\n</code></pre> <pre><code>import { ChromaClient } from \"chromadb\";\n\nconst client = new ChromaClient();\nconst collection = await client.createCollection({\n    name: \"my_collection\",\n    configuration: {\n        hnsw: {\n            space: \"cosine\",\n            ef_construction: 200,\n            ef_search: 100,\n            max_neighbors: 32,\n            num_threads: 4,\n            resize_factor: 1.2,\n            batch_size: 100,\n            sync_threshold: 1000,\n        },\n    },\n});\n</code></pre> <pre><code>package main\n\nimport (\n    \"context\"\n    chroma \"github.com/amikos-tech/chroma-go\"\n    \"github.com/amikos-tech/chroma-go/types\"\n)\n\nfunc main() {\n    ctx := context.Background()\n    client, _ := chroma.NewHTTPClient(ctx,\n        chroma.WithDefaultDatabase(\"default_database\"),\n        chroma.WithDefaultTenant(\"default_tenant\"),\n    )\n    col, _ := client.CreateCollection(ctx, \"my_collection\", false,\n        types.WithHNSWConfiguration(\n            types.WithSpace(\"cosine\"),\n            types.WithEfConstruction(200),\n            types.WithEfSearch(100),\n            types.WithMaxNeighbors(32),\n        ),\n    )\n}\n</code></pre>"},{"location":"core/configuration/#spann-index-configuration","title":"SPANN Index Configuration","text":"<p>Chroma Cloud Only</p> <p>SPANN is the vector index used in Chroma Cloud and distributed Chroma deployments. It is not available in single-node self-hosted Chroma. If you are running Chroma locally, use HNSW configuration instead.</p> <p>SPANN (Space Partition tree AND graph based Nearest neighbor search) is Chroma's distributed vector index, based on the SPFresh paper. It is designed for large-scale datasets where the full index cannot fit in a single machine's memory.</p>"},{"location":"core/configuration/#how-spann-works","title":"How SPANN Works","text":"<pre><code>                           SPANN Architecture\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502                    Query: \"find similar\"                   \u2502\n  \u2502                           \u2502                                \u2502\n  \u2502                           \u25bc                                \u2502\n  \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502\n  \u2502              \u2502   HNSW Centroid     \u2502  O(log C) lookup      \u2502\n  \u2502              \u2502   Index             \u2502  C = num centroids    \u2502\n  \u2502              \u2502  \u250c\u2500\u2500\u2510 \u250c\u2500\u2500\u2510 \u250c\u2500\u2500\u2510    \u2502                       \u2502\n  \u2502              \u2502  \u2502c1\u2502\u2500\u2502c2\u2502\u2500\u2502c3\u2502\u00b7\u00b7\u00b7 \u2502                       \u2502\n  \u2502              \u2502  \u2514\u2500\u2500\u2518 \u2514\u2500\u2500\u2518 \u2514\u2500\u2500\u2518    \u2502                       \u2502\n  \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u2502\n  \u2502                       \u2502 top-K centroids                    \u2502\n  \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502\n  \u2502              \u2502  Probe posting     \u2502  search_nprobe         \u2502\n  \u2502              \u2502  lists for each    \u2502  centroids probed      \u2502\n  \u2502              \u2502  selected centroid \u2502                        \u2502\n  \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u2502\n  \u2502                       \u2502                                    \u2502\n  \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n  \u2502      \u25bc                \u25bc                \u25bc                   \u2502\n  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n  \u2502  \u2502Posting \u2502      \u2502Posting \u2502      \u2502Posting \u2502              \u2502\n  \u2502  \u2502List c1 \u2502      \u2502List c2 \u2502      \u2502List c3 \u2502   Blockfile  \u2502\n  \u2502  \u2502\u250c\u2500\u2510\u250c\u2500\u2510 \u2502      \u2502\u250c\u2500\u2510\u250c\u2500\u2510 \u2502      \u2502\u250c\u2500\u2510\u250c\u2500\u2510 \u2502   storage    \u2502\n  \u2502  \u2502\u2502d\u2502\u2502d\u2502\u2026\u2502      \u2502\u2502d\u2502\u2502d\u2502\u2026\u2502      \u2502\u2502d\u2502\u2502d\u2502\u2026\u2502              \u2502\n  \u2502  \u2502\u2514\u2500\u2518\u2514\u2500\u2518 \u2502      \u2502\u2514\u2500\u2518\u2514\u2500\u2518 \u2502      \u2502\u2514\u2500\u2518\u2514\u2500\u2518 \u2502              \u2502\n  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n  \u2502                       \u2502                                    \u2502\n  \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502\n  \u2502              \u2502  Score, dedupe,    \u2502                        \u2502\n  \u2502              \u2502  return results    \u2502                        \u2502\n  \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n  SPFresh Maintenance (automatic):\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502  Posting list too large (&gt; split_threshold)?              \u2502\n  \u2502    \u2192 Split via balanced 2-means clustering                \u2502\n  \u2502    \u2192 New centroid added to HNSW index                     \u2502\n  \u2502    \u2192 Nearby points reassigned for better recall           \u2502\n  \u2502                                                           \u2502\n  \u2502  Posting list too small (&lt; merge_threshold)?              \u2502\n  \u2502    \u2192 Merge with nearest neighbor centroid                  \u2502\n  \u2502    \u2192 Old centroid removed from HNSW index                 \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key concepts:</p> <ul> <li>Centroids are cluster representatives stored in a small HNSW graph for fast lookup</li> <li>Posting lists store the actual document embeddings grouped by their nearest centroid</li> <li>Multi-posting allows a document to appear in multiple posting lists for better recall</li> <li>SPFresh maintenance automatically splits large clusters and merges small ones to keep cluster sizes balanced</li> </ul>"},{"location":"core/configuration/#parameters_1","title":"Parameters","text":"Parameter Description Default Constraints Mutable <code>space</code> Distance metric <code>l2</code> <code>l2</code>, <code>cosine</code>, <code>ip</code> No <code>search_nprobe</code> Number of centroids probed during search <code>64</code> Max 128 Yes <code>write_nprobe</code> Number of centroids considered during insert <code>32</code> Max 128 No <code>ef_construction</code> HNSW build effort for centroid index <code>200</code> Max 200 No <code>ef_search</code> HNSW search effort for centroid index <code>200</code> Max 200 Yes <code>max_neighbors</code> Max connections in centroid HNSW graph <code>64</code> Max 64 No <code>reassign_neighbor_count</code> Nearby clusters checked during split reassignment <code>64</code> Max 64 No <code>split_threshold</code> Posting list size that triggers a split <code>50</code> 25 - 200 No <code>merge_threshold</code> Posting list size that triggers a merge <code>25</code> 12 - 100 No <p>Tuning Guidance</p> <ul> <li><code>search_nprobe</code> is the primary knob for search quality vs latency. Higher values improve recall   but increase search time. Start with the default (64) and adjust based on your recall requirements.</li> <li><code>split_threshold</code> and <code>merge_threshold</code> control cluster granularity. Smaller split thresholds   create more, smaller clusters (better for high-dimensional data). The merge threshold should always   be less than the split threshold.</li> <li><code>space</code> must match your embedding model's expected distance metric. Use <code>cosine</code> for normalized   embeddings (most common), <code>l2</code> for Euclidean distance, or <code>ip</code> for inner product.</li> </ul>"},{"location":"core/configuration/#examples_1","title":"Examples","text":"PythonTypeScriptGo <p>Create with SPANN configuration:</p> <pre><code>import chromadb\n\nclient = chromadb.CloudClient(\n    tenant=\"my-tenant\",\n    database=\"my-database\",\n)\ncollection = client.create_collection(\n    \"my_collection\",\n    configuration={\n        \"spann\": {\n            \"space\": \"cosine\",\n            \"search_nprobe\": 64,\n            \"ef_search\": 200,\n        }\n    },\n)\n</code></pre> <p>Update mutable parameters after creation:</p> <pre><code>collection.modify(\n    configuration={\n        \"spann\": {\n            \"search_nprobe\": 96,\n            \"ef_search\": 200,\n        }\n    }\n)\n</code></pre> <pre><code>import { ChromaClient } from \"chromadb\";\n\nconst client = new ChromaClient({\n    // Cloud client configuration\n});\nconst collection = await client.createCollection({\n    name: \"my_collection\",\n    configuration: {\n        spann: {\n            space: \"cosine\",\n            search_nprobe: 64,\n            ef_search: 200,\n        },\n    },\n});\n</code></pre> <pre><code>package main\n\nimport (\n    \"context\"\n    chroma \"github.com/amikos-tech/chroma-go/v2\"\n    v2 \"github.com/amikos-tech/chroma-go/v2/pkg/api/v2\"\n)\n\nfunc main() {\n    ctx := context.Background()\n    client, _ := chroma.NewCloudClient(ctx,\n        chroma.WithTenant(\"my-tenant\"),\n        chroma.WithDatabase(\"my-database\"),\n    )\n    col, _ := client.CreateCollection(ctx, \"my_collection\",\n        v2.WithVectorIndexCreate(v2.NewVectorIndexConfig(\n            v2.WithSpace(v2.SpaceCosine),\n            v2.WithSpann(v2.NewSpannConfig(\n                v2.WithSpannSearchNprobe(64),\n                v2.WithSpannEfSearch(200),\n            )),\n        )),\n    )\n}\n</code></pre>"},{"location":"core/configuration/#embedding-function-configuration","title":"Embedding Function Configuration","text":"<p>Starting with Chroma v1.1.13, embedding functions are persisted server-side. You can configure the embedding function at collection creation time, and it will be automatically used on subsequent <code>get_collection</code> calls.</p> <p>Embedding function <code>name</code>/<code>config</code> payloads are defined by upstream schemas:</p> <ul> <li>Embedding Function Schemas</li> <li>Schema README</li> </ul> PythonTypeScriptGo <p>Set via argument (recommended):</p> <pre><code>from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n\nef = OpenAIEmbeddingFunction(model_name=\"text-embedding-3-small\")\ncollection = client.create_collection(\"my_collection\", embedding_function=ef)\n\n# On subsequent access, the EF is auto-resolved:\ncollection = client.get_collection(\"my_collection\")  # no ef needed\n</code></pre> <p>Set via configuration dict:</p> <pre><code>collection = client.create_collection(\n    \"my_collection\",\n    configuration={\n        \"embedding_function\": {\n            \"name\": \"openai\",\n            \"config\": {\n                \"model_name\": \"text-embedding-3-small\",\n                \"api_key_env_var\": \"OPENAI_API_KEY\",\n            },\n        }\n    },\n)\n</code></pre> <p>Custom API key environment variable:</p> <pre><code>ef = OpenAIEmbeddingFunction(\n    model_name=\"text-embedding-3-small\",\n    api_key_env_var=\"MY_CUSTOM_OPENAI_KEY\",  # defaults to OPENAI_API_KEY\n)\ncollection = client.create_collection(\"my_collection\", embedding_function=ef)\n</code></pre> <pre><code>const collection = await client.createCollection({\n    name: \"my_collection\",\n    configuration: {\n        embedding_function: {\n            name: \"openai\",\n            config: {\n                model_name: \"text-embedding-3-small\",\n                apiKeyEnvVar: \"OPENAI_API_KEY\",\n            },\n        },\n    },\n});\n</code></pre> <pre><code>package main\n\nimport (\n    \"context\"\n    \"os\"\n\n    chroma \"github.com/amikos-tech/chroma-go/v2\"\n    v2 \"github.com/amikos-tech/chroma-go/v2/pkg/api/v2\"\n    openai \"github.com/amikos-tech/chroma-go/v2/pkg/embeddings/openai\"\n)\n\nfunc main() {\n    ctx := context.Background()\n    client, _ := chroma.NewHTTPClient(ctx,\n        chroma.WithDefaultDatabase(\"default_database\"),\n        chroma.WithDefaultTenant(\"default_tenant\"),\n    )\n\n    ef, _ := openai.NewOpenAIEmbeddingFunction(os.Getenv(\"OPENAI_API_KEY\"),\n        openai.WithModel(openai.TextEmbedding3Small),\n    )\n    col, _ := client.CreateCollection(ctx, \"my_collection\",\n        v2.WithEmbeddingFunctionCreate(ef),\n    )\n}\n</code></pre>"},{"location":"core/configuration/#1x-server-configuration","title":"1.x Server Configuration","text":"<p>Chroma 1.x is configured via a YAML configuration file. The server loads configuration from the path specified by the <code>CONFIG_PATH</code> environment variable. Individual settings can be overridden with <code>CHROMA_</code>-prefixed environment variables (use <code>__</code> for nested properties).</p> <pre><code># Start with a custom config file\nCONFIG_PATH=/etc/chroma/config.yaml chroma run\n\n# Override individual settings via environment variables\nCHROMA_PORT=9000 chroma run\nCHROMA_CORS_ALLOW_ORIGINS='[\"*\"]' chroma run\nCHROMA_ALLOW_RESET=true chroma run\n</code></pre>"},{"location":"core/configuration/#http-server-settings","title":"HTTP Server Settings","text":"Parameter Description Default Env Override <code>port</code> HTTP server port <code>8000</code> <code>CHROMA_PORT</code> <code>listen_address</code> Network interface to bind to <code>0.0.0.0</code> (all interfaces) <code>CHROMA_LISTEN_ADDRESS</code> <code>max_payload_size_bytes</code> Maximum request body size in bytes <code>41943040</code> (40 MB) <code>CHROMA_MAX_PAYLOAD_SIZE_BYTES</code> <code>cors_allow_origins</code> List of allowed CORS origins. Use <code>[\"*\"]</code> to allow any origin None (CORS disabled) <code>CHROMA_CORS_ALLOW_ORIGINS</code> <code>persist_path</code> Directory for Chroma data files (SQLite DB, HNSW indices) <code>./chroma</code> <code>CHROMA_PERSIST_PATH</code>"},{"location":"core/configuration/#general-settings","title":"General Settings","text":"Parameter Description Default Env Override <code>allow_reset</code> Enable the <code>DELETE /reset</code> endpoint, which deletes all data. Should be <code>false</code> in production <code>false</code> <code>CHROMA_ALLOW_RESET</code> <code>default_knn_index</code> Default vector index type for new collections. Values: <code>hnsw</code> (single-node), <code>spann</code> (distributed/cloud) <code>hnsw</code> <code>CHROMA_DEFAULT_KNN_INDEX</code> <code>enable_schema</code> Enable server-side schema validation for collection operations <code>true</code> <code>CHROMA_ENABLE_SCHEMA</code>"},{"location":"core/configuration/#sqlite-settings","title":"SQLite Settings","text":"Parameter Description Default Env Override <code>sqlitedb.hash_type</code> Hash algorithm for verifying migration file integrity. Values: <code>md5</code>, <code>sha256</code>. Use <code>sha256</code> if your organization prohibits MD5 <code>md5</code> <code>CHROMA_SQLITEDB__HASH_TYPE</code> <code>sqlitedb.migration_mode</code> How schema migrations are handled on startup. Values: <code>apply</code> (run pending migrations), <code>validate</code> (check migrations are applied, fail if not) <code>apply</code> <code>CHROMA_SQLITEDB__MIGRATION_MODE</code>"},{"location":"core/configuration/#opentelemetry","title":"OpenTelemetry","text":"<p>Chroma 1.x emits traces via OpenTelemetry (OTLP gRPC). Tracing is disabled by default. Set the <code>open_telemetry</code> section to enable it.</p> Parameter Description Default Env Override <code>open_telemetry.endpoint</code> OTLP gRPC collector endpoint. Tracing is disabled when not set None (disabled) <code>CHROMA_OPEN_TELEMETRY__ENDPOINT</code> <code>open_telemetry.service_name</code> Service name attached to all emitted spans <code>chromadb</code> <code>CHROMA_OPEN_TELEMETRY__SERVICE_NAME</code> <code>open_telemetry.filters</code> Per-crate log level filters. Each entry has <code>crate_name</code> (Rust crate to filter) and <code>filter_level</code>. Levels: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code> <code>[{crate_name: \"chroma_frontend\", filter_level: \"trace\"}]</code> \u2014"},{"location":"core/configuration/#example-configurations","title":"Example Configurations","text":"<p>Minimal single-node:</p> <pre><code>persist_path: \"./chroma\"\n</code></pre> <p>Single-node with all common options:</p> <pre><code># HTTP server\nport: 8000\nlisten_address: \"0.0.0.0\"\nmax_payload_size_bytes: 41943040\ncors_allow_origins: [\"*\"]\n\n# Storage\npersist_path: \"./chroma\"\nallow_reset: false\n\n# SQLite\nsqlitedb:\n  hash_type: \"md5\"\n  migration_mode: \"apply\"\n\n# Telemetry (optional)\nopen_telemetry:\n  service_name: \"chroma\"\n  endpoint: \"http://otel-collector:4317\"\n  filters:\n    - crate_name: \"chroma_frontend\"\n      filter_level: \"trace\"\n</code></pre> <p>Docker single-node:</p> <pre><code>persist_path: \"/data\"\n</code></pre> <p>Environment Variable Nesting</p> <p>Nested YAML properties are overridden using <code>__</code> as a separator. For example, <code>sqlitedb.hash_type</code> becomes <code>CHROMA_SQLITEDB__HASH_TYPE</code>.</p>"},{"location":"core/configuration/#pre-10-configuration-legacy","title":"Pre-1.0 Configuration (Legacy)","text":"<p>Legacy Configuration</p> <p>The configuration options below apply to Chroma versions prior to 1.0. For Chroma 1.0+, use the <code>configuration</code> dict parameter described above. Legacy <code>metadata</code>-based HNSW configuration (e.g. <code>metadata={\"hnsw:space\": \"cosine\"}</code>) is still supported for backwards compatibility but is deprecated.</p>"},{"location":"core/configuration/#common-configurations-options","title":"Common Configurations Options","text":""},{"location":"core/configuration/#server-configuration","title":"Server Configuration","text":""},{"location":"core/configuration/#core","title":"Core","text":""},{"location":"core/configuration/#is_persistent","title":"<code>IS_PERSISTENT</code>","text":"<p>Defines whether Chroma should persist data or not.</p> <p>Possible values:</p> <ul> <li><code>TRUE</code></li> <li><code>FALSE</code></li> </ul> <p>Default: <code>FALSE</code></p> <p>How to use:</p> CLIPythonDocker <pre><code>export IS_PERSISTENT=TRUE\nchroma run --path ./chroma\n</code></pre> <pre><code>from chromadb.config import Settings\nsettings = Settings(is_persistent=True)\n# run the server with the settings\n</code></pre> <pre><code>docker run -d --rm --name chromadb -v ./chroma:/chroma/chroma -e IS_PERSISTENT=TRUE chromadb/chroma:0.6.3\n</code></pre>"},{"location":"core/configuration/#persist_directory","title":"<code>PERSIST_DIRECTORY</code>","text":"<p>Defines the directory where Chroma should persist data. This can be relative or absolute path. The directory must be writeable to Chroma process.</p> <p>Default: <code>./chroma</code></p>"},{"location":"core/configuration/#allow_reset","title":"<code>ALLOW_RESET</code>","text":"<p>Defines whether Chroma should allow resetting the index (delete all data).</p> <p>Possible values:</p> <ul> <li><code>TRUE</code></li> <li><code>FALSE</code></li> </ul> <p>Default: <code>FALSE</code></p>"},{"location":"core/configuration/#chroma_memory_limit_bytes","title":"<code>CHROMA_MEMORY_LIMIT_BYTES</code>","text":""},{"location":"core/configuration/#chroma_segment_cache_policy","title":"<code>CHROMA_SEGMENT_CACHE_POLICY</code>","text":""},{"location":"core/configuration/#telemetry-and-observability","title":"Telemetry and Observability","text":"<p>In the current Chroma version (as of time or writing <code>0.6.3</code>) the only type of telemetry supported are traces.</p> <p>The following configuration options allow you to configure the tracing service that accepts OpenTelemetry traces via the OLTP GRPC endpoint.</p> <p>In addition to traces Chroma also performs anonymized product telemetry. The product telemetry is enabled by default.</p>"},{"location":"core/configuration/#chroma_otel_collection_endpoint","title":"<code>CHROMA_OTEL_COLLECTION_ENDPOINT</code>","text":"<p>Defines the endpoint of the tracing service that accepts OpenTelemetry traces via the OLTP GRPC endpoint.</p> <p>Value type: <code>Valid URL</code></p> <p>Default: None</p> <p>Example:</p> <pre><code>export CHROMA_OTEL_COLLECTION_ENDPOINT=http://localhost:4317\n</code></pre>"},{"location":"core/configuration/#chroma_otel_service_name","title":"<code>CHROMA_OTEL_SERVICE_NAME</code>","text":"<p>Defines the name of the service that will be used in the tracing service.</p> <p>Default: <code>chroma</code></p> <p>Example:</p> <pre><code>export CHROMA_OTEL_SERVICE_NAME=chroma-dev\n</code></pre>"},{"location":"core/configuration/#chroma_otel_collection_headers","title":"<code>CHROMA_OTEL_COLLECTION_HEADERS</code>","text":"<p>Defines the headers that will be sent with each trace/span.</p> <p>Default: None</p> <p>Example:</p> <pre><code>export CHROMA_OTEL_COLLECTION_HEADERS='{\"X-API-KEY\":\"1234567890\"}'\n</code></pre>"},{"location":"core/configuration/#chroma_otel_granularity","title":"<code>CHROMA_OTEL_GRANULARITY</code>","text":"<p>Defines the granularity of the traces.</p> <p>Possible values:</p> <ul> <li><code>none</code> - No spans are emitted.</li> <li><code>operation</code> - Spans are emitted for each operation.</li> <li><code>operation_and_segment</code> - Spans are emitted for almost all method calls.</li> <li><code>all</code> - Spans are emitted for almost all method calls.</li> </ul> <p>Default: <code>none</code></p> <p>Example:</p> <pre><code>export CHROMA_OTEL_GRANULARITY=all\n</code></pre>"},{"location":"core/configuration/#chroma_product_telemetry_impl","title":"<code>CHROMA_PRODUCT_TELEMETRY_IMPL</code>","text":"<p>Do not change</p> <p>Do not change the default implementation as it may impact Chroma stability, instead use the <code>ANONYMIZED_TELEMETRY</code> configuration.</p> <p>Defines the implementation of the product telemetry.</p> <p>Default: <code>chromadb.telemetry.product.posthog.Posthog</code></p>"},{"location":"core/configuration/#chroma_telemetry_impl","title":"<code>CHROMA_TELEMETRY_IMPL</code>","text":"<p>This is identical to <code>CHROMA_PRODUCT_TELEMETRY_IMPL</code> but for the anonymized telemetry but is kept for backwards compatibility.</p>"},{"location":"core/configuration/#anonymized_telemetry","title":"<code>ANONYMIZED_TELEMETRY</code>","text":"<p>Enables or disables anonymized product telemetry.</p> <p>Possible values:</p> <ul> <li><code>TRUE</code> - Enables anonymized telemetry.</li> <li><code>FALSE</code> - Disables anonymized telemetry.</li> </ul> <p>Default: <code>TRUE</code> (enabled)</p> <p>Read more about how Chroma uses telemetry here.</p> <p>Example:</p> <pre><code>export ANONYMIZED_TELEMETRY=FALSE\n</code></pre>"},{"location":"core/configuration/#maintenance","title":"Maintenance","text":""},{"location":"core/configuration/#migrations","title":"<code>MIGRATIONS</code>","text":"<p>Defines how schema migrations are handled in Chroma.</p> <p>Possible values:</p> <ul> <li><code>none</code> - No migrations are applied.</li> <li><code>validate</code> - Existing schema is validated.</li> <li><code>apply</code> - Migrations are applied.</li> </ul> <p>Default: <code>apply</code></p>"},{"location":"core/configuration/#migrations_hash_algorithm","title":"<code>MIGRATIONS_HASH_ALGORITHM</code>","text":"<p>Defines the algorithm used to hash the migrations. This configuration was introduces as some organizations have strict policies around use of cryptographic algorithms, considering the default <code>md5</code> being a weak hashing algorithm.</p> <p>Possible values:</p> <ul> <li><code>sha256</code> - Uses SHA-256 to hash the migrations.</li> <li><code>md5</code> - Uses MD5 to hash the migrations.</li> </ul> <p>Default: <code>md5</code></p> <p>Example:</p> <pre><code>export MIGRATIONS_HASH_ALGORITHM=sha256\n</code></pre>"},{"location":"core/configuration/#operations-and-distributed","title":"Operations and Distributed","text":""},{"location":"core/configuration/#chroma_sysdb_impl","title":"<code>CHROMA_SYSDB_IMPL</code>","text":""},{"location":"core/configuration/#chroma_producer_impl","title":"<code>CHROMA_PRODUCER_IMPL</code>","text":""},{"location":"core/configuration/#chroma_consumer_impl","title":"<code>CHROMA_CONSUMER_IMPL</code>","text":""},{"location":"core/configuration/#chroma_segment_manager_impl","title":"<code>CHROMA_SEGMENT_MANAGER_IMPL</code>","text":""},{"location":"core/configuration/#chroma_segment_directory_impl","title":"<code>CHROMA_SEGMENT_DIRECTORY_IMPL</code>","text":""},{"location":"core/configuration/#chroma_memberlist_provider_impl","title":"<code>CHROMA_MEMBERLIST_PROVIDER_IMPL</code>","text":""},{"location":"core/configuration/#worker_memberlist_name","title":"<code>WORKER_MEMBERLIST_NAME</code>","text":""},{"location":"core/configuration/#chroma_coordinator_host","title":"<code>CHROMA_COORDINATOR_HOST</code>","text":""},{"location":"core/configuration/#chroma_server_grpc_port","title":"<code>CHROMA_SERVER_GRPC_PORT</code>","text":""},{"location":"core/configuration/#chroma_logservice_host","title":"<code>CHROMA_LOGSERVICE_HOST</code>","text":""},{"location":"core/configuration/#chroma_logservice_port","title":"<code>CHROMA_LOGSERVICE_PORT</code>","text":""},{"location":"core/configuration/#chroma_quota_provider_impl","title":"<code>CHROMA_QUOTA_PROVIDER_IMPL</code>","text":""},{"location":"core/configuration/#chroma_rate_limiting_provider_impl","title":"<code>CHROMA_RATE_LIMITING_PROVIDER_IMPL</code>","text":""},{"location":"core/configuration/#authentication","title":"Authentication","text":""},{"location":"core/configuration/#chroma_auth_token_transport_header","title":"<code>CHROMA_AUTH_TOKEN_TRANSPORT_HEADER</code>","text":""},{"location":"core/configuration/#chroma_client_auth_provider","title":"<code>CHROMA_CLIENT_AUTH_PROVIDER</code>","text":""},{"location":"core/configuration/#chroma_client_auth_credentials","title":"<code>CHROMA_CLIENT_AUTH_CREDENTIALS</code>","text":""},{"location":"core/configuration/#chroma_server_auth_ignore_paths","title":"<code>CHROMA_SERVER_AUTH_IGNORE_PATHS</code>","text":""},{"location":"core/configuration/#chroma_overwrite_singleton_tenant_database_access_from_auth","title":"<code>CHROMA_OVERWRITE_SINGLETON_TENANT_DATABASE_ACCESS_FROM_AUTH</code>","text":""},{"location":"core/configuration/#chroma_server_authn_provider","title":"<code>CHROMA_SERVER_AUTHN_PROVIDER</code>","text":""},{"location":"core/configuration/#chroma_server_authn_credentials","title":"<code>CHROMA_SERVER_AUTHN_CREDENTIALS</code>","text":""},{"location":"core/configuration/#chroma_server_authn_credentials_file","title":"<code>CHROMA_SERVER_AUTHN_CREDENTIALS_FILE</code>","text":""},{"location":"core/configuration/#authorization","title":"Authorization","text":""},{"location":"core/configuration/#chroma_server_authz_provider","title":"<code>CHROMA_SERVER_AUTHZ_PROVIDER</code>","text":""},{"location":"core/configuration/#chroma_server_authz_config","title":"<code>CHROMA_SERVER_AUTHZ_CONFIG</code>","text":""},{"location":"core/configuration/#chroma_server_authz_config_file","title":"<code>CHROMA_SERVER_AUTHZ_CONFIG_FILE</code>","text":""},{"location":"core/configuration/#client-configuration","title":"Client Configuration","text":""},{"location":"core/configuration/#authentication_1","title":"Authentication","text":""},{"location":"core/configuration/#hnsw-configuration-legacy","title":"HNSW Configuration (Legacy)","text":"<p>HNSW parameters were previously configured as collection metadata with the <code>hnsw:</code> prefix. This approach still works for backwards compatibility but is deprecated in favor of the <code>configuration</code> dict.</p> <p>Changing HNSW parameters</p> <p>Some HNSW parameters cannot be changed after index creation via the standard method shown below. If you which to change these parameters, you will need to clone the collection see an example here.</p>"},{"location":"core/configuration/#hnswspace","title":"<code>hnsw:space</code>","text":"<p>Description: Controls the distance metric of the HNSW index. The space cannot be changed after index creation.</p> <p>Default: <code>l2</code></p> <p>Constraints:</p> <ul> <li>Possible values: <code>l2</code>, <code>cosine</code>, <code>ip</code></li> <li>Parameter cannot be changed after index creation.</li> </ul> <p>Example:</p> <pre><code>res = client.create_collection(\"my_collection\", metadata={ \"hnsw:space\": \"cosine\"})\n</code></pre>"},{"location":"core/configuration/#hnswconstruction_ef","title":"<code>hnsw:construction_ef</code>","text":"<p>Description: Controls the number of neighbours in the HNSW graph to explore when adding new vectors. The more neighbours HNSW explores the better and more exhaustive the results will be. Increasing the value will also increase memory consumption.</p> <p>Default: <code>100</code></p> <p>Constraints:</p> <ul> <li>Values must be positive integers.</li> <li>Parameter cannot be changed after index creation.</li> </ul> <p>Example:</p> <pre><code>client.create_collection(\n    \"my_collection\",\n    metadata={ \"hnsw:construction_ef\": 100}\n)\n</code></pre>"},{"location":"core/configuration/#hnswm","title":"<code>hnsw:M</code>","text":"<p>Description: Controls the maximum number of neighbour connections (M), a newly inserted vector. A higher value results in a mode densely connected graph. The impact on this is slower but more accurate searches with increased memory consumption.</p> <p>Default: <code>16</code></p> <p>Constraints:</p> <ul> <li>Values must be positive integers.</li> <li>Parameter cannot be changed after index creation.</li> </ul> <p>Example:</p> <pre><code>client.create_collection(\n    \"my_collection\",\n    metadata={ \"hnsw:M\": 16}\n)\n</code></pre>"},{"location":"core/configuration/#hnswsearch_ef","title":"<code>hnsw:search_ef</code>","text":"<p>Description: Controls the number of neighbours in the HNSW graph to explore when searching. Increasing this requires more memory for the HNSW algo to explore the nodes during knn search.</p> <p>Default: <code>10</code></p> <p>Constraints:</p> <ul> <li>Values must be positive integers.</li> <li>Parameter can be changed after index creation.</li> </ul> <p>Example:</p> <pre><code>client.create_collection(\n    \"my_collection\",\n    metadata={ \"hnsw:search_ef\": 10}\n)\n</code></pre>"},{"location":"core/configuration/#hnswnum_threads","title":"<code>hnsw:num_threads</code>","text":"<p>Description: Controls how many threads HNSW algo use.</p> <p>Default: <code>&lt;number of CPU cores&gt;</code></p> <p>Constraints:</p> <ul> <li>Values must be positive integers.</li> <li>Parameter can be changed after index creation.</li> </ul> <p>Example:</p> <pre><code>client.create_collection(\n    \"my_collection\",\n    metadata={ \"hnsw:num_threads\": 4}\n)\n</code></pre>"},{"location":"core/configuration/#hnswresize_factor","title":"<code>hnsw:resize_factor</code>","text":"<p>Description: Controls the rate of growth of the graph (e.g. how many node capacity will be added) whenever the current graph capacity is reached.</p> <p>Default: <code>1.2</code></p> <p>Constraints:</p> <ul> <li>Values must be positive floating point numbers.</li> <li>Parameter can be changed after index creation.</li> </ul> <p>Example:</p> <pre><code>client.create_collection(\n    \"my_collection\",\n    metadata={ \"hnsw:resize_factor\": 1.2}\n)\n</code></pre>"},{"location":"core/configuration/#hnswbatch_size","title":"<code>hnsw:batch_size</code>","text":"<p>Description: Controls the size of the Bruteforce (in-memory) index. Once this threshold is crossed vectors from BF gets transferred to HNSW index. This value can be changed after index creation. The value must be less than <code>hnsw:sync_threshold</code>.</p> <p>Default: <code>100</code></p> <p>Constraints:</p> <ul> <li>Values must be positive integers.</li> <li>Parameter can be changed after index creation.</li> </ul> <p>Example:</p> <pre><code>client.create_collection(\n    \"my_collection\",\n    metadata={ \"hnsw:batch_size\": 100}\n)\n</code></pre>"},{"location":"core/configuration/#hnswsync_threshold","title":"<code>hnsw:sync_threshold</code>","text":"<p>Description: Controls the threshold when using HNSW index is written to disk.</p> <p>Default: <code>1000</code></p> <p>Constraints:</p> <ul> <li>Values must be positive integers.</li> <li>Parameter can be changed after index creation.</li> </ul>"},{"location":"core/configuration/#examples_2","title":"Examples","text":"<p>Configuring HNSW parameters at creation time</p> <pre><code>import chromadb\n\nclient = chromadb.HttpClient()  # Adjust as per your client\nclient.create_collection(\n    \"my_collection\",\n    metadata={\n        \"hnsw:space\": \"cosine\",\n        \"hnsw:construction_ef\": 100,\n        \"hnsw:M\": 16,\n        \"hnsw:search_ef\": 10,\n        \"hnsw:num_threads\": 4,\n        \"hnsw:resize_factor\": 1.2,\n        \"hnsw:batch_size\": 100,\n        \"hnsw:sync_threshold\": 1000,\n    }\n)\n</code></pre> <p>Updating HNSW parameters after creation</p> <p>Updating HNSW parameters</p> <p>Updating HNSW parameters after index creation is not supported as of version <code>0.5.5</code>.</p>"},{"location":"core/document-ids/","title":"Document IDs","text":"<p>Chroma is unopinionated about document IDs and delegates those decisions to the user. This frees users to build semantics around their IDs.</p>"},{"location":"core/document-ids/#note-on-compound-ids","title":"Note on Compound IDs","text":"<p>While you can choose to use IDs that are composed of multiple sub-IDs (e.g. <code>user_id</code> + <code>document_id</code>), it is important to highlight that Chroma does not support querying by partial ID.</p>"},{"location":"core/document-ids/#common-practices","title":"Common Practices","text":"chromadbx <p>We provide a convinient wrapper for in the form of <code>chromadbx</code> package that provides ID generators for UUIDs, ULIDs,  NonoIDs, and Hashes, among others functions.  You can install it with <code>pip install chromadbx</code>.</p>"},{"location":"core/document-ids/#uuids","title":"UUIDs","text":"<p>UUIDs are a common choice for document IDs. They are unique, and can be generated in a distributed fashion. They are also opaque, which means that they do not contain any information about the document itself. This can be a good thing, as it allows you to change the document without changing the ID.</p> chromadbxPython <pre><code>import chromadb\nfrom chromadbx import UUIDGenerator\n\nclient = chromadb.Client()\ncol = client.get_or_create_collection(\"test\")\nmy_docs = [f\"Document {_}\" for _ in range(10)]\ncol.add(ids=UUIDGenerator(len(my_docs)), documents=my_docs)\n</code></pre> <pre><code>import uuid\nimport chromadb\n\nmy_documents = [\n    \"Hello, world!\",\n    \"Hello, Chroma!\"\n]\n\nclient = chromadb.Client()\ncollection = client.get_or_create_collection(\"collection\")\ncollection.add(ids=[f\"{uuid.uuid4()}\" for _ in range(len(my_documents))], documents=my_documents)\n</code></pre>"},{"location":"core/document-ids/#caveats","title":"Caveats","text":"<p>Predictable Ordering</p> <p>UUIDs especially v4 are not lexicographically sortable. In its current version (0.4.x-0.5.10) Chroma orders responses  of <code>get()</code> by the ID of the documents. Therefore, if you need predictable ordering, you may want to consider a different ID strategy. In version <code>0.5.11</code> ordering is done on internal IDs</p> <p>Storage and Performance Overhead</p> <p>Chroma stores Document IDs as strings and UUIDs are 36 characters long, which can be a lot of overhead if you have a  large number of documents. If you are concerned  about storage overhead, you may want to consider a different ID strategy. Additionally Chroma uses the document IDs when sorting results which also incurs a performance hit.</p>"},{"location":"core/document-ids/#ulids","title":"ULIDs","text":"<p>ULIDs are a variant of UUIDs that are lexicographically sortable. They are also 128 bits long, like UUIDs, but they are encoded in a way that makes them sortable. This can be useful if you need predictable ordering of your documents.</p> <p>ULIDs are also shorter than UUIDs, which can save you some storage space. They are also opaque, like UUIDs, which means that they do not contain any information about the document itself.</p> <p>Install the <code>ulid-py</code> package to generate ULIDs.</p> <pre><code>pip install ulid-py\n</code></pre> chromadbxPython <pre><code>import chromadb\nfrom chromadbx import ULIDGenerator\nimport ulid\nclient = chromadb.Client()\ncol = client.get_or_create_collection(\"test\")\nmy_docs = [f\"Document {_}\" for _ in range(10)]\ncol.add(ids=ULIDGenerator(len(my_docs)), documents=my_docs)\n</code></pre> <pre><code>from ulid import ULID\nimport chromadb\n\nmy_documents = [\n    \"Hello, world!\",\n    \"Hello, Chroma!\"\n]\n_ulid = ULID()\n\nclient = chromadb.Client()\n\ncollection = client.get_or_create_collection(\"name\")\n\ncollection.add(ids=[f\"{_ulid.generate()}\" for _ in range(len(my_documents))], documents=my_documents)\n</code></pre>"},{"location":"core/document-ids/#nanoids","title":"NanoIDs","text":"<p>NanoIDs provide a way to generate unique IDs that are shorter than UUIDs. They are not lexically sortable, but they are unique and can be generated in a distributed fashion. They are also opaque, with low collision rates - (collision probability calculator)[https://zelark.github.io/nano-id-cc/]</p> chromadbxPython <pre><code>import chromadb\nfrom chromadbx import NanoIDGenerator\nclient = chromadb.Client()\ncol = client.get_or_create_collection(\"test\")\nmy_docs = [f\"Document {_}\" for _ in range(10)]\ncol.add(ids=NanoIDGenerator(len(my_docs)), documents=my_docs)\n</code></pre> <pre><code>from nanoid import generate\nimport chromadb\nclient = chromadb.Client()\ncol = client.get_or_create_collection(\"test\")\nmy_docs = [f\"Document {_}\" for _ in range(10)]\ncol.add(ids=[f\"{generate()}\" for _ in range(my_docs)], documents=my_docs)\n</code></pre>"},{"location":"core/document-ids/#hashes","title":"Hashes","text":"<p>Hashes are another common choice for document IDs. They are unique, and can be generated in a distributed fashion. They are also opaque, which means that they do not contain any information about the document itself. This can be a good thing, as it allows you to change the document without changing the ID.</p> chromadbxPython <p>Random SHA256:</p> <pre><code>import chromadb\nfrom chromadbx import RandomSHA256Generator\nclient = chromadb.Client()\ncol = client.get_or_create_collection(\"test\")\nmy_docs = [f\"Document {_}\" for _ in range(10)]\ncol.add(ids=RandomSHA256Generator(len(my_docs)), documents=my_docs)\n</code></pre> <p>Document-based SHA256:</p> <pre><code>import chromadb\nfrom chromadbx import DocumentSHA256Generator\nclient = chromadb.Client()\ncol = client.get_or_create_collection(\"test\")\nmy_docs = [f\"Document {_}\" for _ in range(10)]\ncol.add(ids=DocumentSHA256Generator(documents=my_docs), documents=my_docs)\n</code></pre> <p>Random SHA256:</p> <pre><code>import hashlib\nimport os\nimport chromadb\n\n\ndef generate_sha256_hash() -&gt; str:\n    # Generate a random number\n    random_data = os.urandom(16)\n    # Create a SHA256 hash object\n    sha256_hash = hashlib.sha256()\n    # Update the hash object with the random data\n    sha256_hash.update(random_data)\n    # Return the hexadecimal representation of the hash\n    return sha256_hash.hexdigest()\n\n\nmy_documents = [\n    \"Hello, world!\",\n    \"Hello, Chroma!\"\n]\n\nclient = chromadb.Client()\ncollection = client.get_or_create_collection(\"collection\")\ncollection.add(ids=[generate_sha256_hash() for _ in range(len(my_documents))], documents=my_documents)\n</code></pre> <p>Document-based SHA256:</p> <p>It is also possible to use the document as basis for the hash, the downside of that is that when the document changes, and you have a semantic around the text as relating to the hash, you may need to update the hash.</p> <pre><code>import hashlib\nimport chromadb\n\n\ndef generate_sha256_hash_from_text(text) -&gt; str:\n    # Create a SHA256 hash object\n    sha256_hash = hashlib.sha256()\n    # Update the hash object with the text encoded to bytes\n    sha256_hash.update(text.encode('utf-8'))\n    # Return the hexadecimal representation of the hash\n    return sha256_hash.hexdigest()\n\n\nmy_documents = [\n    \"Hello, world!\",\n    \"Hello, Chroma!\"\n]\n\nclient = chromadb.Client()\ncollection = client.get_or_create_collection(\"collection\")\ncollection.add(ids=[generate_sha256_hash_from_text(my_documents[i]) for i in range(len(my_documents))],\n               documents=my_documents)\n</code></pre>"},{"location":"core/document-ids/#semantic-strategies","title":"Semantic Strategies","text":"<p>In this section we'll explore a few different use cases for building semantics around document IDs.</p> <ul> <li>URL Slugs - if your docs are web pages with permalinks (e.g. blog posts), you can use the URL slug as the document ID.</li> <li>File Paths - if your docs are files on disk, you can use the file path as the document ID.</li> </ul>"},{"location":"core/filters/","title":"Filters","text":"<p>Chroma provides two types of filters:</p> <ul> <li>Metadata - filter documents based on metadata using <code>where</code> clause in either <code>Collection.query()</code> or <code>Collection.get()</code></li> <li>Document - filter documents based on document content using <code>where_document</code> in <code>Collection.query()</code> or <code>Collection.get()</code>.</li> </ul> <p>Those familiar with MongoDB queries will find Chroma's filters very similar.</p> <p>Runnable Examples</p> <p>Complete, runnable filtering examples for each language are available in the examples/filtering directory:</p> <ul> <li>Python</li> <li>TypeScript</li> <li>Go</li> <li>Rust</li> </ul> <p>Interactive playground</p> <p>Build filters interactively</p> <p>     Compose <code>where</code> and <code>where_document</code>, preview payloads, and copy Cloud or Local starter code.   </p> Open Interactive Playground"},{"location":"core/filters/#metadata-filters","title":"Metadata Filters","text":""},{"location":"core/filters/#schema","title":"Schema","text":"<p>You can use the following JSON schema to validate your <code>where</code> filters:</p> <pre><code>{\n    \"$schema\": \"https://json-schema.org/draft/2020-12/schema#\",\n    \"title\": \"Chroma Metadata Where Filter Schema\",\n    \"description\": \"Schema for Chroma metadata filters used in where clauses\",\n    \"oneOf\": [\n        {\n            \"type\": \"object\",\n            \"patternProperties\": {\n                \"^[^$].*$\": {\n                    \"oneOf\": [\n                        {\n                            \"type\": [\"string\", \"number\", \"boolean\"]\n                        },\n                        {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"$eq\": {\"type\": [\"string\", \"number\", \"boolean\"]},\n                                \"$ne\": {\"type\": [\"string\", \"number\", \"boolean\"]},\n                                \"$gt\": {\"type\": \"number\"},\n                                \"$gte\": {\"type\": \"number\"},\n                                \"$lt\": {\"type\": \"number\"},\n                                \"$lte\": {\"type\": \"number\"},\n                                \"$in\": {\n                                  \"oneOf\": [\n                                    {\n                                      \"type\": \"array\",\n                                      \"items\": { \"type\": \"string\" },\n                                      \"minItems\": 1\n                                    },\n                                    {\n                                      \"type\": \"array\",\n                                      \"items\": { \"type\": \"number\" },\n                                      \"minItems\": 1\n                                    },\n                                    {\n                                      \"type\": \"array\",\n                                      \"items\": { \"type\": \"boolean\" },\n                                      \"minItems\": 1\n                                    }\n                                  ]\n                                },\n                                \"$nin\": {\n                                  \"oneOf\": [\n                                    {\n                                      \"type\": \"array\",\n                                      \"items\": { \"type\": \"string\" },\n                                      \"minItems\": 1\n                                    },\n                                    {\n                                      \"type\": \"array\",\n                                      \"items\": { \"type\": \"number\" },\n                                      \"minItems\": 1\n                                    },\n                                    {\n                                      \"type\": \"array\",\n                                      \"items\": { \"type\": \"boolean\" },\n                                      \"minItems\": 1\n                                    }\n                                  ]\n                                },\n                                \"$contains\": {\"type\": [\"string\", \"number\", \"boolean\"]},\n                                \"$not_contains\": {\"type\": [\"string\", \"number\", \"boolean\"]}\n                            },\n                            \"additionalProperties\": false,\n                            \"minProperties\": 1,\n                            \"maxProperties\": 1\n                        }\n                    ]\n                }\n            },\n            \"minProperties\": 1\n        },\n        {\n            \"type\": \"object\",\n            \"properties\": {\n                \"$and\": {\n                    \"type\": \"array\",\n                    \"items\": {\"$ref\": \"#\"},\n                    \"minItems\": 2\n                },\n                \"$or\": {\n                    \"type\": \"array\",\n                    \"items\": {\"$ref\": \"#\"},\n                    \"minItems\": 2\n                }\n            },\n            \"additionalProperties\": false,\n            \"minProperties\": 1,\n            \"maxProperties\": 1\n        }\n    ]\n}\n</code></pre>"},{"location":"core/filters/#equality-eq","title":"Equality (<code>$eq</code>)","text":"<p>This filter matches attribute values that equal to a specified string, boolean, integer or float value. The value check is case-sensitive.</p> <p>Supported value types are:  <code>string</code>, <code>boolean</code>, <code>integer</code> or <code>float</code> (or <code>number</code> in JS/TS)</p> <p>Simple equality:</p> Single condition <p>If you are using simple equality expression <code>{\"metadata_field\": \"is_equal_to_this\"}</code>, you can only specify a single condition.</p> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"metadata_field\": \"is_equal_to_this\"}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    where: { metadata_field: \"is_equal_to_this\" },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhere(chroma.EqString(\"metadata_field\", \"is_equal_to_this\")),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,\n    Some(Where::Metadata(MetadataExpression {\n        key: \"metadata_field\".to_string(),\n        comparison: MetadataComparison::Primitive(\n            PrimitiveOperator::Equal,\n            MetadataValue::Str(\"is_equal_to_this\".to_string()),\n        ),\n    })),\n    None, None, None,\n).await?;\n</code></pre> <p>Alternative syntax:</p> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"metadata_field\": {\"$eq\": \"is_equal_to_this\"}}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    where: { metadata_field: { $eq: \"is_equal_to_this\" } },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhere(chroma.EqString(\"metadata_field\", \"is_equal_to_this\")),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,\n    Some(Where::Metadata(MetadataExpression {\n        key: \"metadata_field\".to_string(),\n        comparison: MetadataComparison::Primitive(\n            PrimitiveOperator::Equal,\n            MetadataValue::Str(\"is_equal_to_this\".to_string()),\n        ),\n    })),\n    None, None, None,\n).await?;\n</code></pre> Validation Failures <p>When validation fails, similar to this message is expected to be returned by Chroma - <code>ValueError: Expected where value to be a str, int, float, or operator expression, got X in get.</code> with <code>X</code> refering to the inferred type of the data.</p>"},{"location":"core/filters/#inequality-ne","title":"Inequality (<code>$ne</code>)","text":"<p>This filter matches attribute values that are not equal to a specified string, boolean, integer or float value. The value check is case-sensitive.</p> <p>Supported value types are:  <code>string</code>, <code>boolean</code>, <code>integer</code> or <code>float</code> (or <code>number</code> in JS/TS)</p> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"metadata_field\": {\"$ne\": \"is_not_equal_to_this\"}}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    where: { metadata_field: { $ne: \"is_not_equal_to_this\" } },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhere(chroma.NotEqString(\"metadata_field\", \"is_not_equal_to_this\")),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,\n    Some(Where::Metadata(MetadataExpression {\n        key: \"metadata_field\".to_string(),\n        comparison: MetadataComparison::Primitive(\n            PrimitiveOperator::NotEqual,\n            MetadataValue::Str(\"is_not_equal_to_this\".to_string()),\n        ),\n    })),\n    None, None, None,\n).await?;\n</code></pre>"},{"location":"core/filters/#greater-than-gt","title":"Greater Than (<code>$gt</code>)","text":"<p>This filter matches attribute values that are strictly greater than a specified numeric (<code>integer</code> or <code>float</code>) value.</p> <p>Greater Than</p> <p>The <code>$gt</code> operator is only supported for numerical values - int or float values.</p> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"metadata_field\": {\"$gt\": 5}}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    where: { metadata_field: { $gt: 5 } },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhere(chroma.GtInt(\"metadata_field\", 5)),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,\n    Some(Where::Metadata(MetadataExpression {\n        key: \"metadata_field\".to_string(),\n        comparison: MetadataComparison::Primitive(\n            PrimitiveOperator::GreaterThan,\n            MetadataValue::Int(5),\n        ),\n    })),\n    None, None, None,\n).await?;\n</code></pre>"},{"location":"core/filters/#greater-than-or-equal-gte","title":"Greater Than or Equal (<code>$gte</code>)","text":"<p>This filter matches attribute values that are greater than or equal a specified numeric (<code>integer</code> or <code>float</code>) value.</p> <p>Greater Than or Equal</p> <p>The <code>$gte</code> operator is only supported for numerical values - int or float values.</p> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"metadata_field\": {\"$gte\": 5.1}}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    where: { metadata_field: { $gte: 5.1 } },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhere(chroma.GteFloat(\"metadata_field\", 5.1)),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,\n    Some(Where::Metadata(MetadataExpression {\n        key: \"metadata_field\".to_string(),\n        comparison: MetadataComparison::Primitive(\n            PrimitiveOperator::GreaterThanOrEqual,\n            MetadataValue::Float(5.1),\n        ),\n    })),\n    None, None, None,\n).await?;\n</code></pre>"},{"location":"core/filters/#less-than-lt","title":"Less Than (<code>$lt</code>)","text":"<p>This filter matches attribute values that are less than specified numeric (<code>integer</code> or <code>float</code>) value.</p> <p>Supported values: <code>integer</code> or <code>float</code></p> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"metadata_field\": {\"$lt\": 5}}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    where: { metadata_field: { $lt: 5 } },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhere(chroma.LtInt(\"metadata_field\", 5)),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,\n    Some(Where::Metadata(MetadataExpression {\n        key: \"metadata_field\".to_string(),\n        comparison: MetadataComparison::Primitive(\n            PrimitiveOperator::LessThan,\n            MetadataValue::Int(5),\n        ),\n    })),\n    None, None, None,\n).await?;\n</code></pre>"},{"location":"core/filters/#less-than-or-equal-lte","title":"Less Than or Equal (<code>$lte</code>)","text":"<p>This filter matches attribute values that are less than or equal specified numeric (<code>integer</code> or <code>float</code>) value.</p> <p>Supported values: <code>integer</code> or <code>float</code></p> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"metadata_field\": {\"$lte\": 5.1}}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    where: { metadata_field: { $lte: 5.1 } },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhere(chroma.LteFloat(\"metadata_field\", 5.1)),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,\n    Some(Where::Metadata(MetadataExpression {\n        key: \"metadata_field\".to_string(),\n        comparison: MetadataComparison::Primitive(\n            PrimitiveOperator::LessThanOrEqual,\n            MetadataValue::Float(5.1),\n        ),\n    })),\n    None, None, None,\n).await?;\n</code></pre>"},{"location":"core/filters/#in-in","title":"In (<code>$in</code>)","text":"<p>This filter matches attribute values that are in the given list of values.</p> <p>Supported value types are:  <code>string</code>, <code>boolean</code>, <code>integer</code> or <code>float</code> (or <code>number</code> in JS/TS)</p> <p>In</p> <p>The <code>$in</code> operator is only supported for list of values of the same type.</p> StringsIntegersInvalid Example PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"metadata_field\": {\"$in\": [\"value1\", \"value2\"]}}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    where: { metadata_field: { $in: [\"value1\", \"value2\"] } },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhere(chroma.InString(\"metadata_field\", \"value1\", \"value2\")),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,\n    Some(Where::Metadata(MetadataExpression {\n        key: \"metadata_field\".to_string(),\n        comparison: MetadataComparison::Set(\n            SetOperator::In,\n            MetadataSetValue::Str(vec![\"value1\".into(), \"value2\".into()]),\n        ),\n    })),\n    None, None, None,\n).await?;\n</code></pre> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"metadata_field\": {\"$in\": [1, 2, 3]}}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    where: { metadata_field: { $in: [1, 2, 3] } },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhere(chroma.InInt(\"metadata_field\", 1, 2, 3)),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,\n    Some(Where::Metadata(MetadataExpression {\n        key: \"metadata_field\".to_string(),\n        comparison: MetadataComparison::Set(\n            SetOperator::In,\n            MetadataSetValue::Int(vec![1, 2, 3]),\n        ),\n    })),\n    None, None, None,\n).await?;\n</code></pre> <pre><code># All values in $in must be the same type - this will fail\nresults = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"metadata_field\": {\"$in\": [1, \"2\", 1.1]}}\n)\n</code></pre>"},{"location":"core/filters/#not-in-nin","title":"Not In (<code>$nin</code>)","text":"<p>This filter matches attribute that do not have the given key or the values of which are not in the given list of values.</p> <p>Supported value types are:  <code>string</code>, <code>boolean</code>, <code>integer</code> or <code>float</code> (or <code>number</code> in JS/TS)</p> <p>Not In</p> <p>The <code>$nin</code> operator is only supported for list of values of the same type.</p> StringsIntegersInvalid Example PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"metadata_field\": {\"$nin\": [\"value1\", \"value2\"]}}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    where: { metadata_field: { $nin: [\"value1\", \"value2\"] } },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhere(chroma.NinString(\"metadata_field\", \"value1\", \"value2\")),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,\n    Some(Where::Metadata(MetadataExpression {\n        key: \"metadata_field\".to_string(),\n        comparison: MetadataComparison::Set(\n            SetOperator::NotIn,\n            MetadataSetValue::Str(vec![\"value1\".into(), \"value2\".into()]),\n        ),\n    })),\n    None, None, None,\n).await?;\n</code></pre> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"metadata_field\": {\"$nin\": [1, 2, 3]}}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    where: { metadata_field: { $nin: [1, 2, 3] } },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhere(chroma.NinInt(\"metadata_field\", 1, 2, 3)),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,\n    Some(Where::Metadata(MetadataExpression {\n        key: \"metadata_field\".to_string(),\n        comparison: MetadataComparison::Set(\n            SetOperator::NotIn,\n            MetadataSetValue::Int(vec![1, 2, 3]),\n        ),\n    })),\n    None, None, None,\n).await?;\n</code></pre> <pre><code># All values in $nin must be the same type - this will fail\nresults = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"metadata_field\": {\"$nin\": [1, \"2\", 1.1]}}\n)\n</code></pre>"},{"location":"core/filters/#array-metadata","title":"Array Metadata","text":"<p>Chroma &gt;= 1.5.0</p> <p>Array metadata and the <code>$contains</code>/<code>$not_contains</code> operators require Chroma 1.5.0 or later.</p> <p>Chroma supports storing arrays in metadata fields. All elements in an array must be of the same type.</p> <p>Supported array element types: <code>string</code>, <code>integer</code>, <code>float</code>, <code>boolean</code></p> <p>Constraints:</p> <ul> <li>Empty arrays are not allowed</li> <li>Nested arrays (arrays of arrays) are not supported</li> <li>All elements must be the same type (no mixed-type arrays)</li> </ul> <p>Runnable Array Examples</p> <p>End-to-end array metadata examples are included in the filtering examples:</p> <ul> <li>Python</li> <li>TypeScript</li> <li>Rust</li> </ul>"},{"location":"core/filters/#storing-array-metadata","title":"Storing Array Metadata","text":"<p>Here is an example of a research paper collection using array metadata to store multi-value fields like topics, authors, and review scores:</p> PythonTypeScriptGoRust <pre><code>import chromadb\n\nclient = chromadb.Client()\ncollection = client.create_collection(\"research_papers\")\n\ncollection.add(\n    ids=[\"paper-1\", \"paper-2\", \"paper-3\"],\n    documents=[\n        \"We introduce a transformer-based architecture for low-resource language translation.\",\n        \"A study on the effects of soil microbiome diversity on crop yield in arid climates.\",\n        \"Applying reinforcement learning to optimize energy consumption in smart grid networks.\",\n    ],\n    metadatas=[\n        {\n            \"authors\": [\"Chen\", \"Okafor\", \"M\u00fcller\"],\n            \"topics\": [\"nlp\", \"transformers\", \"low-resource\"],\n            \"review_scores\": [8, 7, 9],\n            \"year\": 2024,\n        },\n        {\n            \"authors\": [\"Patel\", \"Johansson\"],\n            \"topics\": [\"agriculture\", \"microbiome\", \"climate\"],\n            \"review_scores\": [6, 7, 7],\n            \"year\": 2023,\n        },\n        {\n            \"authors\": [\"Chen\", \"Williams\"],\n            \"topics\": [\"reinforcement-learning\", \"energy\", \"smart-grid\"],\n            \"review_scores\": [9, 8, 9],\n            \"year\": 2024,\n        },\n    ],\n)\n</code></pre> <pre><code>import { ChromaClient } from \"chromadb\";\n\nconst client = new ChromaClient();\nconst collection = await client.createCollection({ name: \"research_papers\" });\n\nawait collection.add({\n    ids: [\"paper-1\", \"paper-2\", \"paper-3\"],\n    documents: [\n        \"We introduce a transformer-based architecture for low-resource language translation.\",\n        \"A study on the effects of soil microbiome diversity on crop yield in arid climates.\",\n        \"Applying reinforcement learning to optimize energy consumption in smart grid networks.\",\n    ],\n    metadatas: [\n        {\n            authors: [\"Chen\", \"Okafor\", \"M\u00fcller\"],\n            topics: [\"nlp\", \"transformers\", \"low-resource\"],\n            review_scores: [8, 7, 9],\n            year: 2024,\n        },\n        {\n            authors: [\"Patel\", \"Johansson\"],\n            topics: [\"agriculture\", \"microbiome\", \"climate\"],\n            review_scores: [6, 7, 7],\n            year: 2023,\n        },\n        {\n            authors: [\"Chen\", \"Williams\"],\n            topics: [\"reinforcement-learning\", \"energy\", \"smart-grid\"],\n            review_scores: [9, 8, 9],\n            year: 2024,\n        },\n    ],\n});\n</code></pre> <p>Go Client</p> <p>Array metadata support in the Go client is pending. See the chroma-go repository for updates.</p> <pre><code>let mut metadata1 = Metadata::new();\nmetadata1.insert(\"authors\".into(), vec![\"Chen\", \"Okafor\", \"M\u00fcller\"].into());\nmetadata1.insert(\"topics\".into(), vec![\"nlp\", \"transformers\", \"low-resource\"].into());\nmetadata1.insert(\"review_scores\".into(), vec![8i64, 7, 9].into());\nmetadata1.insert(\"year\".into(), MetadataValue::Int(2024));\n\nlet mut metadata2 = Metadata::new();\nmetadata2.insert(\"authors\".into(), vec![\"Patel\", \"Johansson\"].into());\nmetadata2.insert(\"topics\".into(), vec![\"agriculture\", \"microbiome\", \"climate\"].into());\nmetadata2.insert(\"review_scores\".into(), vec![6i64, 7, 7].into());\nmetadata2.insert(\"year\".into(), MetadataValue::Int(2023));\n\nlet mut metadata3 = Metadata::new();\nmetadata3.insert(\"authors\".into(), vec![\"Chen\", \"Williams\"].into());\nmetadata3.insert(\"topics\".into(), vec![\"reinforcement-learning\", \"energy\", \"smart-grid\"].into());\nmetadata3.insert(\"review_scores\".into(), vec![9i64, 8, 9].into());\nmetadata3.insert(\"year\".into(), MetadataValue::Int(2024));\n\ncollection.add(\n    vec![\"paper-1\".into(), \"paper-2\".into(), \"paper-3\".into()],\n    vec![embed1, embed2, embed3],\n    Some(vec![\n        Some(\"We introduce a transformer-based architecture for low-resource language translation.\".into()),\n        Some(\"A study on the effects of soil microbiome diversity on crop yield in arid climates.\".into()),\n        Some(\"Applying reinforcement learning to optimize energy consumption in smart grid networks.\".into()),\n    ]),\n    None,\n    Some(vec![Some(metadata1), Some(metadata2), Some(metadata3)]),\n).await?;\n</code></pre>"},{"location":"core/filters/#contains-contains","title":"Contains (<code>$contains</code>)","text":"<p>Returns records where an array metadata field includes a specific value. The filter value must be a scalar matching the array's element type.</p> PythonTypeScriptGoRustPythonTypeScriptGoRust <pre><code># Find papers authored by \"Chen\"\nresults = collection.get(\n    where={\"authors\": {\"$contains\": \"Chen\"}}\n)\n# Returns paper-1 and paper-3\n</code></pre> <pre><code>// Find papers authored by \"Chen\"\nconst results = await collection.get({\n    where: { authors: { $contains: \"Chen\" } },\n});\n// Returns paper-1 and paper-3\n</code></pre> <p>Go Client</p> <p>Array metadata <code>$contains</code> in the Go client is pending. See the chroma-go repository for updates.</p> <pre><code>// Find papers authored by \"Chen\" (Search API - Chroma Cloud)\nlet results = collection.search(vec![\n    SearchPayload::default()\n        .r#where(Key::field(\"authors\").contains(\"Chen\"))\n        .limit(Some(10), 0),\n]).await?;\n// Returns paper-1 and paper-3\n</code></pre> <pre><code># Find papers that received a review score of 9\nresults = collection.get(\n    where={\"review_scores\": {\"$contains\": 9}}\n)\n# Returns paper-1 and paper-3\n</code></pre> <pre><code>// Find papers that received a review score of 9\nconst results = await collection.get({\n    where: { review_scores: { $contains: 9 } },\n});\n// Returns paper-1 and paper-3\n</code></pre> <p>Go Client</p> <p>Array metadata <code>$contains</code> in the Go client is pending. See the chroma-go repository for updates.</p> <pre><code>// Find papers that received a review score of 9 (Search API - Chroma Cloud)\nlet results = collection.search(vec![\n    SearchPayload::default()\n        .r#where(Key::field(\"review_scores\").contains(9))\n        .limit(Some(10), 0),\n]).await?;\n// Returns paper-1 and paper-3\n</code></pre>"},{"location":"core/filters/#not-contains-not_contains","title":"Not Contains (<code>$not_contains</code>)","text":"<p>Returns records where an array metadata field does not include a specific value.</p> PythonTypeScriptGoRust <pre><code># Find papers not tagged with \"nlp\"\nresults = collection.get(\n    where={\"topics\": {\"$not_contains\": \"nlp\"}}\n)\n# Returns paper-2 and paper-3\n</code></pre> <pre><code>// Find papers not tagged with \"nlp\"\nconst results = await collection.get({\n    where: { topics: { $not_contains: \"nlp\" } },\n});\n// Returns paper-2 and paper-3\n</code></pre> <p>Go Client</p> <p>Array metadata <code>$not_contains</code> in the Go client is pending. See the chroma-go repository for updates.</p> <pre><code>// Find papers not tagged with \"nlp\" (Search API - Chroma Cloud)\nlet results = collection.search(vec![\n    SearchPayload::default()\n        .r#where(Key::field(\"topics\").not_contains(\"nlp\"))\n        .limit(Some(10), 0),\n]).await?;\n// Returns paper-2 and paper-3\n</code></pre>"},{"location":"core/filters/#combining-array-filters","title":"Combining Array Filters","text":"<p>Array filters work with <code>$and</code>/<code>$or</code> logical operators and can be mixed with scalar filters:</p> PythonTypeScriptGoRust <pre><code># Papers by \"Chen\" published in 2024 that cover \"energy\"\nresults = collection.query(\n    query_texts=[\"renewable energy optimization\"],\n    where={\n        \"$and\": [\n            {\"authors\": {\"$contains\": \"Chen\"}},\n            {\"topics\": {\"$contains\": \"energy\"}},\n            {\"year\": {\"$eq\": 2024}},\n        ]\n    },\n)\n# Returns paper-3\n</code></pre> <pre><code>// Papers by \"Chen\" published in 2024 that cover \"energy\"\nconst results = await collection.query({\n    queryTexts: [\"renewable energy optimization\"],\n    where: {\n        $and: [\n            { authors: { $contains: \"Chen\" } },\n            { topics: { $contains: \"energy\" } },\n            { year: { $eq: 2024 } },\n        ],\n    },\n});\n// Returns paper-3\n</code></pre> <p>Go Client</p> <p>Array metadata <code>$contains</code> in the Go client is pending. See the chroma-go repository for updates.</p> <pre><code>// Papers by \"Chen\" published in 2024 that cover \"energy\" (Search API - Chroma Cloud)\nlet results = collection.search(vec![\n    SearchPayload::default()\n        .r#where(\n            (Key::field(\"authors\").contains(\"Chen\"))\n                &amp; (Key::field(\"topics\").contains(\"energy\"))\n                &amp; (Key::field(\"year\").eq(2024)),\n        )\n        .rank(knn_query.clone())\n        .limit(Some(10), 0),\n]).await?;\n// Returns paper-3\n</code></pre>"},{"location":"core/filters/#logical-operator-and-and","title":"Logical Operator: And (<code>$and</code>)","text":"<p>The <code>$and</code> logical operator joins two or more simple (<code>$eq</code>, <code>$ne</code>, <code>$gt</code> etc.) filters together and matches records for which all of the conditions in the list are satisfied.</p> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"$and\": [{\"metadata_field1\": \"value1\"}, {\"metadata_field2\": \"value2\"}]}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    where: { $and: [{ metadata_field1: \"value1\" }, { metadata_field2: \"value2\" }] },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhere(chroma.And(\n        chroma.EqString(\"metadata_field1\", \"value1\"),\n        chroma.EqString(\"metadata_field2\", \"value2\"),\n    )),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,\n    Some(Where::Composite(CompositeExpression {\n        operator: BooleanOperator::And,\n        children: vec![\n            Where::Metadata(MetadataExpression {\n                key: \"metadata_field1\".to_string(),\n                comparison: MetadataComparison::Primitive(\n                    PrimitiveOperator::Equal,\n                    MetadataValue::Str(\"value1\".to_string()),\n                ),\n            }),\n            Where::Metadata(MetadataExpression {\n                key: \"metadata_field2\".to_string(),\n                comparison: MetadataComparison::Primitive(\n                    PrimitiveOperator::Equal,\n                    MetadataValue::Str(\"value2\".to_string()),\n                ),\n            }),\n        ],\n    })),\n    None, None, None,\n).await?;\n</code></pre>"},{"location":"core/filters/#logical-operator-or-or","title":"Logical Operator: Or (<code>$or</code>)","text":"<p>The <code>$or</code> logical operator that joins two or more simple (<code>$eq</code>, <code>$ne</code>, <code>$gt</code> etc.) filters together and matches records for which at least one of the conditions in the list is satisfied.</p> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\"$or\": [{\"metadata_field1\": \"value1\"}, {\"metadata_field2\": \"value2\"}]}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    where: { $or: [{ metadata_field1: \"value1\" }, { metadata_field2: \"value2\" }] },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhere(chroma.Or(\n        chroma.EqString(\"metadata_field1\", \"value1\"),\n        chroma.EqString(\"metadata_field2\", \"value2\"),\n    )),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,\n    Some(Where::Composite(CompositeExpression {\n        operator: BooleanOperator::Or,\n        children: vec![\n            Where::Metadata(MetadataExpression {\n                key: \"metadata_field1\".to_string(),\n                comparison: MetadataComparison::Primitive(\n                    PrimitiveOperator::Equal,\n                    MetadataValue::Str(\"value1\".to_string()),\n                ),\n            }),\n            Where::Metadata(MetadataExpression {\n                key: \"metadata_field2\".to_string(),\n                comparison: MetadataComparison::Primitive(\n                    PrimitiveOperator::Equal,\n                    MetadataValue::Str(\"value2\".to_string()),\n                ),\n            }),\n        ],\n    })),\n    None, None, None,\n).await?;\n</code></pre>"},{"location":"core/filters/#logical-operator-nesting","title":"Logical Operator Nesting","text":"<p>Logical Operators can be nested.</p> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where={\n        \"$and\": [\n            {\"metadata_field1\": \"value1\"},\n            {\"$and\": [{\"metadata_field2\": \"value2\"}, {\"metadata_field3\": \"value3\"}]},\n        ]\n    },\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    where: {\n        $and: [\n            { metadata_field1: \"value1\" },\n            { $and: [{ metadata_field2: \"value2\" }, { metadata_field3: \"value3\" }] },\n        ],\n    },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhere(chroma.And(\n        chroma.EqString(\"metadata_field1\", \"value1\"),\n        chroma.And(\n            chroma.EqString(\"metadata_field2\", \"value2\"),\n            chroma.EqString(\"metadata_field3\", \"value3\"),\n        ),\n    )),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,\n    Some(Where::Composite(CompositeExpression {\n        operator: BooleanOperator::And,\n        children: vec![\n            Where::Metadata(MetadataExpression {\n                key: \"metadata_field1\".to_string(),\n                comparison: MetadataComparison::Primitive(\n                    PrimitiveOperator::Equal,\n                    MetadataValue::Str(\"value1\".to_string()),\n                ),\n            }),\n            Where::Composite(CompositeExpression {\n                operator: BooleanOperator::And,\n                children: vec![\n                    Where::Metadata(MetadataExpression {\n                        key: \"metadata_field2\".to_string(),\n                        comparison: MetadataComparison::Primitive(\n                            PrimitiveOperator::Equal,\n                            MetadataValue::Str(\"value2\".to_string()),\n                        ),\n                    }),\n                    Where::Metadata(MetadataExpression {\n                        key: \"metadata_field3\".to_string(),\n                        comparison: MetadataComparison::Primitive(\n                            PrimitiveOperator::Equal,\n                            MetadataValue::Str(\"value3\".to_string()),\n                        ),\n                    }),\n                ],\n            }),\n        ],\n    })),\n    None, None, None,\n).await?;\n</code></pre>"},{"location":"core/filters/#document-filters","title":"Document Filters","text":"<p>Rust: Search API Required</p> <p>In the Rust client, document filters use the Search API (<code>collection.search()</code>) with the <code>Key::Document</code> builder. The Search API requires Chroma Cloud.</p>"},{"location":"core/filters/#schema_1","title":"Schema","text":"<p>You can use the following JSON schema to validate <code>where_document</code> expressions:</p> <pre><code>{\n    \"$schema\": \"https://json-schema.org/draft/2020-12/schema#\",\n    \"title\": \"Chroma Document Filter Schema\",\n    \"description\": \"Schema for Chroma document filters used in where_document clauses\",\n    \"type\": \"object\",\n    \"oneOf\": [\n        {\n            \"properties\": {\n                \"$contains\": {\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\"$contains\"],\n            \"additionalProperties\": false\n        },\n        {\n            \"properties\": {\n                \"$not_contains\": {\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\"$not_contains\"],\n            \"additionalProperties\": false\n        },\n        {\n            \"properties\": {\n                \"$regex\": {\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\"$regex\"],\n            \"additionalProperties\": false\n        },\n        {\n            \"properties\": {\n                \"$not_regex\": {\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\"$not_regex\"],\n            \"additionalProperties\": false\n        },\n        {\n            \"properties\": {\n                \"$and\": {\n                    \"type\": \"array\",\n                    \"items\": {\"$ref\": \"#\"},\n                    \"minItems\": 2\n                }\n            },\n            \"required\": [\"$and\"],\n            \"additionalProperties\": false\n        },\n        {\n            \"properties\": {\n                \"$or\": {\n                    \"type\": \"array\",\n                    \"items\": {\"$ref\": \"#\"},\n                    \"minItems\": 2\n                }\n            },\n            \"required\": [\"$or\"],\n            \"additionalProperties\": false\n        }\n    ]\n}\n</code></pre>"},{"location":"core/filters/#contains-contains_1","title":"Contains (<code>$contains</code>)","text":"<p>Case-Sensitive</p> <p>The <code>$contains</code> document filter performs a case-sensitive full-text search. For example, <code>{\"$contains\": \"Hello\"}</code> will not match a document containing only <code>\"hello\"</code>.</p> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where_document={\"$contains\": \"search_string\"}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    whereDocument: { $contains: \"search_string\" },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhereDocument(chroma.Contains(\"search_string\")),\n)\n</code></pre> <pre><code>let results = collection.search(vec![\n    SearchPayload::default()\n        .r#where(Key::Document.contains(\"search_string\"))\n        .rank(knn_query.clone())\n        .limit(Some(2), 0),\n]).await?;\n</code></pre>"},{"location":"core/filters/#not-contains-not_contains_1","title":"Not Contains (<code>$not_contains</code>)","text":"PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where_document={\"$not_contains\": \"search_string\"}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    whereDocument: { $not_contains: \"search_string\" },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhereDocument(chroma.NotContains(\"search_string\")),\n)\n</code></pre> <pre><code>let results = collection.search(vec![\n    SearchPayload::default()\n        .r#where(Key::Document.not_contains(\"search_string\"))\n        .rank(knn_query.clone())\n        .limit(Some(2), 0),\n]).await?;\n</code></pre>"},{"location":"core/filters/#regex-regex","title":"Regex (<code>$regex</code>)","text":"<p>Matches documents whose content matches the given regular expression pattern.</p> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where_document={\"$regex\": \"search.*pattern\"}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    whereDocument: { $regex: \"search.*pattern\" },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhereDocument(chroma.Regex(\"search.*pattern\")),\n)\n</code></pre> <pre><code>let results = collection.search(vec![\n    SearchPayload::default()\n        .r#where(Key::Document.regex(\"search.*pattern\"))\n        .rank(knn_query.clone())\n        .limit(Some(2), 0),\n]).await?;\n</code></pre>"},{"location":"core/filters/#not-regex-not_regex","title":"Not Regex (<code>$not_regex</code>)","text":"<p>Matches documents whose content does not match the given regular expression pattern.</p> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where_document={\"$not_regex\": \"exclude.*pattern\"}\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    whereDocument: { $not_regex: \"exclude.*pattern\" },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhereDocument(chroma.NotRegex(\"exclude.*pattern\")),\n)\n</code></pre> <pre><code>let results = collection.search(vec![\n    SearchPayload::default()\n        .r#where(Key::Document.not_regex(\"exclude.*pattern\"))\n        .rank(knn_query.clone())\n        .limit(Some(2), 0),\n]).await?;\n</code></pre>"},{"location":"core/filters/#logical-operator-and-and_1","title":"Logical Operator: And (<code>$and</code>)","text":"PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where_document={\n        \"$and\": [\n            {\"$contains\": \"search_string1\"},\n            {\"$contains\": \"search_string2\"},\n        ]\n    },\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    whereDocument: {\n        $and: [{ $contains: \"search_string1\" }, { $contains: \"search_string2\" }],\n    },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhereDocument(chroma.AndDocument(\n        chroma.Contains(\"search_string1\"),\n        chroma.Contains(\"search_string2\"),\n    )),\n)\n</code></pre> <pre><code>let results = collection.search(vec![\n    SearchPayload::default()\n        .r#where(\n            (Key::Document.contains(\"search_string1\"))\n                &amp; (Key::Document.contains(\"search_string2\")),\n        )\n        .rank(knn_query.clone())\n        .limit(Some(2), 0),\n]).await?;\n</code></pre> <p>Logical Operators can be nested.</p> PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where_document={\n        \"$and\": [\n            {\"$contains\": \"search_string1\"},\n            {\n                \"$or\": [\n                    {\"$not_contains\": \"search_string2\"},\n                    {\"$not_contains\": \"search_string3\"},\n                ]\n            },\n        ]\n    },\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    whereDocument: {\n        $and: [\n            { $contains: \"search_string1\" },\n            {\n                $or: [\n                    { $not_contains: \"search_string2\" },\n                    { $not_contains: \"search_string3\" },\n                ],\n            },\n        ],\n    },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhereDocument(chroma.AndDocument(\n        chroma.Contains(\"search_string1\"),\n        chroma.OrDocument(\n            chroma.NotContains(\"search_string2\"),\n            chroma.NotContains(\"search_string3\"),\n        ),\n    )),\n)\n</code></pre> <pre><code>let results = collection.search(vec![\n    SearchPayload::default()\n        .r#where(\n            (Key::Document.contains(\"search_string1\"))\n                &amp; ((Key::Document.not_contains(\"search_string2\"))\n                    | (Key::Document.not_contains(\"search_string3\"))),\n        )\n        .rank(knn_query.clone())\n        .limit(Some(2), 0),\n]).await?;\n</code></pre>"},{"location":"core/filters/#logical-operator-or-or_1","title":"Logical Operator: Or (<code>$or</code>)","text":"PythonTypeScriptGoRust <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2,\n    where_document={\n        \"$or\": [\n            {\"$not_contains\": \"search_string1\"},\n            {\"$not_contains\": \"search_string2\"},\n        ]\n    },\n)\n</code></pre> <pre><code>const results = await collection.query({\n    queryTexts: [\"This is a query document\"],\n    nResults: 2,\n    whereDocument: {\n        $or: [\n            { $not_contains: \"search_string1\" },\n            { $not_contains: \"search_string2\" },\n        ],\n    },\n});\n</code></pre> <pre><code>results, _ := collection.Query(ctx,\n    chroma.WithQueryTexts(\"This is a query document\"),\n    chroma.WithNResults(2),\n    chroma.WithWhereDocument(chroma.OrDocument(\n        chroma.NotContains(\"search_string1\"),\n        chroma.NotContains(\"search_string2\"),\n    )),\n)\n</code></pre> <pre><code>let results = collection.search(vec![\n    SearchPayload::default()\n        .r#where(\n            (Key::Document.not_contains(\"search_string1\"))\n                | (Key::Document.not_contains(\"search_string2\")),\n        )\n        .rank(knn_query.clone())\n        .limit(Some(2), 0),\n]).await?;\n</code></pre>"},{"location":"core/filters/#pagination","title":"Pagination","text":"<p><code>Collection.get()</code> allows users to specify page details <code>limit</code> and <code>offset</code>.</p> PythonTypeScriptGoRust <pre><code>results = collection.get(limit=10, offset=20)\n</code></pre> <pre><code>const results = await collection.get({ limit: 10, offset: 20 });\n</code></pre> <pre><code>results, _ := collection.Get(ctx,\n    chroma.WithLimit(10),\n    chroma.WithOffset(20),\n)\n</code></pre> <pre><code>let results = collection.get(\n    None,       // ids\n    None,       // where\n    Some(10),   // limit\n    Some(20),   // offset\n    None,       // include\n).await?;\n</code></pre> <p></p>"},{"location":"core/filters/#interactive-filter-playground-cloud-local","title":"Interactive Filter Playground (Cloud + Local)","text":"<p>Use this interactive sandbox to sketch a filter payload before running Chroma. Switch between Cloud and Local tabs to see how the client code changes while the filter shape remains consistent. For nested logic and full schema control, switch either section to JSON mode.</p> <p>Filter Playground</p> <p>         Build metadata (<code>where</code>) or document-text (<code>where_document</code>) filters and preview generated code.       </p> Cloud Local Reset <p></p> <p>Cloud advanced options</p> Expand Ranking Vector KNN Hybrid RRF No rank expression Dense query KNN limit Hybrid sparse query Sparse key RRF (k, weights) Pagination Limit Offset Field selection #document #score #metadata #embedding Metadata keys (csv) Grouping / aggregation Group keys (csv) min_k max_k Batch operations Batch size <p>Local query options</p> Expand Call query() get() Limit Offset (get) Include fields documents metadatas distances embeddings <p>Metadata filters (<code>where</code>)</p> Join $and $or JSON mode Add condition <p>Document filters (<code>where_document</code>)</p> Join $and $or JSON mode Add condition Filter JSON Python TypeScript <p>Filter JSON</p> Copy <pre><code></code></pre> <p>   Playground scope: this is a learning aid for composing filter payloads and starter client code; it is not a full schema validator. </p>"},{"location":"core/install/","title":"Installation","text":"<p>Chroma provides packages for Python, JavaScript/TypeScript, Go, and Rust.</p>"},{"location":"core/install/#quick-start","title":"Quick Start","text":"<p>Get a Chroma server running quickly with the CLI or Docker:</p> CLI (pip)CLI (Standalone cURL)Docker <pre><code>pip install chromadb\nchroma run --path ./getting-started\n</code></pre> <pre><code>curl -sSL https://raw.githubusercontent.com/chroma-core/chroma/main/rust/cli/install/install.sh | bash\nchroma run --path ./getting-started\n</code></pre> <pre><code>docker pull chromadb/chroma:1.5.1 &amp;&amp; docker run -p 8000:8000 chromadb/chroma:1.5.1\n</code></pre> Version pinning <p>Avoid relying on <code>latest</code> for production or repeatable environments. Pin to a specific image tag (for example <code>chromadb/chroma:1.5.1</code>) and upgrade intentionally after validation.</p>"},{"location":"core/install/#chroma-cli-standalone-installer","title":"Chroma CLI (Standalone Installer)","text":"<p>If you prefer not to install Chroma through Python or JavaScript package managers, use the standalone CLI installer:</p> cURL (macOS/Linux)Windows (PowerShell) <pre><code>curl -sSL https://raw.githubusercontent.com/chroma-core/chroma/main/rust/cli/install/install.sh | bash\n</code></pre> <pre><code>iex ((New-Object System.Net.WebClient).DownloadString('https://raw.githubusercontent.com/chroma-core/chroma/main/rust/cli/install/install.ps1'))\n</code></pre> <p>The standalone installer installs the <code>chroma</code> CLI binary so you can run:</p> <pre><code>chroma run --path ./getting-started\n</code></pre> <p>For the full CLI installation matrix (<code>pip</code>, <code>pipx</code>, <code>uv</code>, <code>npm</code>, <code>pnpm</code>, <code>bun</code>, <code>yarn</code>, and standalone installers), see Running Chroma and the official CLI installation docs.</p>"},{"location":"core/install/#python","title":"Python","text":"<p>The <code>chromadb</code> package includes everything needed for both local (embedded) usage and connecting to a remote Chroma server. Here's a minimal working example to confirm your installation.</p> Backward compatibility <p>While Chroma strives to be as compatible with older versions as possible, certain releases introduce breaking changes and most importantly database migrations. All database migrations are irreversible and once upgraded to a new version of Chroma, you cannot downgrade to an older version.</p> Releases <p>You can find Chroma releases on PyPI here.</p> Latest ReleaseLatest main branchSpecific VersionFrom PR Branch <pre><code>pip install chromadb\n</code></pre> <p>Directly from GitHub:</p> <pre><code>pip install git+https://github.com/chroma-core/chroma.git@main\n</code></pre> <p>From test PyPI:</p> <pre><code>pip install --index-url https://test.pypi.org/simple/ chromadb\n</code></pre> <p>Installing a specific version of Chroma is useful when you want to ensure that your code works with a specific version of Chroma. To install a specific version of Chroma, run:</p> <p>From PyPI:</p> <pre><code>pip install chromadb==&lt;x.y.z&gt;\n</code></pre> <p>Directly from GitHub (replace <code>x.y.z</code> with the tag of the version you want to install):</p> <pre><code>pip install git+https://github.com/chroma-core/chroma.git@x.y.z\n</code></pre> <p>It is sometimes useful to install a version of Chroma that has still some unreleased functionality. Like a PR that either fixes a bug or brings in a new functionality you may need. To test such unreleased code it is possible to install directly from a GH PR branch.</p> <pre><code>pip install git+https://github.com/chroma-core/chroma.git@&lt;branch_name&gt;\n</code></pre>"},{"location":"core/install/#javascripttypescript","title":"JavaScript/TypeScript","text":"<p>To install the Chroma JS/TS client package, use the following command depending on your package manager. Here's a minimal working example to confirm your installation.</p> YarnNPMPNPM <pre><code>yarn add chromadb @chroma-core/default-embed\n</code></pre> <pre><code>npm install --save chromadb @chroma-core/default-embed\n</code></pre> <pre><code>pnpm add chromadb @chroma-core/default-embed\n</code></pre> Embedding Function Packages <p>All embedding function packages for JS/TS use the <code>@chroma-core/*</code> namespace. For example:</p> <ul> <li><code>@chroma-core/default-embed</code> - Default embedding function</li> <li><code>@chroma-core/openai</code> - OpenAI embeddings</li> <li><code>@chroma-core/cohere</code> - Cohere embeddings</li> </ul>"},{"location":"core/install/#go","title":"Go","text":"<pre><code>go get github.com/amikos-tech/chroma-go\n</code></pre> <p>Here's a minimal working example to confirm your installation.</p>"},{"location":"core/install/#rust","title":"Rust","text":"<pre><code>cargo add chroma\n</code></pre> <p>Here's a minimal working example to confirm your installation.</p>"},{"location":"core/resources/","title":"Resource Requirements","text":"<p>Chroma makes use of the following compute resources:</p> <ul> <li>RAM - In single-node/self-hosted deployments, Chroma keeps vector search structures in memory for low-latency search.</li> <li>Disk - Chroma persists data to disk, including the vector index files, metadata index, system DB, and write-ahead   log (WAL).</li> <li>CPU - Chroma uses CPU for ingest, indexing, filtering, and search execution.</li> </ul> <p>Use the formulas and heuristics below to estimate Chroma resource needs.</p> <p>Scope</p> <p>This page is primarily for single-node/self-hosted sizing (HNSW-based indexing). Distributed/Cloud deployments use different index internals (SPANN) and different resource behavior. See Core Concepts and SPANN Index Configuration for the distributed model.</p>"},{"location":"core/resources/#ram","title":"RAM","text":"<p>Once you select your embedding model, start with this lower-bound estimate for vector payload memory:</p> payload_bytes = vectors \u00d7 dimensions \u00d7 4 payload_gib = payload_bytes 1024 3 <p>Example (<code>10,000,000</code> vectors, <code>1536</code> dimensions):</p> payload_bytes = 10,000,000 \u00d7 1536 \u00d7 4 = 61,440,000,000 payload_gib = 61,440,000,000 1024 3 \u2248 57.2 \u00a0GiB <ul> <li><code>number of vectors</code> - This is the number of vectors you plan to index. These are the documents in your Chroma   collection (or chunks if you use LlamaIndex or LangChain terminology).</li> <li><code>dimensionality of vectors</code> - This is the dimensionality of the vectors output by your embedding model. For example,   if you use the <code>sentence-transformers/paraphrase-MiniLM-L6-v2</code> model, the dimensionality of the vectors is 384.</li> <li><code>4 bytes</code> - This is the size of each component of a vector. Chroma relies on HNSW lib implementation that uses 32bit   floats.</li> </ul> <p>Treat this as a lower-bound estimate. Real memory usage is higher because of:</p> <ul> <li>HNSW graph/link overhead (affected by HNSW settings such as <code>hnsw:M</code> and <code>hnsw:construction_ef</code>)</li> <li>In-memory bruteforce buffer (<code>hnsw:batch_size</code>) before vectors are merged into HNSW</li> <li>Query-time and filter working memory, plus normal process overhead</li> </ul> <p>For production sizing, add headroom and validate under realistic load.</p>"},{"location":"core/resources/#quick-resource-calculator","title":"Quick Resource Calculator","text":"<p>Use this lightweight estimator to get a practical starting point.</p> Dataset size (number of vectors) 10k 100k 1M 10M          Selected:         - Embedding model (dimension) MiniLM (384) Cohere (1024) OAI 3-small (1536) OAI 3-large (3072) Memory profile Balanced (+30%) Lean prototype (+15%) Heavy query load (+50%) Storage profile Typical mixed (~3x) Vector-heavy (~2x) Metadata-heavy (~4x) Vector payload - Estimated RAM - Estimated Disk - CPU hint - <p>     This calculator is for single-node/self-hosted sizing. For distributed/cloud (SPANN), see     Core Concepts and     SPANN Index Configuration.   </p> <p>Important: vector-first estimate</p> <p>This calculator is vector-first and does not fully model document or index growth. If you store large documents (near model context limits), document and metadata storage can exceed vector storage. Chroma also maintains a SQLite full-text index (<code>embedding_fulltext_search</code>) used by <code>where_document</code> queries, which adds disk overhead. See Storage Layout and Filters for details.</p>"},{"location":"core/resources/#disk","title":"Disk","text":"<p>Disk storage requirements depend on vectors, documents, metadata, and WAL behavior.</p> <p>Use these heuristics:</p> <ul> <li>Start with at least the raw vector footprint plus several GB for metadata/documents/system data</li> <li>In many workloads, <code>2-4x</code> the vector payload estimate is a reasonable planning range</li> <li>If you store large documents/metadata blobs, disk needs can exceed that range</li> <li>If you rely on document filtering (<code>where_document</code>), plan additional space for SQLite FTS index structures</li> </ul> <p>WAL Cleanup</p> <p>Recent Chroma versions support automatic WAL pruning. If your persistent data directory was created on older versions, or if auto-pruning is disabled, run WAL cleanup once and verify WAL config. See WAL Pruning and Maintenance.</p>"},{"location":"core/resources/#temporary-disk-space","title":"Temporary Disk Space","text":"<p>Chroma uses temporary storage for its SQLite3 related operations - sorting and buffering large queries. By default, SQLite3 uses <code>/tmp</code> for temporary storage.</p> <p>There are two guidelines to follow:</p> <ul> <li>Have enough space if your application intends to make large queries or has multiple concurrent queries.</li> <li>Ensure temporary storage is on a fast disk to avoid performance bottlenecks.</li> </ul> <p>You can configure the location of sqlite temp files with the <code>SQLITE_TMPDIR</code> environment variable.</p> <p>SQLite3 Temporary Storage</p> <p>You can read more about SQLite3 temporary storage in the SQLite3 documentation.</p>"},{"location":"core/resources/#cpu","title":"CPU","text":"<p>There are no hard requirements for the CPU, but it is recommended to use as much CPU as you can spare as it directly relates to index and search speeds.</p>"},{"location":"core/storage-layout/","title":"Storage Layout","text":"<p>When configured as <code>PersistentClient</code> or running as a server, Chroma persists its data under the provided <code>persist_directory</code>.</p> <p>For <code>PersistentClient</code> the persistent directory is usually passed as <code>path</code> parameter when creating the client, if not passed the default is <code>./chroma/</code> (relative path to where the client is started from).</p> <p>For the server, the persistent directory can be passed as environment variable <code>PERSIST_DIRECTORY</code> or as a command line argument <code>--path</code>. If not passed, the default is <code>./chroma/</code> (relative path to where the server is started).</p> <p>Once the client or the server is started a basic directory structure is created under the persistent directory containing the <code>chroma.sqlite3</code> file. Once collections are created and data is added, subdirectories are created for each collection. The subdirectories are UUID-named and refer to the vector segment.</p> Chroma Ops - Maintenance CLI <p>If you are looking maintenance CLI that can help you inspect, configure and improve the performance of your Chroma, try Chroma Ops.</p>"},{"location":"core/storage-layout/#directory-structure","title":"Directory Structure","text":"<p>The following diagram represents a typical Chroma persistent directory structure:</p> <p></p>"},{"location":"core/storage-layout/#chromasqlite3","title":"<code>chroma.sqlite3</code>","text":"<p>Note about the tables</p> <p>While we try to make it as accurate as possible chroma data layout inside the <code>slite3</code> database is subject to change. The following description is valid as of version <code>0.5.0</code>. The tables are also not representative of the distributed architecture of Chroma.</p> <p>The <code>chroma.sqlite3</code> is typical for Chroma single-node. The file contains the following four types of data:</p> <ul> <li>Sysdb - Chroma system database, responsible for storing tenant, database, collection and segment information.</li> <li>WAL - the write-ahead log, which is used to ensure durability of the data.</li> <li>Metadata Segment - all metadata and documents stored in Chroma.</li> <li>Migrations - the database schema migration scripts.</li> <li>Collections - each collection has its own subdirectory, a UUIDv4-named diretory which stores HNSW index and its metadata.</li> </ul>"},{"location":"core/storage-layout/#sysdb","title":"Sysdb","text":"<p>The system database comprises the following tables:</p> <ul> <li><code>tenants</code> - contains all the tenants in the system. Usually gets initialized with a single tenant - <code>default_tenant</code>.</li> <li><code>databases</code> - contains all the databases per tenant. Usually gets initialized with a single   database - <code>default_database</code> related to the <code>default_tenant</code>.</li> <li><code>collections</code> - contains all the collections per database.</li> <li><code>collection_metadata</code> - contains all the metadata associated with each collection. The metadata for a collection   consists of any user-specified key-value pairs and the <code>hnsw:*</code> keys that store   the HNSW index parameters.</li> <li><code>segments</code> - contains all the segments per collection. Each collection gets two segments - <code>metadata</code> and <code>vector</code>.</li> <li><code>segment_metadata</code> - contains all the metadata associated with each segment. This table   contains <code>hnsw:*</code> keys that store the HNSW index parameters for the vector   segment.</li> </ul>"},{"location":"core/storage-layout/#wal","title":"WAL","text":"<p>The write-ahead log is a table that stores all the changes made to the database. It is used to ensure that the data is durable and can be recovered in case of a crash. The WAL is composed of the following tables:</p> <ul> <li><code>embeddings_queue</code> - contains all data ingested into Chroma. Each row of the table represents an operation upon a   collection (add, update, delete, upsert). The row contains all the necessary information (embedding, document,   metadata and associated relationship to a collection) to replay the operation and ensure data consistency.</li> <li><code>embeddings_queue_config</code> - contains the configuration for the embedding queue. As of version <code>0.6.3</code> the configuration only pertains to automatic embedding queue purging.</li> <li><code>max_seq_id</code> - maintains the maximum sequence ID of the metadata segment that is used as a WAL replay starting point for   the metadata segment.</li> </ul> Example Queue Configuration<pre><code>{\n  \"automatically_purge\": true,\n  \"_type\": \"EmbeddingsQueueConfigurationInternal\"\n}\n</code></pre>"},{"location":"core/storage-layout/#metadata-segment","title":"Metadata Segment","text":"<p>The metadata segment is a table that stores all the metadata and documents stored in Chroma. The metadata segment is composed of the following tables:</p> <ul> <li><code>embeddings</code> - contains embedding listings for all collections.</li> <li><code>embedding_metadata</code> - contains all the metadata associated with each document and its embedding.</li> <li><code>embedding_fulltext_search</code> - document full-text search index. This is a virtual table and upon inspection of the sqlite   will appear as a series of tables starting with <code>embedding_fulltext_search_</code>. This is an FTS5 table and is used for   full-text search queries on documents stored in Chroma (via <code>where_document</code> filter in <code>query</code> and <code>get</code> methods).</li> </ul>"},{"location":"core/storage-layout/#migrations","title":"Migrations","text":"<p>The migrations table contains all schema migrations applied to the <code>chroma.sqlite3</code> database. The table is used for tracking applied migrations.</p>"},{"location":"core/storage-layout/#collection-subdirectories","title":"Collection Subdirectories","text":"<p>Each collection has its own subdirectory, a UUIDv4-named diretory which stores HNSW index and its metadata.</p> <ul> <li><code>header.bin</code> - Holds metadata about the index, such as its parameters and structure details.</li> <li><code>length.bin</code> - Records the number of links each node has, aiding in efficient traversal during searches.</li> <li><code>link_lists.bin</code> - Stores the adjacency lists for nodes, detailing their connections within the graph.</li> <li><code>data_level0.bin</code> - Contains the base layer of the hierarchical graph, storing the actual vectors and their connections.</li> <li><code>index_metadata.pickle</code> - Chroma specific metadata about mapping between ids in <code>embeddings</code> table and labels in the HNSW index.</li> </ul>"},{"location":"core/system_constraints/","title":"Chroma System Constraints","text":"<p>This section contains common constraints of Chroma.</p> <ul> <li>Chroma is thread-safe</li> <li>Chroma is not process-safe</li> <li>Multiple Chroma Clients (Ephemeral, Persistent, Http) can be created from one or more threads within the same process</li> <li>A collection's name is unique within a Tenant and DB</li> <li>A collection's dimensions cannot change after creation =&gt; you cannot change the embedding function after creation</li> <li>Chroma operates in two modes - standalone (PersistentClient, EphemeralClient) and client/server (HttpClient with   ChromaServer)</li> <li>The distance function cannot be changed after collection creation.</li> </ul>"},{"location":"core/system_constraints/#operational-modes","title":"Operational Modes","text":"<p>Chroma can be operated in two modes:</p> <ul> <li>Standalone - This allows embedding Chroma in your python application without the need to communicate with external   processes.</li> <li>Client/Server - This allows embedding Chroma in your python application as a thin-client with minimal dependencies and   communicating with it via REST API. This is useful when you want to use Chroma from multiple processes or even   multiple machines.</li> </ul> <p>Depending on the mode you choose, you will need to consider the following component responsibilities:</p> <ul> <li>Standalone:<ul> <li>Clients (Persistent, Ephemeral) - Responsible for persistence, embedding, querying</li> </ul> </li> <li>Client/Server:<ul> <li>Clients (HttpClient) - Responsible for embedding, communication with Chroma server via REST API</li> <li>Server - Responsible for persistence and querying</li> </ul> </li> </ul> <p></p>"},{"location":"core/tenants-and-databases/","title":"Tenants and Databases","text":"<p>Tenants and Databases are two grouping abstractions that provides means to organize and manage data in Chroma.</p>"},{"location":"core/tenants-and-databases/#tenants","title":"Tenants","text":"<p>A tenant is a logical grouping of databases.</p>"},{"location":"core/tenants-and-databases/#databases","title":"Databases","text":"<p>A database is a logical grouping of collections.</p>"},{"location":"core/advanced/queries/","title":"Chroma Queries","text":"<p>This page explains what happens after you call <code>get()</code>, <code>query()</code>, or <code>search()</code>.</p> <p>We reuse the same running example from Search Concepts:</p> <ul> <li>Query intent: \"Find troubleshooting docs about SSO login failures.\"</li> <li>Constraints: only <code>status=published</code>, <code>year &gt;= 2024</code></li> <li>Output goal: top 20 results (<code>title</code> + <code>score</code>), without one product area dominating.</li> </ul>"},{"location":"core/advanced/queries/#core-concepts","title":"Core Concepts","text":"<p>Modern Rust Chroma has two query execution paths:</p> <ul> <li>Local single-node (<code>query()/get()</code> path): local executor over <code>Sqlite</code> metadata + local HNSW segments.</li> <li>Distributed / Cloud (<code>search()</code> / distributed query workers): worker orchestrators over distributed segments (<code>BlockfileMetadata</code>, <code>BlockfileRecord</code>, <code>HnswDistributed</code>, <code>Spann</code>, <code>QuantizedSpann</code>) plus WAL/log materialization.</li> </ul> <p>When a query runs, Chroma pulls from three storage/index families:</p> Need Local single-node Distributed / Cloud Metadata + documents SQLite tables + FTS5 (<code>where_document</code>) in <code>chroma.sqlite3</code> Blockfile-backed record/metadata segments + recent log materialization Vector ANN Local HNSW segments (persisted or memory) <code>HnswDistributed</code>, <code>Spann</code>, or <code>QuantizedSpann</code> (by collection config) Ranking primitives KNN (<code>query</code>) KNN + sparse paths + rank expressions/fusion (<code>search</code>) <p>For deeper Rust internals (segment families and Arrow-backed blockfiles), see Concepts and Blockfile Format and Update Model.</p>"},{"location":"core/advanced/queries/#query-pipeline","title":"Query Pipeline","text":"<p>Quick flow:</p> <ol> <li>Validation: reject malformed requests early.</li> <li>Candidate selection: apply filters/IDs to decide which records can compete.</li> <li>KNN/rank evaluation: score and order eligible records.</li> <li>Field loading (projection/select): fetch the fields you asked for.</li> <li>Result aggregation: return final rows in the requested shape.</li> </ol> <p>The same idea in one line: filter -&gt; score -&gt; load fields -&gt; return page.</p> Query Pipeline? <p>Why is it called a pipeline? Because each step in the query process depends on its predecessor's output.</p> <p></p>"},{"location":"core/advanced/queries/#advanced-search-semantics","title":"Advanced Search Semantics (Cloud + Local)","text":"<p>This section explains how query composition works beyond filter syntax.</p>"},{"location":"core/advanced/queries/#stage-model","title":"Stage Model","text":"<p>The same query intent is typically expressed as:</p> <ol> <li>Candidate selection (filters)</li> <li>Relevance ranking</li> <li>Optional ranking fusion (hybrid)</li> <li>Optional grouping/aggregation</li> <li>Response shaping (pagination + field selection)</li> </ol> Stage What it controls Local <code>query()/get()</code> Cloud Search API Candidate selection Which records are eligible <code>where</code>, <code>where_document</code> <code>where(...)</code> Relevance ranking Order of eligible records KNN over query embeddings <code>rank(Knn(...))</code> or other rank expressions Hybrid fusion Merge multiple ranking signals Not available as native query primitive <code>rank(Rrf(...))</code> Grouping/aggregation Per-group diversity and caps Not available as native query primitive <code>groupBy(...)</code> (<code>MinK</code>/<code>MaxK</code>) Response shaping Size and returned fields <code>limit</code>, <code>offset</code>, <code>include</code> <code>limit</code>, pagination, <code>select</code> <p>What each stage means:</p> <ul> <li>Filters decide what gets into the eligible pool.</li> <li>Ranking decides what goes first.</li> <li>Fusion/grouping decide how to balance quality and diversity.</li> <li>Response shaping decides what the client actually gets back.</li> </ul>"},{"location":"core/advanced/queries/#boundary-rules","title":"Boundary Rules","text":"<ul> <li>Filters (<code>where</code>, <code>where_document</code>) define eligibility, not final order.</li> <li>Ranking defines order among eligible candidates.</li> <li>RRF is a rank-fusion step for combining multiple ranked lists.</li> <li>Grouping/aggregation reshapes already-ranked results.</li> <li>Pagination and field selection shape the final payload.</li> </ul>"},{"location":"core/advanced/queries/#practical-tradeoffs","title":"Practical Tradeoffs","text":"<ul> <li>Filter selectivity affects latency: tighter filters reduce the eligible pool before similarity search.</li> <li>Ranking depth affects recall and latency: deeper rank windows can improve recall but increase cost.</li> <li>Hybrid helps lexical plus semantic intent: use RRF when exact token matches and semantic matches both matter.</li> <li>Grouping improves diversity: use group caps when one category can dominate top results.</li> </ul> <p>Example:</p> <ul> <li>Without grouping, top results might all come from <code>product_area=auth</code>.</li> <li>With <code>groupBy(product_area)</code> and a per-group cap, results are more balanced across areas.</li> </ul>"},{"location":"core/advanced/queries/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Filters for exact operator syntax.</li> <li>Search Concepts for the conceptual pipeline.</li> <li>Search API Overview for request composition.</li> <li>Ranking and Scoring, Hybrid Search with RRF, and Group By &amp; Aggregation for Cloud behavior details.</li> </ul>"},{"location":"core/advanced/queries/#validation","title":"Validation","text":"<p>The following validations are performed:</p> <ul> <li>Validate <code>where</code> if present</li> <li>Validate <code>where_document</code> if present</li> <li>Ensure collection exists</li> <li>Validate query embeddings dimensions match that of the collection</li> </ul> <p>Common failure patterns:</p> <ul> <li>typo in filter operator</li> <li>querying a collection with incompatible embedding dimensions</li> <li>requesting unsupported fields/options for the chosen API path</li> </ul>"},{"location":"core/advanced/queries/#metadata-pre-filter","title":"Metadata Pre-Filter","text":"<p>Metadata pre-filter is the first narrowing step for filtered queries.</p> <ul> <li>In local/single-node Chroma, this stage evaluates <code>where</code> and <code>where_document</code> against the SQLite metadata segment.</li> <li><code>where_document</code> is backed by SQLite FTS5 (<code>embedding_fulltext_search</code>) as documented in Storage Layout.</li> <li>The output is an eligible ID set passed to the ANN stage; if no filters are provided, this stage is skipped.</li> </ul> <p>Why this matters:</p> <ul> <li>Executing predicates before ANN search reduces unnecessary distance computations.</li> <li>Highly selective filters can reduce latency, but may also reduce the number of final hits if few records satisfy the predicate.</li> </ul> <p>In our running example:</p> <ul> <li><code>where: {\"status\":\"published\",\"year\":{\"$gte\":2024}}</code></li> <li><code>where_document: {\"$contains\":\"SSO\"}</code></li> <li>Only matching IDs move on to KNN/rank.</li> </ul> <p>Research context:</p> <ul> <li>Filtered ANN is known to be harder than unfiltered ANN because vector proximity and predicate selectivity can be weakly correlated.</li> <li>See ACORN (Patel et al., 2024) and Filtered-DiskANN (Gollapudi et al., WWW 2023) for predicate-aware ANN design tradeoffs.</li> </ul>"},{"location":"core/advanced/queries/#knn-search-in-hnsw-index","title":"KNN Search in HNSW Index","text":"<p>This stage runs approximate nearest-neighbor search over the collection's HNSW vector index.</p> <ul> <li>Chroma local stores one HNSW index per collection (see Storage Layout).</li> <li>If metadata pre-filter produced eligible IDs, the KNN stage searches within that constrained set.</li> <li>If <code>include</code> requests embeddings, vectors can be returned as part of this stage.</li> </ul> <p>In our running example:</p> <ul> <li>Chroma scores the query embedding against only the allowed IDs.</li> <li>Lower distance means closer semantic match.</li> </ul> <p>Tuning and tradeoffs:</p> <ul> <li><code>ef_search</code> controls how many neighbors are explored at query time; increasing it typically improves recall at higher latency/memory cost.</li> <li>Construction knobs such as <code>max_neighbors</code> (HNSW <code>M</code>) and <code>ef_construction</code> influence graph quality and memory/ingest cost.</li> <li>Use current index configuration surfaces (<code>configuration.hnsw</code> / <code>configuration.spann</code>) rather than legacy <code>metadata</code> keys (see Configuration).</li> <li>For Cloud's schema-based configuration model (<code>VectorIndexConfig</code>, <code>HnswConfig</code>, <code>SpannConfig</code>), see Index Configuration Reference and Schema Basics.</li> </ul> <p>Research context:</p> <ul> <li>HNSW's core idea is hierarchical small-world graph traversal with strong empirical recall/latency tradeoffs (Malkov &amp; Yashunin, 2016/2018).</li> <li>For Cloud Search API ranking semantics (distance-style ordering, candidate limits, and expression behavior), see Ranking and Scoring.</li> </ul>"},{"location":"core/advanced/queries/#post-search-field-loading-projection-hydration","title":"Post-Search Field Loading (Projection / Hydration)","text":"<p>After KNN returns ranked records (<code>offset_id</code> + score), Chroma loads the requested fields to build the final payload.</p> <ul> <li>Local single-node path: the local executor performs an explicit follow-up <code>Get</code> over returned IDs to load documents/metadata when requested.</li> <li>Distributed/Cloud query path: worker execution uses <code>ProjectionOrchestrator</code>/<code>KnnProjection</code> (for <code>query/knn</code>) or <code>Select</code> in <code>RankOrchestrator</code> (for <code>search</code>) to load fields from record segments plus recent logs.</li> <li>In distributed execution, this is not a SQLite post-query; it is an operator-stage merge of compacted segment data and WAL/log materialization.</li> </ul> <p>What this means:</p> <ul> <li>Ranking can stay fast because it moves IDs/scores first.</li> <li>Field loading happens afterward, only for the rows you keep.</li> </ul> <p>Why this stage exists:</p> <ul> <li>ANN structures are optimized for vector neighborhood traversal, not full document/metadata row retrieval.</li> <li>Decoupling ANN from payload hydration is a common system pattern in vector retrieval pipelines.</li> </ul> <p>Operational guidance:</p> <ul> <li>Requested fields directly affect this stage cost: keep payload small (<code>include</code> in local APIs, <code>select</code> in Cloud Search API) when low latency is important.</li> <li>For Cloud query composition and output shaping, see Search API Overview and Pagination &amp; Selection.</li> <li>For single-node metadata/FTS storage details, see Storage Layout.</li> </ul>"},{"location":"core/advanced/queries/#result-aggregation","title":"Result Aggregation","text":"<p>Result aggregation fuses ranked neighbors with hydrated payloads and requested fields into the final response shape.</p> <p>Example: if your request asks for <code>id</code>, <code>document</code>, and <code>score</code>, this is the stage that combines them into final rows (plus pagination/grouping effects, if configured).</p>"},{"location":"core/advanced/wal-pruning/","title":"Write-ahead Log (WAL) Pruning","text":"<p>Chroma Write-Ahead Log is unbounded by default and grows indefinitely. This can lead to high disk usage and slow performance. To prevent this, it is recommended to prune/cleanup the WAL periodically. Below we offer a couple of tools, including an official and recommended CLI tool, to help you prune your WAL.</p>"},{"location":"core/advanced/wal-pruning/#tooling","title":"Tooling","text":"<p>There are two ways to prune your WAL:</p> <ul> <li>Chroma CLI - this is the official tooling provided by Chroma and is the recommended way to prune your WAL. This   functionality is available either from <code>main</code> branch or Chroma release <code>&gt;0.5.5</code>.</li> <li>chroma-ops</li> </ul>"},{"location":"core/advanced/wal-pruning/#chroma-cli","title":"Chroma CLI","text":"<p>To prune your WAL you need to install Chroma CLI (it comes as part of the core Chroma package):</p> <pre><code>pip install chromadb\n\nchroma utils vacuum --path /path/to/persist_dir\n</code></pre> <p>Auto-pruning</p> <p>Running the above command will enable auto WAL pruning.  This means that Chroma will periodically prune the WAL during its normal operations.</p>"},{"location":"core/advanced/wal-pruning/#chroma-ops","title":"Chroma Ops","text":"<p>To prune your WAL you can run the following command:</p> <pre><code>pip install chroma-ops\nchops cleanup-wal /path/to/persist_dir\n</code></pre> <p>\u26a0\ufe0f IMPORTANT: It is always a good thing to backup your data before you prune the WAL.</p>"},{"location":"core/advanced/wal-pruning/#manual","title":"Manual","text":"<p>Steps:</p> <p>Stop Chroma</p> <p>It is vitally important that you stop Chroma before you prune the WAL.  If you don't stop Chroma you risk corrupting</p> <ul> <li>\u26a0\ufe0f Stop Chroma</li> <li>\ud83d\udcbe Create a backup of your <code>chroma.sqlite3</code> file in your persistent dir</li> <li>\ud83d\udc40 Check your current <code>chroma.sqlite3</code> size (e.g. <code>ls -lh /path/to/persist/dir/chroma.sqlite3</code>)</li> <li>\ud83d\udda5\ufe0f Run the script below</li> <li>\ud83d\udd2d Check your current <code>chroma.sqlite3</code> size again to verify that the WAL has been pruned</li> <li>\ud83d\ude80 Start Chroma</li> </ul> <p>Script (store it in a file like <code>compact-wal.sql</code>)</p> wal_clean.py<pre><code>#!/usr/bin/env python3\n# Call the script: python wal_clean.py ./chroma-test-compact\nimport os\nimport sqlite3\nfrom typing import cast, Optional, Dict\nimport argparse\nimport pickle\n\n\nclass PersistentData:\n    \"\"\"Stores the data and metadata needed for a PersistentLocalHnswSegment\"\"\"\n\n    dimensionality: Optional[int]\n    total_elements_added: int\n    max_seq_id: int\n\n    id_to_label: Dict[str, int]\n    label_to_id: Dict[int, str]\n    id_to_seq_id: Dict[str, int]\n\n\ndef load_from_file(filename: str) -&gt; \"PersistentData\":\n    \"\"\"Load persistent data from a file\"\"\"\n    with open(filename, \"rb\") as f:\n        ret = cast(PersistentData, pickle.load(f))\n        return ret\n\n\ndef clean_wal(chroma_persist_dir: str):\n    if not os.path.exists(chroma_persist_dir):\n        raise Exception(f\"Persist {chroma_persist_dir} dir does not exist\")\n    if not os.path.exists(f'{chroma_persist_dir}/chroma.sqlite3'):\n        raise Exception(\n            f\"SQL file not found int persist dir {chroma_persist_dir}/chroma.sqlite3\")\n    # Connect to SQLite database\n    conn = sqlite3.connect(f'{chroma_persist_dir}/chroma.sqlite3')\n\n    # Create a cursor object\n    cursor = conn.cursor()\n\n    # SQL query\n    query = \"SELECT id,topic FROM segments where scope='VECTOR'\"  # Replace with your query\n\n    # Execute the query\n    cursor.execute(query)\n\n    # Fetch the results (if needed)\n    results = cursor.fetchall()\n    wal_cleanup_queries = []\n    for row in results:\n        # print(row)\n        metadata = load_from_file(\n            f'{chroma_persist_dir}/{row[0]}/index_metadata.pickle')\n        wal_cleanup_queries.append(\n            f\"DELETE FROM embeddings_queue WHERE seq_id &lt; {metadata.max_seq_id} AND topic='{row[1]}';\")\n\n    cursor.executescript('\\n'.join(wal_cleanup_queries))\n    # Close the cursor and connection\n    cursor.close()\n    conn.close()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('persist_dir', type=str)\n    arg = parser.parse_args()\n    print(arg.persist_dir)\n    clean_wal(arg.persist_dir)\n</code></pre> <p>Run the script</p> <pre><code># Let's create a backup\ntar -czvf /path/to/persist/dir/chroma.sqlite3.backup.tar.gz /path/to/persist/dir/chroma.sqlite3\nlsof /path/to/persist/dir/chroma.sqlite3 # make sure that no process is using the file\npython wal_clean.py /path/to/persist/dir/\n# start chroma\n</code></pre>"},{"location":"core/advanced/wal/","title":"Write-ahead Log (WAL)","text":"<p>Chroma uses WAL to ensure data durability, even if things go wrong (e.g. server crashes). To achieve the latter Chroma uses what is known in the DB-industry as WAL or Write-Ahead Log. The purpose of the WAL is to ensure that each user request (aka transaction) is safely stored before acknowledging back to the user. Subsequently, in fact immediately after writing to the WAL, the data is also written to the index. This enables Chroma to serve as real-time search engine, where the data is available for querying immediately after it is written to the WAL.</p> <p>Below is a diagram that illustrates the WAL in ChromaDB (ca. 0.6.3):</p> <p></p>"},{"location":"core/advanced/wal/#vector-indices-overview","title":"Vector Indices Overview","text":"<p>The diagram below illustrates how data gets transferred from the WAL to the binary vector indices (Bruteforce and HNSW):</p> <p></p> <p>For each collection Chroma maintains two binary indices - Bruteforce (in-memory, fast) and HNSW lib (persisted to disk, slow when adding new vectors and persisting). As you can imagine, the BF index serves the role of a buffer that holds the uncommitted to HNWS persisted index portion of the WAL. The HNSW index itself has a max sequence id counter, stored in a metadata file, that indicates from which position in the WAL the buffering to the BF index should begin. The latter buffering usually happens when the collection is first accessed.</p> <p>There are two transfer points (in the diagram, sync threshold) for BF to HNSW:</p> <ul> <li><code>hnsw:batch_size</code> - forces the BF vectors to be added to HNSW in-memory (this is a slow operation)</li> <li> <p><code>hnsw:sync_threshold</code> - forces Chroma to dump the HNSW in-memory index to disk (this is a slow operation)</p> </li> <li> <p>Both of the above sync points are controlled via Collection-level metadata with respective named params. It is   customary <code>hnsw:sync_threshold</code> &gt; <code>hnsw:batch_size</code></p> </li> </ul>"},{"location":"core/advanced/wal/#metadata-indices-overview","title":"Metadata Indices Overview","text":"<p>The following diagram illustrates how data gets transferred from the WAL to the metadata index:</p> <p></p>"},{"location":"core/advanced/wal/#further-reading","title":"Further Reading","text":"<p>For the DevOps minded folks we have a few more resources:</p> <ul> <li>WAL Pruning - Clean up your WAL</li> </ul>"},{"location":"ecosystem/clients/","title":"Chroma Ecosystem Clients","text":""},{"location":"ecosystem/clients/#python","title":"Python","text":"Maintainer Chroma Core team Repo https://github.com/chroma-core/chroma Status \u2705 Stable Version <code>0.5.5.dev0</code> (PyPi Link) Docs https://docs.trychroma.com/reference/py-client Compatibility Python: <code>3.8+</code>, Chroma API Version: <code>0.5.x</code> <p>Feature Support:</p> Feature Supported Create Tenant \u2705 Get Tenant \u2705 Create DB \u2705 Get DB \u2705 Create Collection \u2705 Get Collection \u2705 List Collection \u2705 Count Collection \u2705 Delete Collection \u2705 Add Documents \u2705 Delete Documents \u2705 Update Documents \u2705 Query Documents \u2705 Get Document \u2705 Count Documents \u2705 Auth - Basic \u2705 Auth - Token \u2705 Reset \u2705 <p>Embedding Function Support:</p> Embedding Function Supported OpenAI \u2705 Sentence Transformers \u2705 HuggingFace Inference API \u2705 Cohere \u2705 Google Vertex AI \u2705 Google Generative AI (Gemini) \u2705 OpenCLIP (Multi-modal) \u2705 <p>Embedding Functions</p> <p>The list above is not exhaustive. Check  official docs for up-to-date information.</p>"},{"location":"ecosystem/clients/#javascript","title":"JavaScript","text":"Maintainer Chroma Core team Repo https://github.com/chroma-core/chroma Status \u2705 Stable Version <code>1.8.1</code> (NPM Link) Docs https://docs.trychroma.com/reference/js-client Compatibility Python: <code>3.7+</code>, Chroma API Version: <code>TBD</code> <p>Feature Support:</p> Feature Supported Create Tenant \u2705 Get Tenant \u2705 Create DB \u2705 Get DB \u2705 Create Collection \u2705 Get Collection \u2705 List Collection \u2705 Count Collection \u2705 Delete Collection \u2705 Add Documents \u2705 Delete Documents \u2705 Update Documents \u2705 Query Documents \u2705 Get Document \u2705 Count Documents \u2705 Auth - Basic \u2705 Auth - Token \u2705 Reset \u2705 <p>Embedding Function Support:</p> Embedding Function Supported OpenAI \u2705 Sentence Transformers \u2705 HuggingFace Inference API \u2705 Cohere \u2705 Google Vertex AI \u2705 Google Generative AI (Gemini) \u2705 OpenCLIP (Multi-modal) \u2705 <p>Embedding Functions</p> <p>The list above is not exhaustive. Check  official docs for up-to-date information.</p>"},{"location":"ecosystem/clients/#ruby-client","title":"Ruby Client","text":"<p>https://github.com/mariochavez/chroma</p>"},{"location":"ecosystem/clients/#java-client","title":"Java Client","text":"<p>https://github.com/amikos-tech/chromadb-java-client</p>"},{"location":"ecosystem/clients/#go-client","title":"Go Client","text":"Maintainer Amikos Tech (Chroma Core contributor) Repo https://github.com/amikos-tech/chroma-go Status \u2705 Stable Version <code>0.1.4</code> (Go Pkg Link) Docs https://go-client.chromadb.dev/ Compatibility Go: <code>1.21+</code>, Chroma API Version: <code>0.5.x</code> <p>Feature Support:</p> Feature Supported Create Tenant \u2705 Get Tenant \u2705 Create DB \u2705 Get DB \u2705 Create Collection \u2705 Get Collection \u2705 List Collection \u2705 Count Collection \u2705 Delete Collection \u2705 Add Documents \u2705 Delete Documents \u2705 Update Documents \u2705 Query Documents \u2705 Get Document \u2705 Count Documents \u2705 Auth - Basic \u2705 Auth - Token \u2705 Reset \u2705 <p>Embedding Function Support:</p> Embedding Function Supported OpenAI \u2705 HuggingFace Inference API \u2705 Cohere \u2705 Google Generative AI (Gemini) \u2705 Mistral AI \u2705 Cloudflare Workers AI) \u2705 Together AI \u2705 Ollama \u2705 Nomic AI \u2705 Hugging Face Embedding Inference Server \u2705"},{"location":"ecosystem/clients/#c-client","title":"C# Client","text":"<p>https://github.com/microsoft/semantic-kernel/tree/main/dotnet/src/Connectors/Connectors.Memory.Chroma</p>"},{"location":"ecosystem/clients/#rust-client","title":"Rust Client","text":"<p>https://crates.io/crates/chromadb</p>"},{"location":"ecosystem/clients/#elixir-client","title":"Elixir Client","text":"<p>https://hex.pm/packages/chroma/</p>"},{"location":"ecosystem/clients/#dart-client","title":"Dart Client","text":"<p>https://pub.dev/packages/chromadb</p>"},{"location":"ecosystem/clients/#php-client","title":"PHP Client","text":"<p>https://github.com/CodeWithKyrian/chromadb-php</p>"},{"location":"ecosystem/clients/#php-laravel-client","title":"PHP (Laravel) Client","text":"<p>https://github.com/helgeSverre/chromadb</p>"},{"location":"embeddings/bring-your-own-embeddings/","title":"Creating your own embedding function","text":"<pre><code>from chromadb.api.types import (\n    Documents,\n    EmbeddingFunction,\n    Embeddings\n)\n\n\nclass MyCustomEmbeddingFunction(EmbeddingFunction[Documents]):\n    def __init__(\n            self,\n            my_ef_param: str\n    ):\n        \"\"\"Initialize the embedding function.\"\"\"\n\n    def __call__(self, input: Documents) -&gt; Embeddings:\n        \"\"\"Embed the input documents.\"\"\"\n        return self._my_ef(input)\n</code></pre> <p>Now let's break the above down.</p> <p>First you create a class that inherits from <code>EmbeddingFunction[Documents]</code>. The <code>Documents</code> type is a list of <code>Document</code> objects. Each <code>Document</code> object has a <code>text</code> attribute that contains the text of the document. Chroma also supports multi-modal</p>"},{"location":"embeddings/bring-your-own-embeddings/#example-implementation","title":"Example Implementation","text":"<p>Below is an implementation of an embedding function that works with <code>transformers</code> models.</p> <p>Note</p> <p>This example requires the <code>transformers</code> and <code>torch</code> python packages. You can install them with <code>pip install transformers torch</code>.</p> <p>By default, all <code>transformers</code> models on HF are supported are also supported by the <code>sentence-transformers</code> package. For which Chroma provides out of the box support.</p> <pre><code>import importlib\nfrom typing import Optional, cast\n\nimport numpy as np\nimport numpy.typing as npt\nfrom chromadb.api.types import EmbeddingFunction, Documents, Embeddings\n\n\nclass TransformerEmbeddingFunction(EmbeddingFunction[Documents]):\n    def __init__(\n            self,\n            model_name: str = \"dbmdz/bert-base-turkish-cased\",\n            cache_dir: Optional[str] = None,\n    ):\n        try:\n            from transformers import AutoModel, AutoTokenizer\n\n            self._torch = importlib.import_module(\"torch\")\n            self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self._model = AutoModel.from_pretrained(model_name, cache_dir=cache_dir)\n        except ImportError:\n            raise ValueError(\n                \"The transformers and/or pytorch python package is not installed. Please install it with \"\n                \"`pip install transformers` or `pip install torch`\"\n            )\n\n    @staticmethod\n    def _normalize(vector: npt.NDArray) -&gt; npt.NDArray:\n        \"\"\"Normalizes a vector to unit length using L2 norm.\"\"\"\n        norm = np.linalg.norm(vector)\n        if norm == 0:\n            return vector\n        return vector / norm\n\n    def __call__(self, input: Documents) -&gt; Embeddings:\n        inputs = self._tokenizer(\n            input, padding=True, truncation=True, return_tensors=\"pt\"\n        )\n        with self._torch.no_grad():\n            outputs = self._model(**inputs)\n        embeddings = outputs.last_hidden_state.mean(dim=1)  # mean pooling\n        return [e.tolist() for e in self._normalize(embeddings)]\n</code></pre>"},{"location":"embeddings/cross-encoders/","title":"Cross-Encoders Reranking","text":"<p>Work in Progress</p> <p>This page is a work in progress and may not be complete.</p> <p>For now this is just a tiny snippet how to use a cross-encoder to rerank results returned from Chroma. Soon we will provide a more detailed guide to the usefulness of cross-encoders/rerankers.</p>"},{"location":"embeddings/cross-encoders/#hugging-face-cross-encoders","title":"Hugging Face Cross Encoders","text":"<pre><code>from sentence_transformers import CrossEncoder\nimport numpy as np\nimport chromadb\nclient = chromadb.Client()\ncollection = client.get_or_create_collection(\"my_collection\")\n# add some documents \ncollection.add(ids=[\"doc1\", \"doc2\", \"doc3\"], documents=[\"Hello, world!\", \"Hello, Chroma!\", \"Hello, Universe!\"])\n# query the collection\nquery = \"Hello, world!\"\nresults = collection.query(query_texts=[query], n_results=3)\n\n\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512)\n# rerank the results with original query and documents returned from Chroma\nscores = model.predict([(query, doc) for doc in results[\"documents\"][0]])\n# get the highest scoring document\nprint(results[\"documents\"][0][np.argmax(scores)])\n</code></pre>"},{"location":"embeddings/embedding-models/","title":"Embedding Models","text":"<p>Work in Progress</p> <p>This page is a work in progress.</p> <p>Embedding Models are your best friends in the world of Chroma, and vector databases in general. They take something you understand in the form of text, images, audio etc. and turn it into a list of numbers (embeddings), which a machine learning model can understand. This process makes documents interpretable by a machine learning model.</p> <p>The goal of this page is to arm you with enough knowledge to make an informed decision about which embedding model to choose for your use case.</p> <p>The importance of a model</p> <p>GenAI moves pretty fast therefore we recommend not to over-rely on models too much. When creating your solution create the necessary abstractions and tests to be able to quickly experiment and change things up (don't overdo it on the abstraction though).</p> <p></p>"},{"location":"embeddings/embedding-models/#characteristics-of-an-embedding-model","title":"Characteristics of an Embedding Model","text":"<ul> <li>Modality - the type of data each model is designed to work with. For example, text, images, audio, video. Note: Some   models can work with multiple modalities (e.g. OpenAI's CLIP).</li> <li>Context - The maximum number of tokens the model can process at once.</li> <li>Tokenization - The model's tokenizer or the way a model turns text into tokens to process.</li> <li>Dimensionality - The number of dimensions in the output embeddings/vectors.</li> <li>Training Data - The data the model was trained on.</li> <li>Execution Environment - How the model is run (e.g. local, cloud, API).</li> <li>Loss Function - The function used to train the model e.g. how well the model is doing in predicting the embeddings,   compared to the actual embeddings.</li> </ul>"},{"location":"embeddings/embedding-models/#model-categories","title":"Model Categories","text":"<p>There are several ways to categorize embedding models other than the above characteristics:</p> <ul> <li>Execution environment e.g. API vs local</li> <li>Licensing e.g. open-source vs proprietary</li> <li>Privacy e.g. on-premises vs cloud</li> </ul>"},{"location":"embeddings/embedding-models/#execution-environment","title":"Execution Environment","text":"<p>The execution environment is probably the first choice you should consider when creating your GenAI solution. Can I afford my data to leave the confines of my computer, cluster, organization? If the answer is yes and you are still in the experimentation phase of your GenAI journey we recommend using API-based embedding models.</p>"},{"location":"embeddings/gpu-support/","title":"Embedding Functions GPU Support","text":"<p>By default, Chroma does not require GPU support for embedding functions. However, if you want to use GPU support, some of the functions, especially those running locally provide GPU support.</p>"},{"location":"embeddings/gpu-support/#default-embedding-functions-onnxruntime","title":"Default Embedding Functions (Onnxruntime)","text":"<p>To use the default embedding functions with GPU support, you need to install <code>onnxruntime-gpu</code> package. You can install it with the following command:</p> <pre><code>pip install onnxruntime-gpu\n</code></pre> <p>Note: To ensure no conflicts, you can uninstall <code>onnxruntime</code> (e.g. <code>pip uninstall onnxruntime</code>) in a separate environment.</p> <p>List available providers:</p> <pre><code>import onnxruntime\n\nprint(onnxruntime.get_available_providers())\n</code></pre> <p>Select the desired provider and set it as preferred before using the embedding functions (in the below example, we use <code>CUDAExecutionProvider</code>):</p> <pre><code>import time\nfrom chromadb.utils.embedding_functions import ONNXMiniLM_L6_V2\n\nef = ONNXMiniLM_L6_V2(preferred_providers=['CUDAExecutionProvider'])\n\ndocs = []\nfor i in range(1000):\n    docs.append(f\"this is a document with id {i}\")\n\nstart_time = time.perf_counter()\nembeddings = ef(docs)\nend_time = time.perf_counter()\nprint(f\"Elapsed time: {end_time - start_time} seconds\")\n</code></pre> <p>IMPORTANT OBSERVATION: Our observations are that for GPU support using sentence transformers with model <code>all-MiniLM-L6-v2</code> outperforms onnxruntime with GPU support. In practical terms on a Colab T4 GPU, the onnxruntime example above runs for about 100s whereas the equivalent sentence transformers example runs for about 1.8s.</p>"},{"location":"embeddings/gpu-support/#sentence-transformers","title":"Sentence Transformers","text":"<pre><code>import time\nfrom chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n# This will download the model to your machine and set it up for GPU support\nef = SentenceTransformerEmbeddingFunction(model_name=\"thenlper/gte-small\", device=\"cuda\")\n\n# Test with 10k documents\ndocs = []\nfor i in range(10000):\n    docs.append(f\"this is a document with id {i}\")\n\nstart_time = time.perf_counter()\nembeddings = ef(docs)\nend_time = time.perf_counter()\nprint(f\"Elapsed time: {end_time - start_time} seconds\")\n</code></pre> <p>Note: You can run the above example in google Colab - see the notebook</p>"},{"location":"embeddings/gpu-support/#openclip","title":"OpenCLIP","text":"<p>Prior to PR #1806, we simply used the <code>torch</code> package to load the model and run it on the GPU.</p> <pre><code>import chromadb\nfrom chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\nfrom chromadb.utils.data_loaders import ImageLoader\nimport toch\nimport os\n\nIMAGE_FOLDER = \"images\"\ntoch.device(\"cuda\")\n\nembedding_function = OpenCLIPEmbeddingFunction()\nimage_loader = ImageLoader()\n\nclient = chromadb.PersistentClient(path=\"my_local_data\")\ncollection = client.create_collection(\n    name='multimodal_collection',\n    embedding_function=embedding_function,\n    data_loader=image_loader)\n\nimage_uris = sorted([os.path.join(IMAGE_FOLDER, image_name) for image_name in os.listdir(IMAGE_FOLDER)])\nids = [str(i) for i in range(len(image_uris))]\ncollection.add(ids=ids, uris=image_uris)\n</code></pre> <p>After PR #1806:</p> <pre><code>from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\nembedding_function = OpenCLIPEmbeddingFunction(device=\"cuda\")\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions and Commonly Encountered Issues","text":"<p>This section provides answers to frequently asked questions and information on commonly encountered problem when working with Chroma. These information below is based on interactions with the Chroma community.</p> <p>404 Answer Not Found</p> <p>If you have a question that is not answered here, please reach out to us on our Discord @taz or GitHub Issues</p>"},{"location":"faq/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"faq/#distances-and-similarity","title":"Distances and Similarity","text":"<p>Chroma uses distance metrics to measure how dissimilar a result is from a query. A distance of 0 indicates that the two items are identical, while larger distances indicate greater dissimilarity. This approach starts at 0 and increases upward, aligning with the intuitive notion of distance.</p> <p>In contrast, similarity metrics measure how similar two items are, often on a scale where higher values represent greater similarity. For example:</p> <ul> <li>Cosine Similarity ranges from -1 to 1, where:<ul> <li>1 indicates identical orientation (maximum similarity),</li> <li>0 indicates orthogonality (no similarity),</li> <li>-1 indicates opposite orientation (maximum dissimilarity).</li> </ul> </li> <li>Dot Product can range from negative to positive infinity, depending on the vectors' magnitudes and directions. When vectors are normalized and non-negative, the dot product ranges from 0 to 1.</li> </ul>"},{"location":"faq/#why-does-chroma-use-distance-metrics","title":"Why Does Chroma Use Distance Metrics?","text":"<p>Chroma uses distance metrics because they provide a straightforward way to quantify dissimilarity between vectors. Consistency is another key aspect - irrespecitve of the distance metric used, distance results follow the same conceptual framework. Finally, distance is is an intuitive metric which makes Chroma more accessible to a wider audience.</p>"},{"location":"faq/#what-does-chroma-use-to-index-embedding-vectors","title":"What does Chroma use to index embedding vectors?","text":"<p>Chroma uses its own fork of HNSW lib for indexing and searching embeddings. In addition to HNSW, Chroma also uses a Brute Force index, which acts as a buffer (prior to updating the HNSW graph) and performs exhaustive search using the same distance metric as the HNSW index.</p> <p>Alternative Questions:</p> <ul> <li>What library does Chroma use for vector index and search?</li> <li>What algorithm does Chroma use for vector search?</li> </ul>"},{"location":"faq/#how-to-set-dimensionality-of-my-collections","title":"How to set dimensionality of my collections?","text":"<p>When creating a collection, its dimensionality is determined by the dimensionality of the first embedding added to it. Once the dimensionality is set, it cannot be changed. Therefore, it is important to consistently use embeddings of the same dimensionality when adding or querying a collection.</p> <p>Example:</p> <pre><code>import chromadb\n\nclient = chromadb.Client()\n\ncollection = client.create_collection(\"name\")  # dimensionality is not set yet\n\n# add an embedding to the collection\ncollection.add(ids=[\"id1\"], embeddings=[[1, 2, 3]])  # dimensionality is set to 3\n</code></pre> <p>Alternative Questions:</p> <ul> <li>Can I change the dimensionality of a collection?</li> </ul>"},{"location":"faq/#can-i-use-transformers-models-with-chroma","title":"Can I use <code>transformers</code> models with Chroma?","text":"<p>Generally, yes you can use <code>transformers</code> models with Chroma. Although Chroma does not provide a wrapper for this, you can use <code>SentenceTransformerEmbeddingFunction</code> to achieve the same result. The sentence-transformer library will implicitly do mean-pooling on the last hidden layer, and you'll get a warning about it - <code>No sentence-transformers model found with name [model name]. Creating a new one with MEAN pooling.</code></p> <p>Example:</p> <pre><code>from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n\nef = SentenceTransformerEmbeddingFunction(model_name=\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\")\n\nprint(ef([\"test\"]))\n</code></pre> <p>Warning</p> <p>Not all models will work with the above method. Also mean pooling may not be the best strategy for the model.  Read the model card and try to understand what if any pooling the creators recommend. You may also want to normalize the embeddings before adding them to Chroma (pass <code>normalize_embeddings=True</code> to the <code>SentenceTransformerEmbeddingFunction</code>  EF constructor).</p>"},{"location":"faq/#should-i-store-my-documents-in-chroma","title":"Should I store my documents in Chroma?","text":"<p>Note: This applies to Chroma single-node and local embedded clients. (Chroma version ca. 0.5.x)</p> <p>Chroma allows users to store both embeddings and documents, alongside metadata, in collections. Documents and metadata are both optional and depending on your use case you may choose to store them in Chroma or externally, or not at all.</p> <p>Here are some pros/cons to help you decide whether to store your documents in Chroma:</p> <p>Pros:</p> <ul> <li>Keeps all the data in the same place. You don't have to manage a separate DB for the documents</li> <li>Allows you to do keyword searches on the documents</li> </ul> <p>Cons:</p> <ul> <li>The database can grow substantially in size because documents are effectively duplicated - once for storing them as   metadata for queries and another for the FTS5 index.</li> <li>Queries performance hit</li> </ul>"},{"location":"faq/#dude-wheres-my-data","title":"\"Dude, where's my data?\"","text":"<p>If you are new to Chroma, you might be asking yourself: \"Where is my data been stored?\". As, per usual, the answer is: \"It depends\".</p> <p>Generally Chroma uses <code>PERSIST_DIRECTORY</code> to store the data, but when running in CLI mode, this is overridden by the CLI itself.</p> <ul> <li>Running in CLI mode (<code>--path</code> is not specified) data is stored in the <code>./chroma_data</code> directory.</li> <li>Running in Jupyter notebook, Colab or directly using <code>PersistentClient</code> (unless <code>path</code> is specified or env var <code>PERSIST_DIRECTORY</code> is set), data is stored in the <code>./chroma</code> directory.</li> <li>Running with docker compose (from source repo), the data is stored in docker volume named <code>chroma-data</code> (unless an explicit volume binding is specified)</li> <li>Running with <code>docker run</code> (no volume binding with <code>-v</code>) the data is stored in the container and is lost \u2620\ufe0f when the container is removed.</li> </ul> <p>In all other cases where env var, parameter or binding is specified, the data is stored in your specified directory.</p>"},{"location":"faq/#commonly-encountered-problems","title":"Commonly Encountered Problems","text":""},{"location":"faq/#collection-dimensionality-mismatch","title":"Collection Dimensionality Mismatch","text":"<p>Symptoms:</p> <p>This error usually exhibits in the following error message:</p> <p><code>chromadb.errors.InvalidDimensionException: Embedding dimension XXX does not match collection dimensionality YYY</code></p> <p>Context:</p> <p>When adding/upserting or querying Chroma collection. This error is more visible/pronounced when using the Python APIs, but will also show up in also surface in other clients.</p> <p>Cause:</p> <p>You are trying to add or query a collection with vectors of a different dimensionality than the collection was created with.</p> <p>Explanation/Solution:</p> <p>When you first create a collection <code>client.create_collection(\"name\")</code>, the collection will not have knowledge of its dimensionality so that allows you to add vectors of any dimensionality to it. However, once your first batch of embeddings is added to the collection, the collection will be locked to that dimensionality. Any subsequent query or add operation must use embeddings of the same dimensionality. The dimensionality of the embeddings is a characteristic of the embedding model (EmbeddingFunction) used to generate the embeddings, therefore it is important to consistently use the same EmbeddingFunction when adding or querying a collection.</p> <p>Tip</p> <p>If you do not specify an <code>embedding_function</code> when creating (<code>client.create_collection</code>) or getting (<code>client.get_or_create_collection</code>) a collection, Chroma wil use its default embedding function.</p>"},{"location":"faq/#large-distances-in-search-results","title":"Large Distances in Search Results","text":"<p>Symptoms:</p> <p>When querying a collection, you get results that are in the 10s or 100s.</p> <p>Context:</p> <p>Frequently when using you own embedding function.</p> <p>Cause:</p> <p>The embeddings are not normalized.</p> <p>Explanation/Solution:</p> <p><code>L2</code> (Euclidean distance) and <code>IP</code> (inner product) distance metrics are sensitive to the magnitude of the vectors. Chroma uses <code>L2</code> by default. Therefore, it is recommended to normalize the embeddings before adding them to Chroma.</p> <p>Here is an example how to normalize embeddings using L2 norm:</p> <pre><code>import numpy as np\n\n\ndef normalize_L2(vector):\n    \"\"\"Normalizes a vector to unit length using L2 norm.\"\"\"\n    norm = np.linalg.norm(vector)\n    if norm == 0:\n        return vector\n    return vector / norm\n</code></pre>"},{"location":"faq/#operationalerror-no-such-column-collectionstopic","title":"<code>OperationalError: no such column: collections.topic</code>","text":"<p>Symptoms:</p> <p>The error <code>OperationalError: no such column: collections.topic</code> is raised when trying to access Chroma locally or remotely.</p> <p>Context:</p> <p>After upgrading to Chroma <code>0.5.0</code> or accessing your Chroma persistent data with Chroma client version <code>0.5.0</code>.</p> <p>Cause:</p> <p>In version <code>0.5.x</code> Chroma has made some SQLite3 schema changes that are not backwards compatible with the previous versions. Once you access your persistent data on the server or locally with the new Chroma version it will automatically migrate to the new schema. This operation is not reversible.</p> <p>Explanation/Solution:</p> <p>To resolve this issue you will need to upgrade all your clients accessing the Chroma data to version <code>0.5.x</code>.</p> <p>Here's a link to the migration performed by Chroma - https://github.com/chroma-core/chroma/blob/main/chromadb/migrations/sysdb/00005-remove-topic.sqlite.sql</p>"},{"location":"faq/#sqlite3operationalerror-database-or-disk-is-full","title":"<code>sqlite3.OperationalError: database or disk is full</code>","text":"<p>Symptoms:</p> <p>The error <code>sqlite3.OperationalError: database or disk is full</code> is raised when trying to access Chroma locally or remotely. The error can occur in any of the Chroma API calls.</p> <p>Context:</p> <p>There are two contexts in which this error can occur:</p> <ul> <li>When the persistent disk space is full or the disk quota is reached - This is where your <code>PERSIST_DIRECTORY</code> points   to.</li> <li>When there is not enough space in the temporary director - frequently <code>/tmp</code> on your system or container.</li> </ul> <p>Cause:</p> <p>When inserting new data and your Chroma persistent disk space is full or the disk quota is reached, the database will not be able to write metadata to SQLite3 db thus raising the error.</p> <p>When performing large queries or multiple concurrent queries, the temporary disk space may be exhausted.</p> <p>Explanation/Solution:</p> <p>To work around the first issue, you can increase the disk space or clean up the disk space. To work around the second issue, you can increase the temporary disk space (works fine for containers but might be a problem for VMs) or point SQLite3 to a different temporary directory by using <code>SQLITE_TMPDIR</code> environment variable.</p> SQLite Temp File <p>More information on how sqlite3 uses temp files can be found here.</p>"},{"location":"faq/#runtimeerror-chroma-is-running-in-http-only-client-mode-and-can-only-be-run-with-chromadbapifastapifastapi","title":"<code>RuntimeError: Chroma is running in http-only client mode, and can only be run with 'chromadb.api.fastapi.FastAPI'</code>","text":"<p>Symptoms and Context:</p> <p>The following error is raised when trying to create a new <code>PersistentClient</code>, <code>EphemeralClient</code>, or <code>Client</code>:</p> <pre><code>RuntimeError: Chroma is running in http-only client mode, and can only be run with 'chromadb.api.fastapi.FastAPI' \nas the chroma_api_impl. see https://docs.trychroma.com/usage-guide?lang=py#using-the-python-http-only-client for more information.\n</code></pre> <p>Cause:</p> <p>There are two possible causes for this error:</p> <ul> <li><code>chromadb-client</code> is installed and you are trying to work with a local client.</li> <li>Dependency conflict with <code>chromadb-client</code> and <code>chromadb</code> packages.</li> </ul> <p>Explanation/Solution:</p> <p>Chroma (python) comes in two packages - <code>chromadb</code> and <code>chromadb-client</code>. The <code>chromadb-client</code> package is used to interact with a remote Chroma server. If you are trying to work with a local client, you should use the <code>chromadb</code> package. If you are planning to interact with remote server only it is recommended to use the <code>chromadb-client</code> package.</p> <p>If you intend to work locally with Chroma (e.g. embed in your app) then we suggest that you uninstall the <code>chromadb-client</code> package and install the <code>chromadb</code> package.</p> <p>To check which package you have installed:</p> <pre><code>pip list | grep chromadb\n</code></pre> <p>To uninstall the <code>chromadb-client</code> package:</p> <pre><code>pip uninstall chromadb-client\n</code></pre> Working with virtual environments <p>It is recommended to work with virtual environments to avoid dependency conflicts. To create a virtual environment  you can use the following snippet:</p> <p><pre><code>pip install virtualenv\npython -m venv myenv\nsource myenv/bin/activate\npip install chromadb # and other packages you need\n</code></pre> Alternatively you can use <code>conda</code> or <code>poetry</code> to manage your environments.</p> Default Embedding Function <p>Default embedding function - <code>chromadb.utils.embedding_functions.DefaultEmbeddingFunction</code> - can only be used with <code>chromadb</code> package.</p>"},{"location":"faq/#valueerror-you-must-provide-an-embedding-function-to-compute-embeddings","title":"<code>ValueError: You must provide an embedding function to compute embeddings</code>","text":"<p>Symptoms and Context:</p> <p>The error <code>ValueError: You must provide an embedding function to compute embeddings.https://docs.trychroma.com/embeddings\"</code> is frequently raised when trying to add embeddings to a collection using Python thin client (<code>chromadb-client</code> package).</p> <p>Cause:</p> <p>To reduce the size of the <code>chromadb-client</code> package the default embedding function which requires <code>onnxruntime</code> package is not included and is instead aliased to <code>None</code>.</p> <p>Explanation/Solution:</p> <p>To resolve this issue you must always provide an embedding function when you call <code>get_collection</code> or <code>get_or_create_collection</code> methods to provide the Http client with the necessary information to compute embeddings.</p>"},{"location":"faq/#adding-documents-is-slow","title":"Adding documents is slow","text":"<p>Symptoms:</p> <p>Adding documents to Chroma appears slow.</p> <p>Context:</p> <p>You've tried adding documents to a collection using the <code>add()</code> or <code>upsert()</code> methods.</p> <p>Cause:</p> <p>There are several reasons why the addition may be slow:</p> <ul> <li>Very large batches</li> <li>Slow embeddings</li> <li>Slow network</li> </ul> <p>Let's break down each of the factors.</p> <p>Very large batches</p> <p>If you are trying to add 1000s or even 10,000s of documents at once and depending on how much data is already in your collection Chroma (specifically the HNSW graph updates) can become a bottleneck.</p> <p>To debug if this is the case you can reduce the size of the batch and see if the operation is faster. You can also check how many records are in the collection with <code>count()</code> method.</p> <p>Slow embeddings</p> <p>This is the most common reason for slow addition. Some embedding functions are slower than others. To debug this you can try the following example by adjusting the embeding function to your own. What the code tests is how much it takes to compute the embedings and then to add them to the collection in separate steps such that each can be measured independenty.</p> <pre><code>from chromadb.utils import embedding_functions\nimport chromadb\nimport uuid\n\nlist_of_sentences = [\"Hello world!\", \"How are you?\"] # this should be your list of documents to add\n\n# change the below EF definition to match your embedding function\ndefault_ef = embedding_functions.DefaultEmbeddingFunction() \n\nstart_time = time.perf_counter()\nembeddings=default_ef(list_of_sentences)\nend_time = time.perf_counter()\nprint(f\"Embedding time: {end_time - start_time}\")\n\nclient = chromadb.PersistentClient(path=\"my_chroma_data\")\ncollection = client.get_or_create_collection(\"my_collection\")\n\nstart_time = time.perf_counter()\n# this will add your documents and the generated embeddings without Chroma doing the embedding for you internally\ncollection.add(ids=[f\"{uuid.uuid4()}\" for _ in range(len(list_of_sentences))],documents=list_of_sentences, embeddings=embeddings)\nend_time = time.perf_counter()\nprint(f\"Chroma add time: {end_time - start_time}\")\n</code></pre> <p>Slow network</p> <p>If you are adding documents to a remote Chroma the network speed can become a bottleneck. To debug this you can with a local <code>PersistentClient</code> and see if the operation is faster.</p>"},{"location":"integrations/langchain/","title":"Chroma Integrations With LangChain","text":"<ul> <li>Embeddings - learn how to use Chroma Embedding functions with LC and vice versa</li> <li>Retrievers - learn how to use LangChain retrievers with Chroma</li> </ul>"},{"location":"integrations/langchain/embeddings/","title":"Langchain Embeddings","text":""},{"location":"integrations/langchain/embeddings/#embedding-functions","title":"Embedding Functions","text":"<p>Chroma and Langchain both offer embedding functions which are wrappers on top of popular embedding models.</p> <p>Unfortunately Chroma and LC's embedding functions are not compatible with each other. Below we offer two adapters to convert Chroma's embedding functions to LC's and vice versa.</p> <p>Links:</p> <ul> <li>Chroma Embedding Functions Definition</li> <li>Langchain Embedding Functions Definition</li> </ul>"},{"location":"integrations/langchain/embeddings/#chroma-built-in-langchain-adapter","title":"Chroma Built-in Langchain Adapter","text":"<p>As of version <code>0.5.x</code> Chroma offers a built-in two-way adapter to convert Langchain's embedding function to an adapted embeddings that can be used by both LC and Chroma. Implementation can be found here.</p> HuggingFaceOpenAI <p>Find out more about Langchain's HuggingFace embeddings here.</p> <pre><code># pip install chromadb langchain langchain-huggingface langchain-chroma\nimport chromadb\nfrom chromadb.utils.embedding_functions import create_langchain_embedding\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nlangchain_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\nef = create_langchain_embedding(langchain_embeddings)\nclient = chromadb.PersistentClient(path=\"./chroma-data\")\ncollection = client.get_or_create_collection(name=\"my_collection\", embedding_function=ef)\n\ncollection.add(ids=[\"1\"],documents=[\"test document goes here\"])\n</code></pre> <p>Find out more about Langchain's OpenAI embeddings here.</p> <pre><code># pip install chromadb langchain langchain-openai langchain-chroma\nimport chromadb\nfrom chromadb.utils.embedding_functions import create_langchain_embedding\nfrom langchain_openai import OpenAIEmbeddings\n\nlangchain_embeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-large\",\n    api_key=os.environ[\"OPENAI_API_KEY\"],\n)\nef = create_langchain_embedding(langchain_embeddings)\nclient = chromadb.PersistentClient(path=\"/chroma-data\")\ncollection = client.get_or_create_collection(name=\"my_collection\", embedding_function=ef)\n\ncollection.add(ids=[\"1\"],documents=[\"test document goes here\"])\n</code></pre>"},{"location":"integrations/langchain/embeddings/#custom-adapter","title":"Custom Adapter","text":"<p>Here is the adapter to convert Chroma's embedding functions to LC's:</p> <pre><code>from langchain_core.embeddings import Embeddings\nfrom chromadb.api.types import EmbeddingFunction\n\n\nclass ChromaEmbeddingsAdapter(Embeddings):\n    def __init__(self, ef: EmbeddingFunction):\n        self.ef = ef\n\n    def embed_documents(self, texts):\n        return self.ef(texts)\n\n    def embed_query(self, query):\n        return self.ef([query])[0]\n</code></pre> <p>Here is the adapter to convert LC's embedding function s to Chroma's:</p> <pre><code>from langchain_core.embeddings import Embeddings\nfrom chromadb.api.types import EmbeddingFunction, Documents\n\n\nclass LangChainEmbeddingAdapter(EmbeddingFunction[Documents]):\n    def __init__(self, ef: Embeddings):\n        self.ef = ef\n\n    def __call__(self, input: Documents) -&gt; Embeddings:\n        # LC EFs also have embed_query but Chroma doesn't support that so we just use embed_documents\n        # TODO: better type checking\n        return self.ef.embed_documents(input)\n</code></pre>"},{"location":"integrations/langchain/embeddings/#example-usage","title":"Example Usage","text":"<p>Using Chroma Embedding Functions with Langchain:</p> <pre><code># pip install chromadb langchain langchain-huggingface langchain-chroma\nfrom langchain.vectorstores.chroma import Chroma\nfrom chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n\ntexts = [\"foo\", \"bar\", \"baz\"]\n\ndocs_vectorstore = Chroma.from_texts(\n    texts=texts,\n    collection_name=\"docs_store\",\n    embedding=ChromaEmbeddingsAdapter(SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")),\n)\n</code></pre> <p>Using Langchain Embedding Functions with Chroma:</p> <pre><code># pip install chromadb langchain langchain-huggingface langchain-chroma\nfrom langchain_huggingface import HuggingFaceEmbeddings\nimport chromadb\n\nclient = chromadb.Client()\n\ncollection = client.get_or_create_collection(\"test\", embedding_function=LangChainEmbeddingAdapter(\n    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")))\ncollection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"foo\", \"bar\", \"baz\"])\n</code></pre>"},{"location":"integrations/langchain/retrievers/","title":"\ud83e\udd9c\u26d3\ufe0f Langchain Retriever","text":"<p>TBD: describe what retrievers are in LC and how they work.</p>"},{"location":"integrations/langchain/retrievers/#vector-store-retriever","title":"Vector Store Retriever","text":"<p>In the below example we demonstrate how to use Chroma as a vector store retriever with a filter query.</p> <p>Note that the filter is supplied whenever we create the retriever object so the filter applies to all queries (<code>get_relevant_documents</code>).</p> <pre><code>from langchain.document_loaders import OnlinePDFLoader\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.vectorstores import Chroma\nfrom typing import Dict, Any\nimport chromadb\nfrom langchain_core.embeddings import Embeddings\n\nclient = chromadb.PersistentClient(path=\"./chroma\")\n\ncol = client.get_or_create_collection(\"test\")\n\ncol.upsert([f\"{i}\" for i in range(10)],documents=[f\"This is document #{i}\" for i in range(10)],metadatas=[{\"id\":f\"{i}\"} for i in range(10)])\n\nef = chromadb.utils.embedding_functions.DefaultEmbeddingFunction()\n\nclass DefChromaEF(Embeddings):\n  def __init__(self,ef):\n    self.ef = ef\n\n  def embed_documents(self,texts):\n    return self.ef(texts)\n\n  def embed_query(self, query):\n    return self.ef([query])[0]\n\n\ndb = Chroma(client=client, collection_name=\"test\",embedding_function=DefChromaEF(ef))\n\nretriever = db.as_retriever(search_kwargs={\"filter\":{\"id\":\"1\"}})\n\ndocs = retriever.get_relevant_documents(\"document\")\n\nassert len(docs)==1\n</code></pre> <p>Ref: https://colab.research.google.com/drive/1L0RwQVVBtvTTd6Le523P4uzz3m3fm0pH#scrollTo=xROOfxLohE5j</p>"},{"location":"integrations/llamaindex/","title":"Chroma Integrations With LlamaIndex","text":"<ul> <li>Embeddings - learn how to use LlamaIndex embeddings functions with Chroma and vice versa</li> </ul>"},{"location":"integrations/llamaindex/embeddings/","title":"LlamaIndex Embeddings","text":""},{"location":"integrations/llamaindex/embeddings/#embedding-functions","title":"Embedding Functions","text":"<p>Chroma and LlamaIndex both offer embedding functions which are wrappers on top of popular embedding models.</p> <p>Unfortunately Chroma and LI's embedding functions are not compatible with each other. Below we offer an adapters to convert LI embedding function to Chroma one.</p> <pre><code>from llama_index.core.schema import TextNode\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom chromadb import EmbeddingFunction, Documents, Embeddings\n\n\nclass LlamaIndexEmbeddingAdapter(EmbeddingFunction):\n    def __init__(self, ef: BaseEmbedding):\n        self.ef = ef\n\n    def __call__(self, input: Documents) -&gt; Embeddings:\n        return [node.embedding for node in self.ef([TextNode(text=doc) for doc in input])]\n</code></pre> <p>Text modality</p> <p>The above adapter assumes that the input documents are text. If you are using a different modality,  you will need to modify the adapter accordingly.</p> <p>An example of how to use the above with LlamaIndex:</p> <p>Prerequisites for example</p> <p>Run <code>pip install llama-index chromadb llama-index-embeddings-fastembed fastembed</code></p> <pre><code>import chromadb\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\n\n# make sure to include the above adapter and imports\nembed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\nclient = chromadb.Client()\n\ncol = client.get_or_create_collection(\"test_collection\", embedding_function=LlamaIndexEmbeddingAdapter(embed_model))\n\ncol.add(ids=[\"1\"], documents=[\"this is a test document\"])\n</code></pre>"},{"location":"integrations/ollama/","title":"Chroma Integrations With Ollama","text":"<ul> <li>Embeddings - learn how to use Ollama as embedder for Chroma documents</li> <li>\u2728<code>Coming soon</code> RAG with Ollama - a primer on how to build a simple RAG app with Ollama and Chroma</li> </ul>"},{"location":"integrations/ollama/embeddings/","title":"Ollama","text":"<p>Ollama offers out-of-the-box embedding API which allows you to generate embeddings for your documents. Chroma provides a convenient wrapper around Ollama's embedding API.</p>"},{"location":"integrations/ollama/embeddings/#ollama-embedding-models","title":"Ollama Embedding Models","text":"<p>While you can use any of the ollama models including LLMs to generate embeddings. We generally recommend using specialized models like <code>nomic-embed-text</code> for text embeddings. The latter models are specifically trained for embeddings and are more efficient for this purpose (e.g. the dimensions of the output embeddings are much smaller than those from LLMs e.g. 1024 - nomic-embed-text vs 4096 - llama3)</p> <p>Models:</p> Model Pull Ollama Registry Link <code>nomic-embed-text</code> <code>ollama pull nomic-embed-text</code> nomic-embed-text <code>mxbai-embed-large</code> <code>ollama pull mxbai-embed-large</code> mxbai-embed-large <code>snowflake-arctic-embed</code> <code>ollama pull snowflake-arctic-embed</code> snowflake-arctic-embed <code>all-minilm-l6-v2</code> <code>ollama pull chroma/all-minilm-l6-v2-f32</code> all-minilm-l6-v2-f32"},{"location":"integrations/ollama/embeddings/#basic-usage","title":"Basic Usage","text":"<p>First let's run a local docker container with Ollama. We'll pull <code>nomic-embed-text</code> model:</p> <pre><code>docker run -d --rm -v ./ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\ndocker exec -it ollama ollama run nomic-embed-text # press Ctrl+D to exit after model downloads successfully\n# test it\ncurl http://localhost:11434/api/embeddings -d '{\"model\": \"nomic-embed-text\",\"prompt\": \"Here is an article about llamas...\"}'\n</code></pre> <p>Ollama Docs</p> <p>For more information on Ollama, visit the Ollama GitHub repository.</p> <p>Using the CLI</p> <p>If you have or prefer to use the Ollama CLI, you can use the following command to get a model:</p> <pre><code>ollama pull nomic-embed-text\n</code></pre> <p>Now let's configure our OllamaEmbeddingFunction Embedding (python) function with the default Ollama endpoint:</p>"},{"location":"integrations/ollama/embeddings/#python","title":"Python","text":"<pre><code>import chromadb\nfrom chromadb.utils.embedding_functions import OllamaEmbeddingFunction\n\nclient = chromadb.PersistentClient(path=\"ollama\")\n\n# create EF with custom endpoint\nef = OllamaEmbeddingFunction(\n    model_name=\"nomic-embed-text\",\n    url=\"http://localhost:11434/api/embeddings\",\n)\n\nprint(ef([\"Here is an article about llamas...\"]))\n</code></pre>"},{"location":"integrations/ollama/embeddings/#javascript","title":"JavaScript","text":"<p>For JS users, you can use the <code>OllamaEmbeddingFunction</code> class to create embeddings:</p> <pre><code>const {OllamaEmbeddingFunction} = require('chromadb');\nconst embedder = new OllamaEmbeddingFunction({\n    url: \"http://localhost:11434/api/embeddings\",\n    model: \"nomic-embed-text\"\n})\n\n// use directly\nconst embeddings = embedder.generate([\"Here is an article about llamas...\"])\n</code></pre>"},{"location":"integrations/ollama/embeddings/#golang","title":"Golang","text":"<p>For Golang you can use the <code>chroma-go</code> client's <code>OllamaEmbeddingFunction</code> embedding function to generate embeddings for your documents:</p> <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    ollama \"github.com/amikos-tech/chroma-go/ollama\"\n)\n\nfunc main() {\n    documents := []string{\n        \"Document 1 content here\",\n        \"Document 2 content here\",\n    }\n    // the `/api/embeddings` endpoint is automatically appended to the base URL\n    ef, err := ollama.NewOllamaEmbeddingFunction(ollama.WithBaseURL(\"http://127.0.0.1:11434\"), ollama.WithModel(\"nomic-embed-text\"))\n    if err != nil {\n        fmt.Printf(\"Error creating Ollama embedding function: %s \\n\", err)\n    }\n    resp, err := ef.EmbedDocuments(context.Background(), documents)\n    if err != nil {\n        fmt.Printf(\"Error embedding documents: %s \\n\", err)\n    }\n    fmt.Printf(\"Embedding response: %v \\n\", resp)\n}\n</code></pre> <p>Golang Client</p> <p>You can install the Golang client by running the following command:</p> <pre><code>go get github.com/amikos-tech/chroma-go\n</code></pre> <p>For more information visit https://go-client.chromadb.dev/</p>"},{"location":"running/deployment-patterns/","title":"Deployment Patterns","text":"<p>If you are building with Chroma, you usually start with one of two setups:</p> <ol> <li>Embedded mode - Chroma runs inside your Python process (<code>PersistentClient</code>)</li> <li>Server mode - Chroma runs as a standalone service and your app connects via HTTP (<code>HttpClient</code>)</li> </ol> <p>You do not need to pick forever. Many teams start embedded, then move to server mode when they need shared access across apps or machines.</p> <p>Runnable Examples</p> <p>Complete, standalone files for this page are in examples/deployment-patterns:</p> <ul> <li>Embedded Python app (<code>app_embedded.py</code>)</li> <li>Server Python app (<code>app_http.py</code>)</li> <li>Docker Compose (<code>docker-compose.yml</code>)</li> <li>Python dependencies (<code>requirements.txt</code>)</li> </ul>"},{"location":"running/deployment-patterns/#embedded-in-your-application","title":"Embedded in your application","text":"<p>This is the easiest way to get moving: your app and Chroma run in the same Python process, and data is stored in a local folder.</p> <p>Pick this when you want:</p> <ul> <li>the shortest path from idea to working prototype</li> <li>simple local development (no separate DB service to run)</li> <li>low-latency reads/writes in one app process</li> </ul> <p>One important gotcha</p> <p>Chroma is thread-safe but not process-safe. Avoid multiple processes writing to the same local <code>path</code>.</p>"},{"location":"running/deployment-patterns/#example-1-embedded-persistentclient-in-a-python-service","title":"Example 1: Embedded <code>PersistentClient</code> in a Python service","text":"<ol> <li>Install Chroma:</li> </ol> <pre><code>pip install chromadb\n</code></pre> <p>If you are running the repo examples directly, use pinned dependencies:</p> <pre><code>pip install -r examples/deployment-patterns/requirements.txt\n</code></pre> <ol> <li>Create <code>app_embedded.py</code>.</li> </ol> <p>A complete runnable version is available at <code>examples/deployment-patterns/embedded/python/app_embedded.py</code>.</p> <pre><code>import chromadb\n\n\nclass EmbeddedKnowledgeBase:\n    def __init__(self, path: str = \"./chroma_data\") -&gt; None:\n        self.client = chromadb.PersistentClient(path=path)\n        self.collection = self.client.get_or_create_collection(\n            name=\"support_kb\",\n            embedding_function=None,  # using explicit embeddings below\n        )\n\n    def add_article(self, article_id: str, text: str, embedding: list[float], product: str) -&gt; None:\n        self.collection.upsert(\n            ids=[article_id],\n            documents=[text],\n            embeddings=[embedding],\n            metadatas=[{\"product\": product}],\n        )\n\n    def search(self, query_embedding: list[float], product: str, n_results: int = 2) -&gt; dict:\n        return self.collection.query(\n            query_embeddings=[query_embedding],\n            where={\"product\": product},\n            n_results=n_results,\n            include=[\"documents\", \"metadatas\", \"distances\"],\n        )\n\n\nif __name__ == \"__main__\":\n    kb = EmbeddedKnowledgeBase(path=\"./chroma_data\")\n\n    kb.add_article(\n        article_id=\"a1\",\n        text=\"Refunds are available within 30 days for annual plans.\",\n        embedding=[0.11, 0.20, 0.31],\n        product=\"billing\",\n    )\n    kb.add_article(\n        article_id=\"a2\",\n        text=\"You can rotate API keys from the admin settings page.\",\n        embedding=[0.12, 0.18, 0.30],\n        product=\"platform\",\n    )\n\n    result = kb.search(query_embedding=[0.10, 0.19, 0.32], product=\"billing\")\n    print(\"Top match:\", result[\"documents\"][0])\n</code></pre> <ol> <li>Run it:</li> </ol> <pre><code>python app_embedded.py\n</code></pre> <p>That is it. Your vectors are now persisted under <code>./chroma_data</code> and travel with your app lifecycle.</p>"},{"location":"running/deployment-patterns/#standalone-server","title":"Standalone server","text":"<p>In this pattern, Chroma runs as its own service. Your app talks to it over HTTP using <code>HttpClient</code>.</p> <p>Pick this when you want:</p> <ul> <li>multiple app instances using one shared Chroma deployment</li> <li>a clean boundary between app code and database service</li> <li>the option to scale app and database separately</li> </ul>"},{"location":"running/deployment-patterns/#example-2-typical-server-deployment-python-httpclient","title":"Example 2: Typical server deployment + Python <code>HttpClient</code>","text":"<ol> <li>Start Chroma server with Docker Compose.</li> </ol> <p>A complete runnable version is available at <code>examples/deployment-patterns/server/docker-compose.yml</code>.</p> docker-compose.yml<pre><code>services:\n  chroma:\n    image: chromadb/chroma:1.5.1\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./chroma-data:/data\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/api/v2/heartbeat\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n</code></pre> <pre><code>docker compose up -d\n</code></pre> <ol> <li>Create <code>app_http.py</code> and connect with <code>HttpClient</code>.</li> </ol> <p>A complete runnable version is available at <code>examples/deployment-patterns/server/python/app_http.py</code>.</p> <pre><code>import os\nimport chromadb\n\n\ndef get_client() -&gt; chromadb.HttpClient:\n    return chromadb.HttpClient(\n        host=os.getenv(\"CHROMA_HOST\", \"localhost\"),\n        port=int(os.getenv(\"CHROMA_PORT\", \"8000\")),\n        ssl=os.getenv(\"CHROMA_SSL\", \"false\").lower() == \"true\",\n    )\n\n\nif __name__ == \"__main__\":\n    client = get_client()\n    collection = client.get_or_create_collection(\n        name=\"support_kb\",\n        embedding_function=None,\n    )\n\n    collection.upsert(\n        ids=[\"a3\"],\n        documents=[\"Password reset links expire after 15 minutes.\"],\n        embeddings=[[0.09, 0.22, 0.28]],\n        metadatas=[{\"product\": \"platform\"}],\n    )\n\n    result = collection.query(\n        query_embeddings=[[0.10, 0.21, 0.29]],\n        n_results=1,\n        where={\"product\": \"platform\"},\n        include=[\"documents\", \"distances\"],\n    )\n    print(\"Top match:\", result[\"documents\"][0])\n</code></pre> <ol> <li>Set host/port when needed (for example in containers):</li> </ol> <pre><code>export CHROMA_HOST=chroma\nexport CHROMA_PORT=8000\npython app_http.py\n</code></pre>"},{"location":"running/deployment-patterns/#usual-production-shape","title":"Usual production shape","text":"<ul> <li>put Chroma behind a reverse proxy or load balancer for HTTPS and controlled access</li> <li>mount durable storage for Chroma data (<code>/data</code> in container deployments)</li> <li>use <code>/api/v2/heartbeat</code> for health checks</li> <li>scale your app replicas and Chroma deployment independently</li> </ul> <p>For next steps, see:</p> <ul> <li>Running Chroma</li> <li>Health Checks</li> <li>Systemd service</li> <li>Road To Production</li> </ul>"},{"location":"running/health-checks/","title":"Health Checks","text":"<p>Health checks tell your orchestrator whether Chroma is running and ready to serve requests. They are used to keep traffic away from unready instances, trigger automated restarts for unhealthy ones, and coordinate startup order for dependent services.</p>"},{"location":"running/health-checks/#docker-compose","title":"Docker Compose","text":"<p>The simplest form of health check is to use the <code>healthcheck</code> directive in the <code>docker-compose.yml</code> file. This is useful if you are deploying Chroma alongside other services that may depend on it.</p> <pre><code>services:\n  chromadb:\n    image: chromadb/chroma:1.5.1\n    volumes:\n      - ./chroma-data:/data\n    ports:\n      - \"8000:8000\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/api/v2/heartbeat\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n</code></pre>"},{"location":"running/health-checks/#kubernetes","title":"Kubernetes","text":"<p>In kubernetes you can use the <code>livenessProbe</code> and <code>readinessProbe</code> to check the health of the server. This is useful if you are deploying Chroma in a kubernetes cluster.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: chroma\n  labels:\n    app: chroma\nspec:\n    replicas: 1\n    selector:\n        matchLabels:\n          app: chroma\n    template:\n        metadata:\n            labels:\n                app: chroma\n        spec:\n            containers:\n              - name: chroma\n                image: &lt;chroma-image&gt;\n                ports:\n                - containerPort: 8000\n                livenessProbe:\n                    httpGet:\n                        path: /api/v2/heartbeat\n                        port: 8000\n                    initialDelaySeconds: 5\n                    periodSeconds: 5\n                readinessProbe:\n                    httpGet:\n                        path: /api/v2/heartbeat\n                        port: 8000\n                    initialDelaySeconds: 5\n                    periodSeconds: 5\n                startupProbe:\n                    httpGet:\n                      path: /api/v2/heartbeat\n                      port: 8000\n                    failureThreshold: 3\n                    periodSeconds: 60\n                    initialDelaySeconds: 60\n</code></pre> <p>Alternative to the <code>httpGet</code> you can also use <code>tcpSocket</code>:</p> <pre><code>          readinessProbe:\n            tcpSocket:\n              port: 8000\n            failureThreshold: 3\n            timeoutSeconds: 30\n            periodSeconds: 60\n          livenessProbe:\n            tcpSocket:\n              port: 8000\n            failureThreshold: 3\n            timeoutSeconds: 30\n            periodSeconds: 60\n          startupProbe:\n            tcpSocket:\n              port: 8000\n            failureThreshold: 3\n            periodSeconds: 60\n            initialDelaySeconds: 60\n</code></pre>"},{"location":"running/maintenance/","title":"Maintenance","text":"<p>This section describes maintenance tooling and procedures for running your Chroma database.</p>"},{"location":"running/maintenance/#chroma-ops-tooling","title":"Chroma Ops (Tooling)","text":"<p>Chroma Ops is a maintenance CLI for Chroma. It provides a set of commands for inspecting, configuring and improving the performance of your Chroma database.</p>"},{"location":"running/maintenance/#use-cases","title":"Use Cases","text":"<p>Chroma Ops is designed to help you maintain a healthy Chroma database. It can also be used for inspecting the state of your database. The following use cases are supported:</p> <ul> <li>\ud83d\udce6 Database Maintenance</li> <li><code>db info</code> - gathers general information about your Chroma persistent database</li> <li><code>db clean</code> - cleans up the database from unused files (for now only orphanated HNSW segment directories)</li> <li>\ud83d\udcdd Write-Ahead Log (WAL) Maintenance</li> <li><code>wal info</code> - gathers information about the Write-Ahead Log (WAL)</li> <li><code>wal commit</code> - commits the WAL to all collections with outstanding changes</li> <li><code>wal export</code> - exports the WAL to a <code>jsonl</code> file. This can be used for debugging and for auditing.</li> <li><code>wal config</code> - allows you to configure the WAL for your Chroma database.</li> <li><code>wal clean</code> - cleans up the WAL from old, committed transactions.</li> <li>\ud83d\udd0d Full Text Search (FTS) Maintenance</li> <li><code>fts rebuild</code> - rebuilds the FTS index for all collections or change the tokenizer.</li> <li>\ud83e\uddec Vector Index (HNSW) Maintenance</li> <li><code>hnsw info</code> - gathers information about the HNSW index for a given collection</li> <li><code>hnsw rebuild</code> - rebuilds the HNSW index for a given collection and allows the modification of otherwise immutable (construction-only) parameters. Useful command to keep your HNSW index healthy and prevent fragmentation.</li> <li><code>hnsw config</code> - allows you to configure the HNSW index for your Chroma database.</li> <li>\ud83d\udcf8 Collection Maintenance</li> <li><code>collection snapshot</code> - creates a snapshot of a collection. The snapshots are self-contained and are meant to be used for backup and restore.</li> </ul> <p>Need help/Need more?</p> <p>If you need help or need more features, please join the Discord server and let us know. Or just do a pull request on GitHub.</p>"},{"location":"running/maintenance/#installation","title":"Installation","text":"<p>Chroma Ops can be installed using pip:</p> <pre><code>pip install --upgrade chromadb-ops\n</code></pre>"},{"location":"running/maintenance/#usage","title":"Usage","text":""},{"location":"running/maintenance/#database-maintenance","title":"Database Maintenance","text":""},{"location":"running/maintenance/#database-info","title":"Database Info","text":"<p>What it does: Gathers general information about your Chroma persistent database (works only for local persistent databases).</p> <p>Why it's useful: Run this command to better understand the current state of your database. It can provide you with invaluable information about any potential issues and also helps us help you in debugging issues.</p> <pre><code>chops db info /path/to/persist_dir\n</code></pre> <p>Options:</p> <ul> <li><code>--skip-collection-names</code> (<code>-s</code>) - to skip specific collections</li> <li><code>--privacy-mode</code> (<code>-p</code>) - privacy mode hides paths and collection names so that the output can be shared without   exposing sensitive information</li> </ul> <p>When sharing larger outputs consider storing the output in a file:</p> <pre><code>chops db info /path/to/persist_dir -p &gt; chroma_info.txt\n</code></pre> <p>Example output:</p> <pre><code>chops db info smallc\n\n                                 General Info\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                    Property \u2503 Value                                          \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502              Chroma Version \u2502 0.5.5                                          \u2502\n\u2502        Number of Collection \u2502 1                                              \u2502\n\u2502           Persist Directory \u2502 /tmp/tmp9l3ceuvp                               \u2502\n\u2502      Persist Directory Size \u2502 142.2MiB                                       \u2502\n\u2502              SystemDB size: \u2502 81.6MiB (/tmp/tmp9l3ceuvp/chroma.sqlite3)      \u2502\n\u2502     Orphan HNSW Directories \u2502 []                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Collections \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 test \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             'test' Collection Data\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503         Table Data \u2503 Value                                                   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502                 ID \u2502 9e80e4fd-fd4b-47b8-810c-e8ffa57c1912                    \u2502\n\u2502               Name \u2502 test                                                    \u2502\n\u2502           Metadata \u2502 None                                                    \u2502\n\u2502          Dimension \u2502 1536                                                    \u2502\n\u2502             Tenant \u2502 default_tenant                                          \u2502\n\u2502           Database \u2502 default_database                                        \u2502\n\u2502            Records \u2502 10,000                                                  \u2502\n\u2502        WAL Entries \u2502 10,000                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Segments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                            Metadata Segment (test)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                Property \u2503 Value                                              \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502              Segment ID \u2502 832fa2cd-6c40-4eee-ad7d-35f260acaaaa               \u2502\n\u2502                    Type \u2502 urn:chroma:segment/metadata/sqlite                 \u2502\n\u2502                   Scope \u2502 METADATA                                           \u2502\n\u2502        SysDB Max Seq ID \u2502 10,000                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              HNSW Segment (test)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                     Property \u2503 Value                                         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502                   Segment ID \u2502 13609103-d317-4556-a744-008c96229b72          \u2502\n\u2502                         Type \u2502 urn:chroma:segment/vector/hnsw-local-persist\u2026 \u2502\n\u2502                        Scope \u2502 VECTOR                                        \u2502\n\u2502                         Path \u2502 /tmp/tmp9l3ceuvp/13609103-d317-4556-a744-008\u2026 \u2502\n\u2502             SysDB Max Seq ID \u2502 0                                             \u2502\n\u2502                HNSW Dir Size \u2502 60.6MiB                                       \u2502\n\u2502     HNSW Metadata Max Seq ID \u2502 10,000                                        \u2502\n\u2502   HNSW Metadata Total Labels \u2502 10,000                                        \u2502\n\u2502                      WAL Gap \u2502 0                                             \u2502\n\u2502 HNSW Raw Total Active Labels \u2502 10,000                                        \u2502\n\u2502    HNSW Raw Allocated Labels \u2502 10,000                                        \u2502\n\u2502           HNSW Orphan Labels \u2502 set()                                         \u2502\n\u2502          Fragmentation Level \u2502 0.0                                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>\u26a0\ufe0f Interesting things to look for:</p> <ul> <li>Fragmentation Level - the higher the value the more unnecessary memory and performance hits your HNSW index suffers.   It needs to be rebuilt.</li> <li>Orphan HNSW Directories - these are directories that are not associated with any collection. They can be safely   deleted.</li> <li>WAL Entries - high values usually means that you need prune your WAL. Use either this tool or   the official Chroma CLI.</li> <li>HNSW Orphan Labels - this must always be empty set, if you see anything else report it   in Discord @taz.</li> </ul> <p>How to Read the output</p> <p>General Info</p> <p>This section presents general Chroma persistent dir info.</p> <ul> <li>Chroma Version - the currently installed Chroma version.</li> <li>Number of Collection - the number of collections in the persistent dir.</li> <li>Persist Directory - the path to the persistent dir (if privacy mode is off).</li> <li>Persist Directory Size - the size of the persistent dir.</li> <li>SystemDB size - the size of the system database (if privacy mode is off the full path to the sqlite3 file is shown).</li> <li>Orphan HNSW Directories - a list of orphan HNSW directories. These directories are present in the persistent dir but   are not associated with any collection.</li> </ul> <p>Collections</p> <ul> <li>ID - the collection ID.</li> <li>Name - the collection name.</li> <li>Metadata - the metadata associated with the collection.</li> <li>Dimension - the dimension of the embeddings in the collection. (this can be None in case no vectors are present and   the collection is newly created).</li> <li>Tenant - the tenant of the collection.</li> <li>Database - the database of the collection.</li> <li>Records - the number of records in the collection.</li> <li>WAL Entries - the number of WAL entries in the collection (as of 0.5.5 for new instances Chroma will clean WAL for   each collection periodically).</li> </ul> <p>Metadata Segment</p> <ul> <li>Segment ID - the segment ID.</li> <li>Type - the segment type.</li> <li>Scope - the segment scope.</li> <li>SysDB Max Seq ID - the maximum sequence ID in the system database.</li> </ul> <p>HNSW Segment</p> <ul> <li>Segment ID - the segment ID.</li> <li>Type - the segment type.</li> <li>Scope - the segment scope.</li> <li>Path - the path to the HNSW directory.</li> <li>SysDB Max Seq ID - the maximum sequence ID in the system database.</li> <li>HNSW Dir Size - the size of the HNSW directory.</li> <li>HNSW Metadata Max Seq ID - the maximum sequence ID in the HNSW metadata.</li> <li>HNSW Metadata Total Labels - the total number of labels in the HNSW metadata.</li> <li>WAL Gap - the difference between the maximum sequence ID in the system database and the maximum sequence ID in the   HNSW   metadata. The gap usually represents the number of WAL entries that are not committed to the HNSW index.</li> <li>HNSW Raw Total Active Labels - the total number of active labels in the HNSW index.</li> <li>HNSW Raw Allocated Labels - the total number of allocated labels in the HNSW index.</li> <li>HNSW Orphan Labels - a set of orphan labels in the HNSW index. These are labels in the HNSW index that are not visible   to Chroma as they are not part of the metadata. This set should always be empty, if not please report it!!!</li> <li>Fragmentation Level - the fragmentation level of the HNSW index.</li> </ul>"},{"location":"running/maintenance/#database-clean","title":"Database Clean","text":"<p>What it does: Cleans up the database from unused files. It will remove all orphanated HNSW segment directories.</p> <p>Why it's useful: Orphanated HNSW segment directories sometimes are the byproduct of a filesystem failure to remove the HNSW segment directory, most commonly encountered on Windows systems, but any type of file loocking or disk operation failure can cause Chroma to leave behind these directories.</p> <pre><code>chops db clean /path/to/persist_dir\n</code></pre> <p>Supported options are:</p> <ul> <li><code>--dry-run</code> (<code>-d</code>) - to see what would be deleted without actually deleting anything.</li> </ul> <p>Example output:</p> <pre><code>chops db clean smallc\nChromaDB version: 0.6.2\nCleaning up orphanated segment dirs...\n\n                             Orphanated HNSW segment dirs                             \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Segment ID                           \u2503 Path                                        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2E9021A8-A767-4339-B2C2-2F4B22C05F1D \u2502 smallc/2E9021A8-A767-4339-B2C2-2F4B22C05F1D \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAre you sure you want to delete these segment dirs? [y/N]: \n</code></pre>"},{"location":"running/maintenance/#wal-maintenance","title":"WAL Maintenance","text":""},{"location":"running/maintenance/#wal-info","title":"WAL Info","text":"<p>What it does: Gathers information about the Write-Ahead Log (WAL).</p> <p>Why it's useful: Run this command to better understand the current state of the Write-Ahead Log (WAL). It can provide you with invaluable information about any potential issues and also helps us help you in debugging issues.</p> <pre><code>chops wal info /path/to/persist_dir\n</code></pre> <p>Example output:</p> <pre><code>chops wal info smallc\nChromaDB version: 0.6.2\n\nWAL config is set to: auto purge.\n                                         WAL Info                                         \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Collection \u2503 Topic                                                             \u2503 Count \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 test       \u2502 persistent://default/default/97f5234e-d02a-43b8-9909-99447950c949 \u2502 20    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"running/maintenance/#wal-export","title":"WAL Export","text":"<p>What it does: Exports the Write-Ahead Log (WAL) to a <code>jsonl</code> file. This can be used for debugging and for auditing.</p> <p>Why it's useful: This command is useful for exporting the Write-Ahead Log (WAL) to a <code>jsonl</code> file. This can be used for debugging and for auditing.</p> <pre><code>chops wal export /path/to/persist_dir\n</code></pre> <p>Example output:</p> <pre><code>chops wal export smallc --out wal.jsonl\nChromaDB version: 0.6.2\n       Exporting WAL        \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Collection \u2503 WAL Entries \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 test       \u2502 20          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAre you sure you want to export the WAL? [y/N]: y\nExported 20 rows\n</code></pre>"},{"location":"running/maintenance/#wal-commit","title":"WAL Commit","text":"<p>What it does: Commits the Write-Ahead Log (WAL) to all collections with outstanding changes.</p> <p>Why it's useful: This command is useful for committing the Write-Ahead Log (WAL) to all collections with outstanding changes.</p> <pre><code>chops wal commit /path/to/persist_dir\n</code></pre> <p>Options:</p> <ul> <li><code>--skip</code> (<code>-s</code>) - skip certain collections by running <code>chops wal commit /path/to/persist_dir --skip &lt;collection_name&gt;</code></li> <li><code>--yes</code> (<code>-y</code>) - skip confirmation prompt (default: <code>False</code>, prompt will be shown)</li> </ul> <p>Example output:</p> <pre><code>chops wal commit smallc\nChromaDB version: 0.6.2\n     WAL Commit Summary     \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Collection \u2503 WAL Entries \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 test       \u2502 20          \u2502\n\u2502 test1      \u2502 0           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n   Skipped    \n Collections  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Collection \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAre you sure you want to commit the WAL in smallc? As part of the WAL commit action your database will be migrated to currently installed version 0.6.2. [y/N]: y\nProcessing index for collection test (0137d64b-8d71-42f5-b0d9-28716647b068) - total vectors in index 20\nWAL commit completed.\n</code></pre>"},{"location":"running/maintenance/#wal-clean","title":"WAL Clean","text":"<p>What it does: Cleans up the Write-Ahead Log (WAL) from committed transactions. Recent Chroma version automatically prune the WAL so this is not needed unless you have older version of Chroma or disabled automatic WAL pruning.</p> <p>Why it's useful: Keep your WAL in check so it doesn't grow too large (in case automatic WAL pruning is disabled).</p> <pre><code>chops wal clean /path/to/persist_dir\n</code></pre> <p>Options:</p> <ul> <li><code>--yes</code> (<code>-y</code>) - skip confirmation prompt (default: <code>False</code>, prompt will be shown)</li> </ul> <p>Example output:</p> <pre><code>chops wal clean smallc                                                                                                                                                                                                                                                                        11:33:36  \u2601  main \u2602 \u26a1 \u272d\nChromaDB version: 0.6.2\nSize before: 429596\n\nAre you sure you want to clean up the WAL in smallc? This action will delete all WAL entries that are not committed to the HNSW index. [y/N]: y\nCleaning up WAL\nWAL cleaned up. Size after: 388636\n</code></pre>"},{"location":"running/maintenance/#wal-configuration","title":"WAL Configuration","text":"<p>What it does: Configures the Write-Ahead Log (WAL) for your Chroma database.</p> <p>Why it's useful: This command is useful for configuring the Write-Ahead Log (WAL) for your Chroma database.</p> <pre><code>chops wal config /path/to/persist_dir --purge off\n</code></pre> <p>Options:</p> <ul> <li><code>--purge</code> option can be set to <code>auto</code> (automatically purge the WAL when the number of records in the collection exceeds the number of   records in the WAL) or <code>off</code> (disable automatic purge of the WAL). Automatic WAL purge is enabled by default. The automatic purge keeps your slite3 file smaller and faster, but it makes it hard or impossible to restore Chroma.</li> <li><code>--yes</code> option can be set to <code>true</code> (skip confirmation prompt) or <code>false</code> (show confirmation prompt). The default is <code>false</code>.</li> </ul> <p>Example output:</p> <pre><code>chops wal config smallc --purge off\nChromaDB version: 0.6.2\n                           Current WAL config                            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Config key                                \u2503 Config Change             \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Automatically purge (automatically_purge) \u2502 True (old) -&gt; False (new) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAre you sure you want to update the WAL config? [y/N]: y\nWAL config updated successfully!\n</code></pre>"},{"location":"running/maintenance/#full-text-search-fts-maintenance","title":"Full Text Search (FTS) Maintenance","text":""},{"location":"running/maintenance/#fts-rebuild","title":"FTS Rebuild","text":"<p>What it does: Rebuilds the Full Text Search (FTS) index for all collections.</p> <p>Why it's useful: This command is useful for rebuilding the Full Text Search (FTS) index for all collections.</p> <pre><code>chops fts rebuild /path/to/persist_dir\n</code></pre> <p>Additional options:</p> <ul> <li><code>--yes</code> (<code>-y</code>) - skip confirmation prompt (default: <code>False</code>, prompt will be shown)</li> <li><code>--tokenizer</code> (<code>-t</code>) - the tokenizer to use for the index.</li> </ul> <p>Example output:</p> <pre><code>chops fts rebuild --tokenizer unicode61 smallc\nChromaDB version: 0.6.2\n\nAre you sure you want to rebuild the FTS index in smallc? This action will drop the existing FTS index and create a new one. [y/N]: y\nRebuilt FTS. Will try to start your Chroma now.\nNOTE: Depending on the size of your documents in Chroma it may take a while for Chroma to start up again.\nChroma started successfully. FTS rebuilt.\n</code></pre>"},{"location":"running/maintenance/#hnsw-maintenance","title":"HNSW Maintenance","text":""},{"location":"running/maintenance/#hnsw-info","title":"HNSW Info","text":"<p>What it does: Gathers information about the HNSW index for a given collection.</p> <p>Why it's useful: This command is useful for gathering information about the HNSW index for a given collection.</p> <pre><code>chops hnsw info /path/to/persist_dir\n</code></pre> <p>Additional options:</p> <ul> <li><code>--collection</code> (<code>-c</code>) - the collection name</li> <li><code>--verbose</code> (<code>-v</code>) - If specified, the HNSW index will be loaded for more accurate fragmentation level reporting.</li> </ul> <p>Example output:</p> <pre><code>chops hnsw info smallc -c test\nChromaDB version: 0.6.2\n    HNSW details for collection test in default_database database    \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Metric              \u2503 Value                                       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Space               \u2502 cosine                                      \u2502\n\u2502 Dimensions          \u2502 384                                         \u2502\n\u2502 EF Construction     \u2502 200                                         \u2502\n\u2502 EF Search           \u2502 100                                         \u2502\n\u2502 M                   \u2502 64                                          \u2502\n\u2502 Number of threads   \u2502 16                                          \u2502\n\u2502 Resize factor       \u2502 1.2                                         \u2502\n\u2502 Batch size          \u2502 100                                         \u2502\n\u2502 Sync threshold      \u2502 1000                                        \u2502\n\u2502 Segment ID          \u2502 0137d64b-8d71-42f5-b0d9-28716647b068        \u2502\n\u2502 Path                \u2502 smallc/0137d64b-8d71-42f5-b0d9-28716647b068 \u2502\n\u2502 Has metadata        \u2502 True                                        \u2502\n\u2502 Number of elements  \u2502 20                                          \u2502\n\u2502 Collection ID       \u2502 97f5234e-d02a-43b8-9909-99447950c949        \u2502\n\u2502 Index size          \u2502 41.6KiB                                     \u2502\n\u2502 Fragmentation level \u2502 0.00% (estimated)                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"running/maintenance/#hnsw-rebuild","title":"HNSW Rebuild","text":"<p>What it does: Rebuilds the HNSW index for a given collection and allows the modification of otherwise immutable (construction-only) parameters.</p> <p>Why it's useful: This command is useful for rebuilding the HNSW index for a given collection and allows the modification of otherwise immutable (construction-only) parameters.</p> <pre><code>chops hnsw rebuild /path/to/persist_dir\n</code></pre> <p>Options:</p> <ul> <li><code>--backup</code> (<code>-b</code>) - backup the old index. At the end of the rebuild process the location of the backed up index will be printed out. (default: <code>True</code>)</li> <li><code>--database</code> (<code>-d</code>) - the database name (default: <code>default_database</code>)</li> <li><code>--yes</code> (<code>-y</code>) - skip confirmation prompt (default: <code>False</code>, prompt will be shown)</li> <li><code>--space</code> (<code>-s</code>) - the distance metric to use for the index.</li> <li><code>--construction-ef</code> (<code>-c</code>) - the construction ef to use for the index.</li> <li><code>--search-ef</code> (<code>-e</code>) - the search ef to use for the index.</li> <li><code>--m</code> (<code>-m</code>) - the m to use for the index.</li> <li><code>--num-threads</code> (<code>-t</code>) - the number of threads to use for the index.</li> <li><code>--resize-factor</code> (<code>-r</code>) - the resize factor to use for the index.</li> <li><code>--batch-size</code> (<code>-b</code>) - the batch size to use for the index.</li> <li><code>--sync-threshold</code> (<code>-s</code>) - the sync threshold to use for the index.</li> </ul> <p>Unchanged options will be skipped</p> <p>All the HNSW index options default to <code>None</code> which means no changes will be made if the parameter is not specified. Additionally, any options provided that are identical to the current index configuration will be skipped.</p> <p>Example output:</p> <pre><code>chops hnsw rebuild smallc -c test --m 64 --construction-ef 200\nChromaDB version: 0.6.2\n    HNSW details for collection test in default_database database    \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Metric              \u2503 Value                                       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Space               \u2502 cosine                                      \u2502\n\u2502 Dimensions          \u2502 384                                         \u2502\n\u2502 EF Construction     \u2502 200                                         \u2502\n\u2502 EF Search           \u2502 100                                         \u2502\n\u2502 M                   \u2502 64                                          \u2502\n\u2502 Number of threads   \u2502 16                                          \u2502\n\u2502 Resize factor       \u2502 1.2                                         \u2502\n\u2502 Batch size          \u2502 100                                         \u2502\n\u2502 Sync threshold      \u2502 1000                                        \u2502\n\u2502 Segment ID          \u2502 0137d64b-8d71-42f5-b0d9-28716647b068        \u2502\n\u2502 Path                \u2502 smallc/0137d64b-8d71-42f5-b0d9-28716647b068 \u2502\n\u2502 Has metadata        \u2502 True                                        \u2502\n\u2502 Number of elements  \u2502 20                                          \u2502\n\u2502 Collection ID       \u2502 97f5234e-d02a-43b8-9909-99447950c949        \u2502\n\u2502 Index size          \u2502 47.6KiB                                     \u2502\n\u2502 Fragmentation level \u2502 0.00% (estimated)                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    HNSW segment config changes     \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Config Key           \u2503 Old \u2503 New \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 hnsw:construction_ef \u2502 100 \u2502 200 \u2502\n\u2502 hnsw:M               \u2502 102 \u2502 64  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\nAre you sure you want to rebuild this index? [y/N]: y\nBackup of old index created at smallc/0137d64b-8d71-42f5-b0d9-28716647b068_backup_20250208100514\n    HNSW details for collection test in default_database database    \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Metric              \u2503 Value                                       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Space               \u2502 cosine                                      \u2502\n\u2502 Dimensions          \u2502 384                                         \u2502\n\u2502 EF Construction     \u2502 200                                         \u2502\n\u2502 EF Search           \u2502 100                                         \u2502\n\u2502 M                   \u2502 64                                          \u2502\n\u2502 Number of threads   \u2502 16                                          \u2502\n\u2502 Resize factor       \u2502 1.2                                         \u2502\n\u2502 Batch size          \u2502 100                                         \u2502\n\u2502 Sync threshold      \u2502 1000                                        \u2502\n\u2502 Segment ID          \u2502 0137d64b-8d71-42f5-b0d9-28716647b068        \u2502\n\u2502 Path                \u2502 smallc/0137d64b-8d71-42f5-b0d9-28716647b068 \u2502\n\u2502 Has metadata        \u2502 True                                        \u2502\n\u2502 Number of elements  \u2502 20                                          \u2502\n\u2502 Collection ID       \u2502 97f5234e-d02a-43b8-9909-99447950c949        \u2502\n\u2502 Index size          \u2502 41.6KiB                                     \u2502\n\u2502 Fragmentation level \u2502 0.00%                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"running/maintenance/#hnsw-configuration","title":"HNSW Configuration","text":"<p>What it does: Configures the HNSW index for your Chroma database.</p> <p>Why it's useful: This command is useful for configuring the HNSW index for your Chroma database.</p> <pre><code>chops hnsw config /path/to/persist_dir --collection &lt;collection_name&gt;\n</code></pre> <p>Options:</p> <ul> <li><code>--search-ef</code> (<code>-e</code>) - the search ef to use for the index.</li> <li><code>--num-threads</code> (<code>-t</code>) - the number of threads to use for the index.</li> <li><code>--resize-factor</code> (<code>-r</code>) - the resize factor to use for the index.</li> <li><code>--batch-size</code> (<code>-b</code>) - the batch size to use for the index.</li> <li><code>--sync-threshold</code> (<code>-s</code>) - the sync threshold to use for the index.</li> </ul> <p>Example output:</p> <pre><code>chops hnsw config smallc -c test --search-ef 100\nChromaDB version: 0.6.2\n HNSW segment config changes  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Config Key     \u2503 Old \u2503 New \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 hnsw:search_ef \u2502 110 \u2502 100 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\nAre you sure you want to apply these changes? [y/N]: y\nHNSW index configuration modified successfully\n</code></pre>"},{"location":"running/maintenance/#collection-maintenance","title":"Collection Maintenance","text":""},{"location":"running/maintenance/#collection-snapshot","title":"Collection Snapshot","text":"<p>What it does: Creates a snapshot of a collection. The snapshots are self-contained sqlite3 files.</p> <p>Why it's useful: The command is useful if you want to create a backup or a point-in-time copy of a collection in its entirety. The snapshot files are self-contained and use sqlite3 as a storage engine. You can use <code>sqlite3</code> commands to inspect the snapshot files.</p> <pre><code>chops collection snapshot /path/to/persist_dir --collection &lt;collection_name&gt; -o /path/to/snapshot.sqlite3\n</code></pre> <p>Additional options:</p> <ul> <li><code>--yes</code> (<code>-y</code>) - skip confirmation prompt (default: <code>False</code>, prompt will be shown)</li> <li><code>--collection</code> (<code>-c</code>) - the collection name</li> <li><code>--output</code> (<code>-o</code>) - the path to the output snapshot file</li> </ul> <p>Example output:</p> <pre><code>chops collection snapshot ./smallc --collection test -o snapshot.sqlite3\nChromaDB version: 0.6.2\n\nAre you sure you want to overwrite /Users/tazarov/experiments/chroma/chromadb-ops/snapshot.sqlite3 file? [y/N]: y\nBootstrapping snapshot database...\nSnapshot database bootstrapped in /Users/tazarov/experiments/chroma/chromadb-ops/snapshot.sqlite3\nCopying collection test to snapshot database...\n  Copying collection to snapshot   \n            database...            \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Table                   \u2503 Count \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Embeddings Queue        \u2502 20    \u2502\n\u2502 Max Seq ID              \u2502 1     \u2502\n\u2502 Embeddings              \u2502 20    \u2502\n\u2502 Embedding Metadata      \u2502 20    \u2502\n\u2502 Segments                \u2502 2     \u2502\n\u2502 Segment Metadata        \u2502 3     \u2502\n\u2502 Collections             \u2502 1     \u2502\n\u2502 Collection Metadata     \u2502 0     \u2502\n\u2502 HNSW Segment Data Files \u2502 5     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAre you sure you want to copy this collection to the snapshot database? [y/N]: y\nCollection test copied to snapshot database in /Users/tazarov/experiments/chroma/chromadb-ops/snapshot.sqlite3\n</code></pre>"},{"location":"running/performance-tips/","title":"Performance Tips","text":"<p>This section covers tips and tricks of how to improve your Chroma performance.</p>"},{"location":"running/performance-tips/#rebuild-hnsw-for-your-architecutre","title":"Rebuild HNSW for your architecutre","text":"<p>Single node chroma core package and server ship with a default HNSW build which is optimized for maximum compatibility. The default HNSW does not make use of available optimization for your CPU architecture such as SIMD/AVX.</p> <p>You can rebuild the HNSW index for the core package or the server as follows.</p> Core PackageServer <p>To rebuild the HNSW index locally you may need to install build tooling such as <code>gcc</code> depending on your operating system.</p> <pre><code>pip install --no-binary :all: chroma-hnswlib\n</code></pre> <p>In the following snippet, we clone the Chroma repository (you'll need git and docker installed), and then build a new docker image with the HNSW rebuild flag set to <code>true</code>.</p> <pre><code>git clone https://github.com/chroma-core/chroma.git &amp;&amp; cd chroma\ndocker build --build-arg REBUILD_HNSWLIB=true -t my-chroma-image:latest .\n</code></pre> Need help? <p>If you need help with the above steps, please reach out to us on Discord (look for <code>@taz</code>)</p>"},{"location":"running/performance-tips/#reducing-shortening-the-dimensionality-of-your-embeddings","title":"Reducing (shortening) the dimensionality of your embeddings","text":"<p>Some embeddings models (or APIs) offer the ability to reduce the dimensionality of the resulting embeddings. This is a great way to reduce the storage and memory requirements of your Chroma.</p> <p>Currently the following embedding functions support this feature:</p> <ul> <li>OpenAI with 3rd generation models (i.e. <code>text-embedding-3-small</code> and <code>text-embedding-3-large</code>)</li> </ul>"},{"location":"running/performance-tips/#openai-example","title":"OpenAI Example","text":"<p>For more information on shortening embeddings see the official OpenAI Blog post.</p> PythonJavascript <pre><code>from chromadb.utils.embedding_functions.openai_embedding_function import (\n    OpenAIEmbeddingFunction,\n)\nimport os\n\nef = OpenAIEmbeddingFunction(api_key=os.environ[\"OPENAI_API_KEY\"], model_name=\"text-embedding-3-small\", dimensions=64)\nembeddings = ef([\"hello world\"])\n</code></pre> <pre><code>import  {OpenAIEmbeddingFunction} from \"chromadb\"\n\nconst embedder = new OpenAIEmbeddingFunction({\n        openai_api_key: process.env.OPENAI_API_KEY,\n        openai_embedding_dimensions: 64,\n        openai_model: \"text-embedding-3-small\",\n    });\nconst embeddings = embedder.generate([\"hello world\"]);\n</code></pre>"},{"location":"running/performance-tips/#defragment-your-hnsw-indices","title":"Defragment your HNSW indices","text":"<p>If you have many updates (other than <code>add</code>) on your collections, overtime the HNSW indices become fragmented which has the following consequences:</p> <ul> <li>Increased memory usage</li> <li>Increased disk usage</li> <li>Increased query times</li> <li>Reduced accuracy</li> </ul> <p>To mitigate the above side-effects, you can periodically defragment/compact your HNSW indices. To do that use the <code>chops hnsw rebuild</code> command.</p>"},{"location":"running/road-to-prod/","title":"Road To Production","text":"<p>In this section we will cover considerations for operating Chroma ina production environment.</p> <p>To operate Chroma in production your deployment must follow your organization's best practices and guidelines around business continuity, security, and compliance. Here we will list the core concepts and offer some guidance on how to achieve them.</p> <p>Core system abilities:</p> <ul> <li>High Availability - The deployment should be able to handle failures while continuing to serve requests.</li> <li>Scalability - The deployment should be able to handle increased load by adding more resources (aka scale   horizontally).</li> <li>Privacy and Security - The deployment should protect data from unauthorized access and ensure data integrity.</li> <li>Observability - The deployment should provide metrics and logs to help operators understand the system's health.</li> <li>Backup and Restore - The deployment should have a backup and restore strategy to protect against data loss.</li> <li>Disaster Recovery - The deployment should have a disaster recovery plan to recover from catastrophic failures.</li> <li>Maintenance - The deployment should be easy to maintain and upgrade.</li> </ul> <p>While our guidance is most likely incomplete it can be taken as a compliment to your own organizational processes. For those deploying Chroma in a smaller enterprise without such processes, we advise common sense and caution.</p>"},{"location":"running/road-to-prod/#high-availability","title":"High Availability","text":""},{"location":"running/road-to-prod/#scalability","title":"Scalability","text":""},{"location":"running/road-to-prod/#privacy-and-security","title":"Privacy and Security","text":""},{"location":"running/road-to-prod/#data-security","title":"Data Security","text":""},{"location":"running/road-to-prod/#in-transit","title":"In Transit","text":"<p>The bare minimum for securing data in transit is to use HTTPS when performing Chroma API calls. This ensures that data is encrypted when it is sent over the network.</p> <p>There are several ways to achieve this:</p> <ul> <li>Use a reverse proxy like Envoy or Nginx to terminate SSL/TLS connections.</li> <li>Use a load balancer like AWS ELB or Google Cloud Load Balancer to terminate SSL/TLS connections (technically a Envoy   and Nginx are also LBs).</li> <li>Use a service mesh like Istio or Linkerd to manage SSL/TLS connections between services.</li> <li>Enable SSL/TLS in your Chroma server.</li> </ul> <p>Depending on your requirements you may choose one or more of these options.</p> <p>Reverse Proxy:</p> <p>Load Balancer:</p> <p>Service Mesh:</p> <p>Chroma Server:</p>"},{"location":"running/road-to-prod/#at-rest","title":"At Rest","text":""},{"location":"running/road-to-prod/#access-control","title":"Access Control","text":""},{"location":"running/road-to-prod/#authentication","title":"Authentication","text":""},{"location":"running/road-to-prod/#authorization","title":"Authorization","text":""},{"location":"running/road-to-prod/#observability","title":"Observability","text":""},{"location":"running/road-to-prod/#backup-and-restore","title":"Backup and Restore","text":""},{"location":"running/road-to-prod/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"running/road-to-prod/#maintenance","title":"Maintenance","text":""},{"location":"running/running-chroma/","title":"Running Chroma","text":""},{"location":"running/running-chroma/#local-server","title":"Local Server","text":"<p>Article Link</p> <p>This article is also available on Medium Running ChromaDB \u2014 Part 1: Local Server.</p>"},{"location":"running/running-chroma/#chroma-cli","title":"Chroma CLI","text":"<p>The simplest way to run Chroma locally is via the Chroma <code>cli</code>.</p> <p>Prerequisites:</p> <ul> <li>Python 3.9+ (for <code>pip</code>, <code>pipx</code>, or <code>uv</code>) - Download Python | Python.org</li> <li>Node.js (for <code>npm</code>, <code>pnpm</code>, <code>bun</code>, or <code>yarn</code>) - Download Node.js | nodejs.org</li> <li><code>curl</code> (or Windows PowerShell) for standalone CLI install script</li> </ul> <p>Install Chroma CLI with any of the following:</p>"},{"location":"running/running-chroma/#python","title":"Python","text":"pipuv (pip-compatible)pipxuv tool (pipx-like) <pre><code>pip install chromadb\n</code></pre> <pre><code>uv venv .venv\nsource .venv/bin/activate  # macOS/Linux\n# Windows (PowerShell): .venv\\Scripts\\Activate.ps1\nuv pip install chromadb\n</code></pre> <pre><code>pipx install chromadb\n</code></pre> <pre><code>uv tool install chromadb\n</code></pre>"},{"location":"running/running-chroma/#javascript-global","title":"JavaScript (Global)","text":"npmpnpmbunyarn <pre><code>npm install -g chromadb\n</code></pre> <pre><code>pnpm add -g chromadb\n</code></pre> <pre><code>bun add -g chromadb\n</code></pre> <pre><code>yarn global add chromadb\n</code></pre>"},{"location":"running/running-chroma/#standalone-installer","title":"Standalone Installer","text":"cURL (macOS/Linux)Windows (PowerShell) <pre><code>curl -sSL https://raw.githubusercontent.com/chroma-core/chroma/main/rust/cli/install/install.sh | bash\n</code></pre> <pre><code>iex ((New-Object System.Net.WebClient).DownloadString('https://raw.githubusercontent.com/chroma-core/chroma/main/rust/cli/install/install.ps1'))\n</code></pre> <pre><code>chroma run --host localhost --port 8000 --path ./my_chroma_data\n</code></pre> <p><code>--host</code> The host to bind to, by default <code>localhost</code>. Use <code>0.0.0.0</code> to expose it on your local network.</p> <p><code>--port</code> The port on which to listen to, by default this is <code>8000</code>.</p> <p><code>--path</code> The path where to persist your Chroma data locally.</p> <p>Target Path Install</p> <p>It is possible to install Chroma in a specific directory by running <code>pip install chromadb -t /path/to/dir</code>. To run Chroma CLI from that install location, execute: <code>/path/to/dir/bin/chroma run --path ./my_chroma_data</code></p> <p>For advanced 1.x server settings (YAML config and <code>CHROMA_</code> overrides), see Chroma Configuration.</p> Optional: CLI with YAML config (collapsed) chroma.local.yaml<pre><code>port: 8000\nlisten_address: \"127.0.0.1\"\npersist_path: \"./my_chroma_data\"\nallow_reset: false\nsqlitedb:\n  hash_type: \"md5\"\n  migration_mode: \"apply\"\n</code></pre> <pre><code>CONFIG_PATH=./chroma.local.yaml chroma run\n</code></pre>"},{"location":"running/running-chroma/#docker","title":"Docker","text":"<p>Running Chroma server locally can be achieved via a simple docker command as shown below.</p> <p>Prerequisites:</p> <ul> <li>Docker - Overview of Docker Desktop | Docker Docs</li> </ul> <pre><code>docker run -d --rm --name chromadb \\\n  -p 8000:8000 \\\n  -v ./chroma-data:/data \\\n  chromadb/chroma:1.5.1\n</code></pre> <p>Options:</p> <ul> <li><code>-p 8000:8000</code> specifies the port on which the Chroma server will be exposed.</li> <li><code>-v</code> specifies a local dir which is where Chroma will store its data so when the container is destroyed the data   remains. For current Chroma server images, mount <code>/data</code> to persist DB files.</li> <li><code>chromadb/chroma:1.5.1</code> indicates the Chroma release version.</li> </ul> <p>Current v1.x Images</p> <p>Legacy environment variables such as <code>IS_PERSISTENT</code>, <code>PERSIST_DIRECTORY</code>, and <code>ANONYMIZED_TELEMETRY</code> are from older server configuration flows and should not be used in the default v1.x Docker run setup.</p> Optional: Docker with YAML config file (collapsed) chroma.docker.yaml<pre><code>port: 8000\nlisten_address: \"0.0.0.0\"\npersist_path: \"/data\"\nallow_reset: false\nsqlitedb:\n  hash_type: \"md5\"\n  migration_mode: \"apply\"\n</code></pre> <pre><code>docker run -d --rm --name chromadb \\\n  -p 8000:8000 \\\n  -v ./chroma-data:/data \\\n  -v ./chroma.docker.yaml:/chroma/config.yaml:ro \\\n  -e CONFIG_PATH=/chroma/config.yaml \\\n  chromadb/chroma:1.5.1\n</code></pre>"},{"location":"running/running-chroma/#docker-compose","title":"Docker Compose","text":"<p>Chroma server can also be run with Docker Compose by creating a <code>docker-compose.yaml</code>.</p> <p>Prerequisites:</p> <ul> <li>Docker - Overview of Docker Desktop | Docker Docs</li> </ul> <pre><code>services:\n  chromadb:\n    image: chromadb/chroma:1.5.1\n    volumes:\n      - ./chroma-data:/data\n    ports:\n      - \"8000:8000\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/api/v2/heartbeat\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n</code></pre> <p>The above will create a container with Chroma <code>1.5.1</code>, expose it on local port <code>8000</code>, and persist data in <code>./chroma-data</code> relative to where <code>docker-compose.yaml</code> is run.</p> Optional: Docker Compose with YAML config file (collapsed) <pre><code>services:\n  chromadb:\n    image: chromadb/chroma:1.5.1\n    volumes:\n      - ./chroma-data:/data\n      - ./chroma.docker.yaml:/chroma/config.yaml:ro\n    environment:\n      - CONFIG_PATH=/chroma/config.yaml\n    ports:\n      - \"8000:8000\"\n</code></pre> <p>Versioning</p> <p>When running Chroma with docker compose try to pin the version to a specific release. This will ensure intentional upgrades and avoid potential issues (usually with clients).</p>"},{"location":"running/running-chroma/#minikube-with-helm-chart","title":"Minikube With Helm Chart","text":"<p>KinD Alternative</p> <p>This deployment can also be done with <code>KinD</code>, depending on your preference.</p> <p>A more advanced approach to running Chroma locally (but also on a remote cluster) is to deploy it using a Helm chart.</p> <p>Community-Maintained Chart</p> <p>The chart used here is not a first-party Chroma chart and is maintained by a core contributor.</p> <p>Prerequisites:</p> <ul> <li>Docker - Overview of Docker Desktop | Docker Docs</li> <li>Install minikube - minikube start | minikube (k8s.io)</li> <li>kubectl - Install Tools | Kubernetes</li> <li>Helm - Helm | Installing Helm</li> </ul> <p>Once you have all of the above, running Chroma in a local <code>minikube</code> cluster is quite simple.</p> <p>Create a <code>minikube</code> cluster:</p> <pre><code>minikube start --addons=ingress -p chroma\nminikube profile chroma\n</code></pre> <p>Get and install the chart:</p> <pre><code>helm repo add chroma https://amikos-tech.github.io/chromadb-chart/\nhelm repo update\nhelm install chroma chroma/chromadb \\\n  --set image.tag=\"1.5.1\"\n</code></pre> Auth values for Chroma <code>&gt;= 1.0.0</code> <p>Chart values under <code>chromadb.auth.*</code> are legacy and ignored. Use network-level controls (private networking, ingress auth, API gateway, mTLS) when needed.</p> <p>The first step to connect and start using Chroma is to forward your port:</p> <pre><code>minikube service chroma-chromadb --url\n</code></pre> <p>The command returns a local URL such as <code>http://127.0.0.1:61892</code>.</p> Driver-specific behavior <p>On some setups (for example Docker driver on macOS), this command runs a local tunnel in the foreground. Keep that terminal open while you use the URL.</p> <p>Test it out (<code>pip install chromadb</code>):</p> <pre><code>import chromadb\n\nclient = chromadb.HttpClient(host=\"http://127.0.0.1:61892\")\nclient.heartbeat()  # public endpoint\n\nclient.get_version()  # public endpoint\n\nclient.list_collections()  # expected to work in this local chart setup\n</code></pre> <p>For more information about the Helm chart, see amikos-tech/chromadb-chart or Artifact Hub.</p>"},{"location":"running/systemd-service/","title":"Systemd service","text":"<p>You can run Chroma as a systemd service which wil allow you to automatically start Chroma on boot and restart it if it crashes.</p>"},{"location":"running/systemd-service/#docker-compose","title":"Docker Compose","text":"<p>The following is an examples systemd service for running Chroma using Docker Compose.</p> <p>Create a file <code>/etc/systemd/system/chroma.service</code> with the following content:</p> <p>Example assumptions</p> <p>The below example assumes Debian-based system with docker-ce installed.</p> <pre><code>[Unit]\nDescription = Chroma Service\nAfter = network.target docker.service\nRequires = docker.service\n\n[Service]\nType = forking\nUser = root\nGroup = root\nWorkingDirectory = /home/admin/chroma\nExecStart = /usr/bin/docker compose up -d\nExecStop = /usr/bin/docker compose down\nRemainAfterExit = true\n\n[Install]\nWantedBy = multi-user.target\n</code></pre> <p>Replace <code>WorkingDirectory</code> with the path to your docker compose is. You may also need to replace <code>/usr/bin/docker</code> with the path to your docker binary.</p> <p>Alternatively you can install directly from a gist:</p> <pre><code>wget https://gist.githubusercontent.com/tazarov/9c46966de0b32a4962dcc79dce8b2646/raw/7cf8c471f33fba8a51d6f808f9b1af6ca1b0923c/chroma-docker.service \\\n  -O /etc/systemd/system/chroma.service\n</code></pre> <p>Loading, enabling and starting the service:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable chroma\nsudo systemctl start chroma\n</code></pre> <p>Type=forking</p> <p>In the above example, we use <code>Type=forking</code> because Docker Compose runs in the background (<code>-d</code>). If you are using a different command that runs in the foreground, you may need to use <code>Type=simple</code> instead.</p>"},{"location":"running/systemd-service/#chroma-cli","title":"Chroma CLI","text":"<p>The following is an examples systemd service for running Chroma using the Chroma CLI.</p> <p>Create a file <code>/etc/systemd/system/chroma.service</code> with the following content:</p> <p>Example assumptions</p> <p>The below example assumes that Chroma is installed in Python <code>site-packages</code> package.</p> <pre><code>[Unit]\nDescription = Chroma Service\nAfter = network.target\n\n[Service]\nType = simple\nUser = root\nGroup = root\nWorkingDirectory = /chroma\nExecStart=/usr/local/bin/chroma run --host 127.0.0.1 --port 8000 --path /chroma/data --log-path /var/log/chroma.log\n\n[Install]\nWantedBy = multi-user.target\n</code></pre> <p>Replace the <code>WorkingDirectory</code>, <code>/chroma/data</code> and <code>/var/log/chroma.log</code> with the appropriate paths.</p> <p>Safe Config</p> <p>The above example service listens and <code>localhost</code> which may not work if you are looking to expose Chroma to outside world. Adjust the <code>--host</code> and <code>--port</code> flags as needed.</p> <p>Alternatively you can install from a gist:</p> <pre><code>wget https://gist.githubusercontent.com/tazarov/5e10ce892c06757d8188a8a34cd6d26d/raw/327a9d0b07afeb0b0cb77453aa9171fdd190984f/chroma-cli.service \\\n  -O /etc/systemd/system/chroma.service\n</code></pre> <p>Loading, enabling and starting the service:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable chroma\nsudo systemctl start chroma\n</code></pre> <p>Type=simple</p> <p>In the above example, we use <code>Type=simple</code> because the Chroma CLI runs in the foreground. If you are using a different command that runs in the background, you may need to use <code>Type=forking</code> instead.</p>"},{"location":"security/","title":"Security","text":"<p>Security is an important topic and this section is devoted to it.</p> <p>There are many ways to secure a service, such as Chroma and this section attempts to encompass the most common use cases.</p> <p>Way to secure Chroma include:</p> <ul> <li>In-transit encryption using SSL/TLS certificates</li> <li>Access control</li> <li>At-rest encryption</li> <li>Adding authentication and authorization</li> </ul>"},{"location":"security/#ssltls-certificates","title":"SSL/TLS Certificates","text":"<p>Securing your Chroma with a proxy is one of the most common ways to secure your Chroma. Ensuring that all traffic between your client and Chroma server is encrypted is a good practice.</p> <p>There are multiple ways to secure your Chroma instance using SSL/TLS certificates and here we'll explore a few.</p> <ul> <li>SSL/TLS certificate in Chroma server - configure and use SSL/TLS certificates directly in Chroma.</li> <li>Proxy with SSL/TLS termination - use a proxy to terminate SSL/TLS and forward traffic to Chroma.</li> <li>(Coming soon) Cloud Provider API Gateway with SSL/TLS termination - use a cloud provider's API Gateway to terminate SSL/TLS and   forward traffic to Chroma.</li> </ul>"},{"location":"security/#authentication-and-authorization","title":"Authentication and Authorization","text":"<p>Version prior to 1.0.x support legacy authentication and authorization - Configure Chroma built-in authentication and authorization.</p> <p>Versions 1.0.0-1.0.10 do not support Authentication or Authorization natively so you will need to adjust your deployment with a proxy-based authentcation.</p>"},{"location":"security/auth-1.0.x/","title":"Authentication in Chroma v1.0.x","text":""},{"location":"security/auth-1.0.x/#envoy","title":"Envoy","text":"<p>You can secure your Chroma instance with a token-based auth using Envoy proxy.</p> <p>Create a <code>envoy.yaml</code> configuration file with the following content (adjust as needed or combined with SSL):</p> <pre><code>static_resources:\n  listeners:\n    - name: listener_0\n      address:\n        socket_address:\n          address: 0.0.0.0\n          port_value: 8000\n      filter_chains:\n        - filters:\n            - name: envoy.filters.network.http_connection_manager\n              typed_config:\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\n                stat_prefix: ingress_http\n                route_config:\n                  name: chroma_route\n                  virtual_hosts:\n                    - name: local_chromadb\n                      domains: [ \"*\" ]\n                      routes:\n                        - match:\n                            prefix: \"/\"\n                          route:\n                            cluster: chromadb_service\n                            prefix_rewrite: \"/\"\n                http_filters:\n                  - name: envoy.filters.http.rbac\n                    typed_config:\n                      \"@type\": type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBAC\n                      rules:\n                        action: ALLOW\n                        policies:\n                          \"static-token-policy\":\n                            permissions:\n                              - header:\n                                  name: %CHROMA_AUTH_TOKEN_TRANSPORT_HEADER%\n                                  string_match:\n                                    exact: %CHROMA_SERVER_AUTHN_CREDENTIALS%\n                            principals:\n                              - any: true\n                  - name: envoy.filters.http.router\n                    typed_config:\n                      \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\n  clusters:\n    - name: chromadb_service\n      connect_timeout: 0.25s\n      type: LOGICAL_DNS\n      lb_policy: ROUND_ROBIN\n      load_assignment:\n        cluster_name: chromadb_service\n        endpoints:\n          - lb_endpoints:\n              - endpoint:\n                  address:\n                    socket_address:\n                      address: chromadb\n                      port_value: 8000\n</code></pre> <p>Then create a<code>entrypoint.sh</code> startup script to interpolate the values in the <code>envoy.yaml</code> configuration.</p> <pre><code>#!/bin/sh\n\nsed 's/%CHROMA_AUTH_TOKEN_TRANSPORT_HEADER%/'$CHROMA_AUTH_TOKEN_TRANSPORT_HEADER'/g' /opt/bitnami/envoy/conf/envoy.yaml &gt; /tmp/envoy_temp.yaml\n\nif [ $CHROMA_AUTH_TOKEN_TRANSPORT_HEADER = \"Authorization\" ]; then\n  sed -i 's/%CHROMA_SERVER_AUTHN_CREDENTIALS%/Bearer '$CHROMA_SERVER_AUTHN_CREDENTIALS'/g' /tmp/envoy_temp.yaml\nelse\n  sed -i 's/%CHROMA_SERVER_AUTHN_CREDENTIALS%/'$CHROMA_SERVER_AUTHN_CREDENTIALS'/g' /tmp/envoy_temp.yaml\nfi\n\ncat /tmp/envoy_temp.yaml\n\n/opt/bitnami/envoy/bin/envoy -c /tmp/envoy_temp.yaml\n</code></pre> <p>Last but not least your <code>docker-compose.yaml</code>:</p> <pre><code>networks:\n  net:\n    driver: bridge\nservices:\n  envoy:\n    image: bitnami/envoy\n    volumes:\n      - ./envoy.yaml:/opt/bitnami/envoy/conf/envoy.yaml\n      - ./certs:/etc/envoy/certs\n      - ./entrypoint.sh:/entrypoint.sh\n    ports:\n      - \"8000:8000\"\n    environment:\n      CHROMA_SERVER_AUTHN_CREDENTIALS: ${CHROMA_SERVER_AUTHN_CREDENTIALS:-chr0m4t0k3n}\n      CHROMA_AUTH_TOKEN_TRANSPORT_HEADER: ${CHROMA_AUTH_TOKEN_TRANSPORT_HEADER:-Authorization}\n    networks:\n      - net\n    entrypoint: |\n      sh -c \"\n      chmod +x /entrypoint.sh &amp;&amp; \\\n      /entrypoint.sh\n      \"\n  chromadb:\n    image: chromadb/chroma:1.0.10\n    volumes:\n      - ./chroma-data:/data\n    networks:\n      - net\n    healthcheck:\n      # Adjust below to match your container port\n      test: [\"CMD\", \"bash\", \"-c\", \"echo -n '' &gt; /dev/tcp/127.0.0.1/8000\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n</code></pre> <p>To get going configure your preferred auth type:</p> <ul> <li>Bearer <code>Authorization</code> header</li> <li><code>X-Chroma-Token</code> header</li> </ul>"},{"location":"security/auth-1.0.x/#authorization-header","title":"<code>Authorization</code> header:","text":"<pre><code>export CHROMA_AUTH_TOKEN_TRANSPORT_HEADER=Authorization\nexport CHROMA_SERVER_AUTHN_CREDENTIALS=myT0k3n123\ndocker compose up -d\n</code></pre> <p>Verify:</p> <pre><code>curl -v http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections -H \"Authorization: Bearer myT0k3n123\" \n</code></pre> <p>Header format</p> <p>Observe the presence of <code>Bearer</code> in the authorization header</p> <pre><code>import chromadb\nfrom chromadb.config import Settings\n\nclient = chromadb.HttpClient(\n  settings=Settings(\n      chroma_client_auth_provider=\"chromadb.auth.token_authn.TokenAuthClientProvider\",\n      chroma_client_auth_credentials=\"myT0k3n123\",\n      chroma_auth_token_transport_header=\"Authorization\"\n  )\n)\n\n# if everything is correctly configured the below should list all collections\nclient.list_collections()\n</code></pre>"},{"location":"security/auth-1.0.x/#x-chroma-token-header","title":"<code>X-Chroma-Token</code> header:","text":"<pre><code>export CHROMA_AUTH_TOKEN_TRANSPORT_HEADER=X-Chroma-Token\nexport CHROMA_SERVER_AUTHN_CREDENTIALS=myT0k3n123\ndocker compose up -d\n</code></pre> <p>Verify:</p> <pre><code>curl -v http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database/collections -H \"X-Chroma-Token: myT0k3n123\"\n</code></pre> <pre><code>import chromadb\nfrom chromadb.config import Settings\n\nclient = chromadb.HttpClient(\n  settings=Settings(\n      chroma_client_auth_provider=\"chromadb.auth.token_authn.TokenAuthClientProvider\",\n      chroma_client_auth_credentials=\"myT0k3n123\",\n      chroma_auth_token_transport_header=\"X-Chroma-Token\"\n  )\n)\n\n# if everything is correctly configured the below should list all collections\nclient.list_collections()\n</code></pre>"},{"location":"security/chroma-ssl-cert/","title":"SSL/TLS Certificates in Chroma","text":"<p>Chroma uses uvicorn as an ASGI server, which can be configured to use SSL/TLS certificates.</p> <p>CLI not supported</p> <p>Using certificates with Chroma CLI is not yet supported.</p> Performance Impact <p>Using certificates within Chroma will have a performance impact as <code>uvicorn</code> will need to hnadle  the encryption and decryption of the data. If performance is of concern,  consider using a reverse proxy like <code>nginx</code> or <code>envoy</code> to handle the SSL/TLS termination.</p>"},{"location":"security/chroma-ssl-cert/#self-signed-certificates","title":"Self-Signed Certificates","text":""},{"location":"security/chroma-ssl-cert/#creating-a-self-signed-certificate","title":"Creating a self-signed certificate","text":"<p>Important</p> <p>The <code>SAN</code> (Subject Alternative Name) is required for the certificate to work as modern security standards require    the certificate to match the domain name.</p> <p>You will also need to create a <code>openssl.cnf</code> file in the same directory with the following content:</p> <pre><code>```ini\n[req]\ndistinguished_name = req_distinguished_name\nx509_extensions = usr_cert\n\n[req_distinguished_name]\nCN = $ENV::CHROMA_DOMAIN\n\n[usr_cert]\nsubjectAltName = DNS:$ENV::CHROMA_DOMAIN\n```\n</code></pre> Certificate Domain - CHROMA_DOMAIN <p>You can set the <code>CHROMA_DOMAIN</code> environment variable to the domain you want to use for the certificate. </p> OpenSSLDocker <p>To run the following you will need to have <code>openssl</code> installed on your system.</p> <pre><code>export CHROMA_DOMAIN=${CHROMA_DOMAIN:-\"localhost\"}\nopenssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 \\\n  -keyout certs/serverkey.pem \\\n  -subj '/O=Chroma/C=US' \\\n  -out certs/servercert.pem \\\n  -config openssl.cnf\n</code></pre> <p>This will create a self-signed certificate and key in the <code>certs</code> directory.</p> <p>If you are using Docker, you can use the following command to generate the certificates:</p> <pre><code>docker run --rm -v $(pwd)/certs:/certs \\\n  -v $(pwd)/openssl.cnf:/etc/ssl/openssl.cnf \\\n  -e CHROMA_DOMAIN=localhost \\\n  openquantumsafe/openssl3 \\\n  openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 \\\n  -keyout /certs/serverkey.pem \\\n  -subj '/O=Chroma/C=US' \\\n  -out /certs/servercert.pem \\\n  -config /etc/ssl/openssl.cnf\n</code></pre> Security Warning <p>Self-signed certificates are not recommended for production use. They are only suitable for testing and development purposes. Additionally in the above example the keyfile is not password protected, which is also not recommended for production use.</p>"},{"location":"security/chroma-ssl-cert/#configuring-and-running-chroma","title":"Configuring and running Chroma","text":"<p>You can run Chroma with the SSL/TLS certificate generate above or any other certificate you have.</p> DockerDocker Compose <p>To run Chroma with the self-signed certificate, you can use the following command:</p> <pre><code>docker run --rm -it -p 8000:8000 \\\n  -v $(pwd)/certs:/chroma/certs \\\n  chromadb/chroma:0.5.0 \\\n  --workers 1 \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --proxy-headers \\\n  --log-config chromadb/log_config.yml \\\n  --timeout-keep-alive 30 \\\n  --ssl-keyfile /chroma/certs/serverkey.pem \\\n  --ssl-certfile /chroma/certs/servercert.pem\n</code></pre> <p>To run Chroma with the self-signed certificate using Docker Compose, you can use the following <code>docker-compose.yml</code> file:</p> <pre><code>version: '3.9'\n\nnetworks:\n  net:\n    driver: bridge\n\nservices:\n  server:\n    image: chromadb/chroma:0.6.3\n    volumes:\n      # Be aware that indexed data are located in \"/chroma/chroma/\"\n      # Default configuration for persist_directory in chromadb/config.py\n      # Read more about deployments: https://docs.trychroma.com/deployment\n      - chroma-data:/chroma/chroma\n    command: \"--workers 1 --host 0.0.0.0 --port 8000 --proxy-headers --log-config chromadb/log_config.yml --timeout-keep-alive 30 --ssl-keyfile /chroma/certs/serverkey.pem --ssl-certfile /chroma/certs/servercert.pem\"\n    environment:\n      - IS_PERSISTENT=TRUE\n      - CHROMA_SERVER_AUTHN_PROVIDER=${CHROMA_SERVER_AUTHN_PROVIDER}\n      - CHROMA_SERVER_AUTHN_CREDENTIALS_FILE=${CHROMA_SERVER_AUTHN_CREDENTIALS_FILE}\n      - CHROMA_SERVER_AUTHN_CREDENTIALS=${CHROMA_SERVER_AUTHN_CREDENTIALS}\n      - CHROMA_AUTH_TOKEN_TRANSPORT_HEADER=${CHROMA_AUTH_TOKEN_TRANSPORT_HEADER}\n      - PERSIST_DIRECTORY=${PERSIST_DIRECTORY:-/chroma/chroma}\n      - CHROMA_OTEL_EXPORTER_ENDPOINT=${CHROMA_OTEL_EXPORTER_ENDPOINT}\n      - CHROMA_OTEL_EXPORTER_HEADERS=${CHROMA_OTEL_EXPORTER_HEADERS}\n      - CHROMA_OTEL_SERVICE_NAME=${CHROMA_OTEL_SERVICE_NAME}\n      - CHROMA_OTEL_GRANULARITY=${CHROMA_OTEL_GRANULARITY}\n      - CHROMA_SERVER_NOFILE=${CHROMA_SERVER_NOFILE}\n    restart: unless-stopped\n    ports:\n      - \"8000:8000\"\n    healthcheck:\n      # Adjust below to match your container port\n      test: [ \"CMD\", \"curl\", \"-f\", \"http://localhost:8000/api/v1/heartbeat\" ]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n    networks:\n      - net\n\nvolumes:\n  chroma-data:\n    driver: local\n</code></pre>"},{"location":"security/chroma-ssl-cert/#using-a-certificate-authority","title":"Using a Certificate Authority","text":"<p>Examples below will demonstrate how to use <code>certbot</code> to generate a certificate with a given certificate authority.</p>"},{"location":"security/chroma-ssl-cert/#lets-encrypt","title":"Let's Encrypt","text":"<p>Coming soon!</p>"},{"location":"security/chroma-ssl-cert/#aws-certificate-manager","title":"AWS Certificate Manager","text":"<p>Coming soon!</p>"},{"location":"security/legacy-auth/","title":"Chroma-native Auth (Legacy)","text":"<p>Chroma-native Auth is not supported in v1.0.x</p> <p>Chroma native-auth described in this article is not supported in Chroma versions 1.0.0-1.0.10 (latest as of time of writing). DO NOT USE the below if you are on any of the affected version as it will not secure your instance.</p> <p>Chroma offers built in authentication and authorization mechanisms to secure your Chroma instance.</p> <p>Auth Disabled by Default</p> <p>By default, Chroma does not require authentication. You must enable it manually. If you are deploying Chroma in a public-facing  environment, it is highly recommended to enable authentication.</p> <p>Auth needs the company of SSL/TLS</p> <p>Authentication without encryption is insecure. If you are deploying Chroma in a public-facing environment, it is highly recommended that you add SSL/TLS.</p>"},{"location":"security/legacy-auth/#authentication","title":"Authentication","text":"<p>Chroma supports two types of authentication:</p> <ul> <li>Basic Auth - RFC 7617 compliant pre-emptive authentication with username and password credentials in Authorization header.</li> <li>Token Auth - Standard token-based auth with <code>Authorization</code> or <code>X-Chroma-Token</code> headers.</li> </ul> <p>For each authentication method there are configurations in both client and server.</p>"},{"location":"security/legacy-auth/#basic-authentication","title":"Basic Authentication","text":"<p>Server</p> <p>Generate a password file with bcrypt hashed password:</p> <pre><code>docker run --rm --entrypoint htpasswd httpd:2 -Bbn admin password123 &gt;&gt; server.htpasswd\n</code></pre> <p>Verify the password file:</p> <pre><code>docker run --rm -v ./server.htpasswd:/server.htpasswd --entrypoint htpasswd httpd:2 -vb /server.htpasswd admin password123\n</code></pre> Multiple users <p>Chroma supports multiple users in the htpasswd file. You can add multiple users by running the command multiple  times WITHOUT <code>-c</code> flag.</p> Frequently encountered Chroma errors <p>If you see the following error:</p> <pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n</code></pre> <p>It is likely that you have not used the <code>-B</code> (bcrypt) flag when creating the password file.</p> <p>Environment variables:</p> <pre><code>export CHROMA_SERVER_AUTHN_CREDENTIALS_FILE=\"server.htpasswd\"\nexport CHROMA_SERVER_AUTHN_PROVIDER=\"chromadb.auth.basic_authn.BasicAuthenticationServerProvider\"\n</code></pre> <p>Running the server:</p> CLIDockerDocker Compose <pre><code>export CHROMA_SERVER_AUTHN_CREDENTIALS_FILE=\"server.htpasswd\"\nexport CHROMA_SERVER_AUTHN_PROVIDER=\"chromadb.auth.basic_authn.BasicAuthenticationServerProvider\"\nchroma run --path /chroma-data\n</code></pre> <pre><code>docker run --rm -v ./server.htpasswd:/chroma/server.htpasswd \\\n -e CHROMA_SERVER_AUTHN_CREDENTIALS_FILE=\"server.htpasswd\" \\\n -e CHROMA_SERVER_AUTHN_PROVIDER=\"chromadb.auth.basic_authn.BasicAuthenticationServerProvider\" \\\n -p 8000:8000 \\\n chromadb/chroma:latest\n</code></pre> <p>Create a <code>docker-compose.yaml</code> with the following content:</p> <pre><code>networks:\n  net:\n    driver: bridge\nservices:\n  chromadb:\n    image: chromadb/chroma:latest\n    volumes:\n      - ./chromadb:/chroma/chroma\n      - ./server.htpasswd:/chroma/server.htpasswd\n    environment:\n      - IS_PERSISTENT=TRUE\n      - PERSIST_DIRECTORY=/chroma/chroma # this is the default path, change it as needed\n      - ANONYMIZED_TELEMETRY=${ANONYMIZED_TELEMETRY:-TRUE}\n      - CHROMA_SERVER_AUTHN_CREDENTIALS_FILE=server.htpasswd\n      - CHROMA_SERVER_AUTHN_PROVIDER=chromadb.auth.basic_authn.BasicAuthenticationServerProvider\n    ports:\n      - 8000:8000\n    networks:\n      - net\n</code></pre> <p>Run the following command to start the Chroma server:</p> <pre><code>docker compose -f docker-compose.yaml up -d\n</code></pre> Is my config right? <p>If you have correctly configured the server you should see the following line in the server logs:</p> <pre><code>Starting component BasicAuthenticationServerProvider\n</code></pre> <p>Client</p> Python SyncPython AsyncJSGoJava <pre><code>import chromadb\nfrom chromadb.config import Settings\n\nclient = chromadb.HttpClient(\n  settings=Settings(\n      chroma_client_auth_provider=\"chromadb.auth.basic_authn.BasicAuthClientProvider\",\n      chroma_client_auth_credentials=\"admin:admin\")\n)\n\n# if everything is correctly configured the below should list all collections\nclient.list_collections()\n</code></pre> <pre><code>import chromadb\nimport base64\n\nbase64_credentials = base64.b64encode(b\"admin:admin\").decode(\"utf-8\")\n\nclient = await chromadb.AsyncHttpClient(headers={\"Authorization\": f\"Basic {base64_credentials}\"})\n</code></pre> <pre><code>// const {ChromaClient} = require(\"chromadb\"); // CommonJS\nimport { ChromaClient } from \"chromadb\"; // ES Modules\nconst client = new ChromaClient({\n    url: \"http://localhost:8000\",\n    auth: {\n        provider: \"basic\",\n        credentials: \"admin:admin\",\n    }\n});\n</code></pre> <pre><code>package main\n\nimport (\n    \"context\"\n    \"log\"\n    chroma \"github.com/amikos-tech/chroma-go\"\n  \"github.com/amikos-tech/chroma-go/types\"\n)\n\nfunc main() {\n    client, err := chroma.NewClient(\n        chroma.WithBasePath(\"http://localhost:8000\"),\n        chroma.WithAuth(types.NewBasicAuthCredentialsProvider(\"admin\", \"admin\")),\n    )\n    if err != nil {\n        log.Fatalf(\"Error creating client: %s \\n\", err)\n    }\n    _, err = client.ListCollections(context.TODO())\n    if err != nil {\n        log.Fatalf(\"Error calling ListCollections: %s \\n\", err)\n    }\n}\n</code></pre> <p>The below example shows auth with just headers. A more robust authentication mechanism is being implemented.</p> <pre><code>package tech.amikos;\n\nimport tech.amikos.chromadb.*;\nimport tech.amikos.chromadb.Collection;\n\nimport java.util.*;\n\npublic class Main {\n    public static void main(String[] args) {\n        try {\n            Client client = new Client(System.getenv(\"http://localhost:8000\"));\n            client.setDefaultHeaders(new HashMap&lt;&gt;() {{\n                put(\"Authorization\", \"Basic \" + Base64.getEncoder().encodeToString(\"admin:admin\".getBytes()));\n            }});\n            // your code here\n        } catch (Exception e) {\n            System.out.println(e);\n        }\n    }\n}\n</code></pre> Testing with cURL <pre><code>curl -v http://localhost:8000/api/v1/collections -u user1:change_this_password\n</code></pre>"},{"location":"security/legacy-auth/#token-authentication","title":"Token Authentication","text":"<p>Server</p> <p>Environment variables:</p> <pre><code>export CHROMA_SERVER_AUTHN_CREDENTIALS=\"chr0ma-t0k3n\"\nexport CHROMA_SERVER_AUTHN_PROVIDER=\"chromadb.auth.token_authn.TokenAuthenticationServerProvider\"\nexport CHROMA_AUTH_TOKEN_TRANSPORT_HEADER=\"Authorization\" # or X-Chroma-Token\n</code></pre> <p>Auth Headers</p> <p>Chroma supports two token transport headers:</p> <ul> <li><code>Authorization</code> (default) - the clients are expected to pass <code>Authorization: Bearer &lt;token&gt;</code> header</li> <li><code>X-Chroma-Token</code> - the clients are expected to pass <code>X-Chroma-Token: &lt;token&gt;</code> header</li> </ul> <p>The header can be configured via <code>CHROMA_AUTH_TOKEN_TRANSPORT_HEADER</code> environment variable.</p> <p>Running the server:</p> CLIDockerDocker Compose <pre><code>export CHROMA_SERVER_AUTHN_CREDENTIALS=\"chr0ma-t0k3n\"\nexport CHROMA_SERVER_AUTHN_PROVIDER=\"chromadb.auth.token_authn.TokenAuthenticationServerProvider\"\nexport CHROMA_AUTH_TOKEN_TRANSPORT_HEADER=\"Authorization\"\nchroma run --path /chroma-data\n</code></pre> <pre><code>docker run --rm -e CHROMA_SERVER_AUTHN_CREDENTIALS=\"chr0ma-t0k3n\" \\\n -e CHROMA_SERVER_AUTHN_PROVIDER=\"chromadb.auth.token_authn.TokenAuthenticationServerProvider\" \\\n -e CHROMA_AUTH_TOKEN_TRANSPORT_HEADER=\"Authorization\" \\\n -p 8000:8000 \\\n chromadb/chroma:latest\n</code></pre> <p>Create a <code>docker-compose.yaml</code> with the following content:</p> <pre><code>networks:\n  net:\n    driver: bridge\nservices:\n  chromadb:\n    image: chromadb/chroma:latest\n    volumes:\n      - ./chromadb:/chroma/chroma\n    environment:\n      - IS_PERSISTENT=TRUE\n      - PERSIST_DIRECTORY=/chroma/chroma # this is the default path, change it as needed\n      - ANONYMIZED_TELEMETRY=${ANONYMIZED_TELEMETRY:-TRUE}\n      - CHROMA_SERVER_AUTHN_CREDENTIALS=\"chr0ma-t0k3n\"\n      - CHROMA_AUTH_TOKEN_TRANSPORT_HEADER=\"Authorization\"\n      - CHROMA_SERVER_AUTHN_PROVIDER=chromadb.auth.token_authn.TokenAuthenticationServerProvider\n    ports:\n      - 8000:8000\n    networks:\n      - net\n</code></pre> <p>Run the following command to start the Chroma server:</p> <pre><code>docker compose -f docker-compose.yaml up -d\n</code></pre> Is my config right? <p>If you have correctly configured the server you should see the following line in the server logs:</p> <pre><code>Starting component TokenAuthenticationServerProvider\n</code></pre> <p>Client</p> Python SyncPython AsyncJSGoJava <pre><code>import chromadb\nfrom chromadb.config import Settings\n\nclient = chromadb.HttpClient(\n  settings=Settings(\n      chroma_client_auth_provider=\"chromadb.auth.token_authn.TokenAuthClientProvider\",\n      chroma_client_auth_credentials=\"chr0ma-t0k3n\",\n      chroma_auth_token_transport_header=\"Authorization\"\n  )\n)\n\n# if everything is correctly configured the below should list all collections\nclient.list_collections()\n</code></pre> <pre><code>import chromadb\n# for Authorization header\nclient = await chromadb.AsyncHttpClient(headers={\"Authorization\": \"Bearer chr0ma-t0k3n\"})\n# for X-Chroma-Token header\nclient = await chromadb.AsyncHttpClient(headers={\"X-Chroma-Token\": \"chr0ma-t0k3n\"})\n\n# if everything is correctly configured the below should list all collections\nawait client.list_collections()\n</code></pre> <pre><code>// const {ChromaClient} = require(\"chromadb\"); // CommonJS\nimport { ChromaClient } from \"chromadb\"; // ES Modules\nconst client = new ChromaClient({\n    url: \"http://localhost:8000\",\n    auth: {\n        provider: \"token\",\n        credentials: \"chr0ma-t0k3n\",\n    }\n});\n</code></pre> <pre><code>package main\n\nimport (\n    \"context\"\n    \"log\"\n    chroma \"github.com/amikos-tech/chroma-go\"\n    \"github.com/amikos-tech/chroma-go/types\"\n)\n\nfunc main() {\n    client, err := chroma.NewClient(\n        chroma.WithBasePath(\"http://localhost:8000\"), \n        chroma.WithAuth(types.NewTokenAuthCredentialsProvider(\"chr0ma-t0k3n\", types.AuthorizationTokenHeader)),\n    )\n    if err != nil {\n        log.Fatalf(\"Error creating client: %s \\n\", err)\n    }\n    _, err = client.ListCollections(context.TODO())\n    if err != nil {\n        log.Fatalf(\"Error calling ListCollections: %s \\n\", err)\n    }\n}\n</code></pre> <p>The example below shows authorization with just headers. A more robust auth mechanism is under implementation.</p> <pre><code>package tech.amikos;\n\nimport tech.amikos.chromadb.*;\nimport tech.amikos.chromadb.Collection;\n\nimport java.util.*;\n\npublic class Main {\n    public static void main(String[] args) {\n        try {\n            Client client = new Client(System.getenv(\"http://localhost:8000\"));\n            client.setDefaultHeaders(new HashMap&lt;&gt;() {{\n                put(\"Authorization\", \"Bearer chr0ma-t0k3n\");\n            }});\n            // your code here\n        } catch (Exception e) {\n            System.out.println(e);\n        }\n    }\n}\n</code></pre> Testing with cURL <pre><code>curl -v http://localhost:8000/api/v1/collections -H \"Authorization: Bearer chr0ma-t0k3n\"\n</code></pre>"},{"location":"security/legacy-auth/#authorization","title":"Authorization","text":"<p>Coming soon!</p>"},{"location":"security/ssl-proxies/","title":"SSL/TLS Proxy","text":"<p>In this section we'll explore how to secure Chroma with a TLS-terminated HTTPS proxy. Below we'll give two examples of how to do this using Envoy and Nginx. The certificates are self-signed and generated using OpenSSL, but in the future we'll also provide examples of how to achieve this with Let's Encrypt and certbot.</p>"},{"location":"security/ssl-proxies/#getting-the-cert","title":"Getting The cert","text":"<p>To manually generate a certificate follow the steps here.</p>"},{"location":"security/ssl-proxies/#envoy","title":"Envoy","text":"<p>The following envoy configuration will create a listener on port 443 that will forward all requests to the <code>chromadb</code>.</p> <pre><code>static_resources:\n  listeners:\n    - name: listener_0\n      address:\n        socket_address:\n          address: 0.0.0.0\n          port_value: 443\n      filter_chains:\n        - filters:\n            - name: envoy.filters.network.http_connection_manager\n              typed_config:\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\n                stat_prefix: ingress_http\n                route_config:\n                  name: chroma_route\n                  virtual_hosts:\n                    - name: local_chromadb\n                      domains: [ \"*\" ]\n                      routes:\n                        - match:\n                            prefix: \"/\"\n                          route:\n                            cluster: chromadb_service\n                            prefix_rewrite: \"/\"\n                http_filters:\n                  - name: envoy.filters.http.router\n                    typed_config:\n                      \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\n          transport_socket:\n            name: envoy.transport_sockets.tls\n            typed_config:\n              \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\n              common_tls_context:\n                tls_certificates:\n                  - certificate_chain:\n                      filename: \"/etc/envoy/certs/servercert.pem\"\n                    private_key:\n                      filename: \"/etc/envoy/certs/serverkey.pem\"\n  clusters:\n    - name: chromadb_service\n      connect_timeout: 0.25s\n      type: LOGICAL_DNS\n      lb_policy: ROUND_ROBIN\n      load_assignment:\n        cluster_name: chromadb_service\n        endpoints:\n          - lb_endpoints:\n              - endpoint:\n                  address:\n                    socket_address:\n                      address: chromadb\n                      port_value: 8000\n</code></pre> <p>Finally the docker compose to tie things up where we have added a <code>cert-gen</code> step to automatically generate the certificates, prior to starting the <code>envoy</code> and <code>chromadb</code> services.</p> <pre><code>version: '3'\nnetworks:\n  net:\n    driver: bridge\nservices:\n  cert-gen:\n    image: openquantumsafe/openssl3\n    volumes:\n      - ./certs:/certs\n      - ./openssl.cnf:/etc/ssl/openssl.cnf\n    command: |\n      sh -c \"[ -f /certs/servercert.pem ] || \\\n      openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 -keyout /certs/serverkey.pem -out /certs/servercert.pem -subj '/O=Chroma/C=US' -config /etc/ssl/openssl.cnf\"\n    environment:\n      - CHROMA_DOMAIN=${CHROMA_DOMAIN:-localhost}\n  envoy:\n    image: bitnami/envoy\n    volumes:\n      - ./envoy.yaml:/opt/bitnami/envoy/conf/envoy.yaml\n      - ./certs:/etc/envoy/certs\n      - ./wait-for-certs.sh:/usr/local/bin/wait-for-certs.sh\n    ports:\n      - \"443:443\"\n    networks:\n      - net\n    depends_on:\n      cert-gen:\n        condition: service_completed_successfully\n      chromadb:\n        condition: service_healthy\n    entrypoint: |\n      sh -c \"/usr/local/bin/wait-for-certs.sh &amp;&amp; \\\n      /opt/bitnami/envoy/bin/envoy -c /opt/bitnami/envoy/conf/envoy.yaml\"\n  chromadb:\n    image: chromadb/chroma:0.6.3\n    volumes:\n      - ./chromadb:/chroma/chroma\n    environment:\n      - IS_PERSISTENT=TRUE\n      - ANONYMIZED_TELEMETRY=${ANONYMIZED_TELEMETRY:-TRUE}\n    networks:\n      - net\n    healthcheck:\n      # Adjust below to match your container port\n      test: [ \"CMD\", \"curl\", \"-f\", \"http://localhost:8000/api/v1/heartbeat\" ]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n</code></pre>"},{"location":"security/ssl-proxies/#nginx","title":"Nginx","text":"<p>Use the following Nginx config (<code>nginx.conf</code>) as a starting point and build from there:</p> <pre><code>server {\n    listen 443 ssl;\n    server_name localhost;\n\n    ssl_certificate /etc/nginx/certs/servercert.pem;\n    ssl_certificate_key /etc/nginx/certs/serverkey.pem;\n\n    location / {\n        proxy_pass http://chromadb:8000;\n        proxy_set_header Host $host;\n        proxy_http_version 1.1;  # Use HTTP/1.1\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre> <p>Create a <code>docker-compose.yml</code> file with the following content:</p> <p>Config Files</p> <p>For the following <code>docker-compose.yaml</code> to operate successfully <code>openssl.cnf</code> and <code>nginx.conf</code> files need to be present in the same directory.</p> <pre><code>version: '3'\nnetworks:\n  net:\n    driver: bridge\nservices:\n  cert-gen:\n    image: openquantumsafe/openssl3\n    volumes:\n      - ./certs:/certs\n      - ./openssl.cnf:/etc/ssl/openssl.cnf\n    command: |\n      sh -c \"[ -f /certs/servercert.pem ] || \\\n      openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 -keyout /certs/serverkey.pem -out /certs/servercert.pem -subj '/O=Chroma/C=US' -config /etc/ssl/openssl.cnf\"\n    environment:\n      - CHROMA_DOMAIN=${CHROMA_DOMAIN:-localhost}\n  chromadb:\n    image: chromadb/chroma:0.6.3\n    volumes:\n      - ./chromadb:/chroma/chroma\n    environment:\n      - IS_PERSISTENT=TRUE\n      - ANONYMIZED_TELEMETRY=${ANONYMIZED_TELEMETRY:-TRUE}\n    ports:\n      - \"8000:8000\"\n    healthcheck:\n      # Adjust below to match your container port\n      test: [ \"CMD\", \"curl\", \"-f\", \"http://localhost:8000/api/v1/heartbeat\" ]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n    networks:\n      - net\n  nginx:\n    image: nginx:latest\n    depends_on:\n      - cert-gen\n      - chromadb\n    ports:\n      - \"443:443\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/conf.d/default.conf\n      - ./certs:/etc/nginx/certs\n    networks:\n      - net\n    depends_on:\n      chromadb:\n        condition: service_healthy\n</code></pre>"},{"location":"strategies/backup/","title":"ChromaDB Backups","text":"<p>Depending on your use case there are a few different ways to back up your ChromaDB data.</p> <ul> <li>API export - this approach is relatively simple, slow for large datasets and may result in a backup that is missing   some updates, should your data change frequently.</li> <li>Disk snapshot - this approach is fast, but is highly dependent on the underlying storage. Should your cloud provider   and underlying volume support snapshots, this is a good option.</li> <li>Filesystem backup - this approach is also fast, but requires stopping your Chroma container to avoid data corruption.   This is a good option if you can afford to stop your Chroma container for a few minutes.</li> </ul> <p>Other Options</p> <p>Have another option in mind, feel free to add it to the above list.</p>"},{"location":"strategies/backup/#api-export","title":"API Export","text":""},{"location":"strategies/backup/#with-chroma-datapipes","title":"With Chroma Datapipes","text":"<p>One way to export via the API is to use Tooling like Chroma Data Pipes. Chroma Data Pipes is a command-line tool that provides a simple way import/export/transform ChromaDB data.</p> <p>Exporting from local filesystem:</p> <pre><code>cdp export \"file:///absolute/path/to/chroma-data/my-collection-name\" &gt; my_chroma_data.jsonl\n</code></pre> <p>Exporting from remote server:</p> <pre><code>cdp export \"http://remote-chroma-server:8000/my-collection-name\" &gt; my_chroma_data.jsonl\n</code></pre> <p>Get Help</p> <p>Read more about Chroma Data Pipes here</p>"},{"location":"strategies/backup/#disk-snapshot","title":"Disk Snapshot","text":"<p>TBD</p>"},{"location":"strategies/backup/#filesystem-backup","title":"Filesystem Backup","text":""},{"location":"strategies/backup/#from-docker-container","title":"From Docker Container","text":"<p>Sometimes you have been running Chroma in a Docker container without a host mount, intentionally or unintentionally. So all your data is now stored in the container's filesystem. Here's how you can back up your data:</p> <ol> <li>Stop the container:</li> </ol> <pre><code>docker stop &lt;chroma-container-id/name&gt;\n</code></pre> <ol> <li>Create a backup of the container's filesystem:</li> </ol> <pre><code>docker cp &lt;chroma-container-id/name&gt;:/chroma/chroma /path/to/backup\n</code></pre> <p><code>/path/to/backup</code> is the directory where you want to store the backup on your host machine.</p>"},{"location":"strategies/batching/","title":"Batching","text":"<p>It is often that you may need to ingest a large number of documents into Chroma. The problem you may face is related to the underlying SQLite version of the machine running Chroma which imposes a maximum number of statements and parameters which Chroma translates into a batchable record size, exposed via the <code>max_batch_size</code> parameter of the <code>ChromaClient</code> class.</p> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient(path=\"test\")\nprint(\"Number of documents that can be inserted at once: \",client.max_batch_size)\n</code></pre>"},{"location":"strategies/batching/#creating-batches","title":"Creating Batches","text":"<p>Due to consistency and data integrity reasons, Chroma does not offer, yet, out-of-the-box batching support. The below code snippet shows how to create batches of documents and ingest them into Chroma.</p> <pre><code>import chromadb\nfrom chromadb.utils.batch_utils import create_batches\nimport uuid\n\nclient = chromadb.PersistentClient(path=\"test-large-batch\")\nlarge_batch = [(f\"{uuid.uuid4()}\", f\"document {i}\", [0.1] * 1536) for i in range(100000)]\nids, documents, embeddings = zip(*large_batch)\nbatches = create_batches(api=client,ids=list(ids), documents=list(documents), embeddings=list(embeddings))\ncollection = client.get_or_create_collection(\"test\")\nfor batch in batches:\n    print(f\"Adding batch of size {len(batch[0])}\")\n    collection.add(ids=batch[0],\n                   documents=batch[3],\n                   embeddings=batch[1],\n                   metadatas=batch[2])\n</code></pre>"},{"location":"strategies/cors/","title":"CORS Configuration for Browser-Based Access","text":"<p>Chroma JS package allows you to use Chroma in your browser-based SPA application. This is great, but that means that you'll need to configure Chroma to work with your browser to avoid CORS issues.</p>"},{"location":"strategies/cors/#setting-up-chroma-for-browser-based-access","title":"Setting up Chroma for Browser-Based Access","text":""},{"location":"strategies/cors/#chroma-10-or-later","title":"Chroma 1.0 or later","text":"<p>To allow browsers to directly access your Chroma instance you'll need to configure the <code>CHROMA_CORS_ALLOW_ORIGINS</code>. The <code>CHROMA_CORS_ALLOW_ORIGINS</code> environment variable controls the hosts which are allowed to access your Chroma instance.</p> <p>Note</p> <p>The <code>CHROMA_CORS_ALLOW_ORIGINS</code> environment variable is a list of strings. Each string is a URL that is allowed to access your Chroma instance. If you want to allow all hosts to access your Chroma instance, you can set <code>CHROMA_CORS_ALLOW_ORIGINS</code> to <code>[\"*\"]</code>. This is not recommended for production environments.</p> CLIDockerDocker Compose <pre><code>export CHROMA_CORS_ALLOW_ORIGINS='[\"http://localhost:3000\"]'\nchroma run --path /path/to/chroma-data\n</code></pre> <p>Verify with <code>curl -i -X GET http://localhost:8000/api/v2/version -H \"Origin: http://localhost:3000\"</code> in the response you should see <code>access-control-allow-origin: http://localhost:3000</code> being returned if all works fine</p> <pre><code>docker run -e CHROMA_CORS_ALLOW_ORIGINS='[\"http://localhost:3000\"]' -p 8000:8000 chromadb/chroma:1.5.0\n</code></pre> <p>Verify with <code>curl -i -X GET http://localhost:8000/api/v2/version -H \"Origin: http://localhost:3000\"</code> in the response you should see <code>access-control-allow-origin: http://localhost:3000</code> being returned if all works fine</p> <pre><code>version: '3.9'\n\nnetworks:\n  net:\n    driver: bridge\n\nservices:\n  server:\n    image: chromadb/chroma:1.5.0\n    volumes:\n      # Be aware that indexed data are located in \"/data/\"\n      - chroma-data:/data\n    environment:\n      - CHROMA_CORS_ALLOW_ORIGINS=[\"http://localhost:3000\"]\n    restart: unless-stopped # possible values are: \"no\", always\", \"on-failure\", \"unless-stopped\"\n    ports:\n      - \"8000:8000\"\n    healthcheck:\n      # Adjust below to match your container port\n      test: [ \"CMD\", \"curl\", \"-f\", \"http://localhost:8000/api/v2/heartbeat\" ]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n    networks:\n      - net\n\nvolumes:\n  chroma-data:\n    driver: local\n</code></pre> <p>Run <code>docker compose up</code> to start your Chroma instance. Verify with <code>curl -i -X GET http://localhost:8000/api/v2/version -H \"Origin: http://localhost:3000\"</code> in the response you should see <code>access-control-allow-origin: http://localhost:3000</code> being returned if all works fine</p>"},{"location":"strategies/cors/#chroma-pre-10-legacy","title":"Chroma Pre-1.0 (Legacy)","text":"<p>To allow browsers to directly access your Chroma instance you'll need to configure the <code>CHROMA_SERVER_CORS_ALLOW_ORIGINS</code>. The <code>CHROMA_SERVER_CORS_ALLOW_ORIGINS</code> environment variable controls the hosts which are allowed to access your Chroma instance.</p> <p>Note</p> <p>The <code>CHROMA_SERVER_CORS_ALLOW_ORIGINS</code> environment variable is a list of strings. Each string is a URL that is allowed to access your Chroma instance. If you want to allow all hosts to access your Chroma instance, you can set <code>CHROMA_SERVER_CORS_ALLOW_ORIGINS</code> to <code>[\"*\"]</code>. This is not recommended for production environments.</p> <p>The below examples assume that your web app is running on <code>http://localhost:3000</code>. You can find an example of NextJS and Langchain here.</p> CLIDockerDocker Compose <pre><code>export CHROMA_SERVER_CORS_ALLOW_ORIGINS='[\"http://localhost:3000\"]'\nchroma run --path /path/to/chroma-data\n</code></pre> <pre><code>docker run -e CHROMA_SERVER_CORS_ALLOW_ORIGINS='[\"http://localhost:3000\"]' -v /path/to/chroma-data:/chroma/chroma -p 8000:8000 chromadb/chroma:0.6.3\n</code></pre> <pre><code>version: '3.9'\n\nnetworks:\n  net:\n    driver: bridge\n\nservices:\n  server:\n    image: chromadb/chroma:0.6.3\n    volumes:\n      # Be aware that indexed data are located in \"/chroma/chroma/\"\n      # Default configuration for persist_directory in chromadb/config.py\n      # Read more about deployments: https://docs.trychroma.com/deployment\n      - chroma-data:/chroma/chroma\n    command: \"--workers 1 --host 0.0.0.0 --port 8000 --proxy-headers --log-config chromadb/log_config.yml --timeout-keep-alive 30\"\n    environment:\n      - IS_PERSISTENT=TRUE\n      - CHROMA_SERVER_AUTH_PROVIDER=${CHROMA_SERVER_AUTH_PROVIDER}\n      - CHROMA_SERVER_AUTHN_CREDENTIALS_FILE=${CHROMA_SERVER_AUTHN_CREDENTIALS_FILE}\n      - CHROMA_SERVER_AUTHN_CREDENTIALS=${CHROMA_SERVER_AUTHN_CREDENTIALS}\n      - CHROMA_AUTH_TOKEN_TRANSPORT_HEADER=${CHROMA_AUTH_TOKEN_TRANSPORT_HEADER}\n      - PERSIST_DIRECTORY=${PERSIST_DIRECTORY:-/chroma/chroma}\n      - CHROMA_OTEL_EXPORTER_ENDPOINT=${CHROMA_OTEL_EXPORTER_ENDPOINT}\n      - CHROMA_OTEL_EXPORTER_HEADERS=${CHROMA_OTEL_EXPORTER_HEADERS}\n      - CHROMA_OTEL_SERVICE_NAME=${CHROMA_OTEL_SERVICE_NAME}\n      - CHROMA_OTEL_GRANULARITY=${CHROMA_OTEL_GRANULARITY}\n      - CHROMA_SERVER_NOFILE=${CHROMA_SERVER_NOFILE}\n      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=[\"http://localhost:3000\"]\n    restart: unless-stopped # possible values are: \"no\", always\", \"on-failure\", \"unless-stopped\"\n    ports:\n      - \"8000:8000\"\n    healthcheck:\n      # Adjust below to match your container port\n      test: [ \"CMD\", \"curl\", \"-f\", \"http://localhost:8000/api/v1/heartbeat\" ]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n    networks:\n      - net\n\nvolumes:\n  chroma-data:\n    driver: local\n</code></pre> <p>Run <code>docker compose up</code> to start your Chroma instance.</p>"},{"location":"strategies/keyword-search/","title":"Keyword Search","text":"<p>Chroma uses SQLite for storing metadata and documents. Additionally documents are indexed using SQLite FTS5 for fast text search.</p> PythonJS/TS <pre><code>import chromadb\nfrom chromadb.config import Settings\n\nclient = chromadb.PersistentClient(path=\"test\", settings=Settings(allow_reset=True))\n\nclient.reset()\ncol = client.get_or_create_collection(\"test\")\n\ncol.upsert(ids=[\"1\", \"2\", \"3\"], documents=[\"He is a technology freak and he loves AI topics\", \"AI technology are advancing at a fast pace\", \"Innovation in LLMs is a hot topic\"],metadatas=[{\"author\": \"John Doe\"}, {\"author\": \"Jane Doe\"}, {\"author\": \"John Doe\"}])\ncol.query(query_texts=[\"technology\"], where_document={\"$or\":[{\"$contains\":\"technology\"}, {\"$contains\":\"freak\"}]})\n</code></pre> <p>The above should return:</p> <pre><code>{'ids': [['2', '1']],\n'distances': [[1.052205477809135, 1.3074231535113972]],\n'metadatas': [[{'author': 'Jane Doe'}, {'author': 'John Doe'}]],\n'embeddings': None,\n'documents': [['AI technology are advancing at a fast pace',\n  'He is a technology freak and he loves AI topics']],\n'uris': None,\n'data': None}\n</code></pre> <pre><code>const { ChromaClient, OpenAIEmbeddingFunction } = require(\"chromadb\");\n\n(async () =&gt; {\n    const client = new ChromaClient({\n        url: \"http://localhost:8000\",\n    });\n\n    const collection = client.getOrCreateCollection(\"test\");\n\n    await collection.upsert({\n        ids: [\"1\", \"2\", \"3\"],\n        documents: [\"He is a technology freak and he loves AI topics\", \"AI technology are advancing at a fast pace\", \"Innovation in LLMs is a hot topic\"],\n        metadatas: [{ author: \"John Doe\" }, { author: \"Jane Doe\" }, { author: \"John Doe\" }],\n    });\n\n    const results = await collection.query({\n        queryTexts: [\"technology\"],\n        whereDocument: {\n            \"$or\": [\n                { \"$contains\": \"technology\" },\n                { \"$contains\": \"freak\" }\n            ]\n        }\n    });\n})();\n</code></pre>"},{"location":"strategies/memory-management/","title":"Memory Management","text":"<p>This section provided additional info and strategies how to manage memory in Chroma.</p>"},{"location":"strategies/memory-management/#lru-cache-strategy","title":"LRU Cache Strategy","text":"<p>Out of the box Chroma offers an LRU cache strategy which unloads segments (collections) that are not used while trying to abide to the configured memory usage limits.</p> <p>To enable the LRU cache the following two settings parameters or environment variables need to be set:</p> PythonEnvironment Variables <pre><code>from chromadb.config import Settings\n\nsettings = Settings(\n    chroma_segment_cache_policy=\"LRU\",\n    chroma_memory_limit_bytes=10000000000  # ~10GB\n)\n</code></pre> <pre><code>export CHROMA_SEGMENT_CACHE_POLICY=LRU\nexport CHROMA_MEMORY_LIMIT_BYTES=10000000000  # ~10GB\n</code></pre>"},{"location":"strategies/memory-management/#manualcustom-collection-unloading","title":"Manual/Custom Collection Unloading","text":"<p>Local Clients</p> <p>The below code snippets assume you are working with a <code>PersistentClient</code> or an <code>EphemeralClient</code> instance.</p> <p>At the time of writing (Chroma v0.6.3), Chroma does not allow you to manually unloading of collections from memory.</p> <p>Here we provide a simple utility function to help users unload collections from memory.</p> <p>Internal APIs</p> <p>The below code relies on internal APIs and may change in future versions of Chroma.  The function relies on Chroma internal APIs which may change. The below snippet has been tested with Chroma <code>0.4.24+</code>.</p> <pre><code>import gc\nimport os\n\nimport chromadb\nimport psutil\nfrom chromadb.types import SegmentScope\n\n\ndef bytes_to_gb(bytes_value):\n    return bytes_value / (1024 ** 3)\n\n\ndef get_process_info():\n    pid = os.getpid()\n    p = psutil.Process(pid)\n    with p.oneshot():\n        mem_info = p.memory_info()\n        # disk_io = p.io_counters()\n    return {\n        \"memory_usage\": bytes_to_gb(mem_info.rss),\n    }\n\n\ndef unload_index(collection_name: str, chroma_client: chromadb.PersistentClient):\n    \"\"\"\n    Unloads binary hnsw index from memory and removes both segments (binary and metadata) from the segment cache.\n    \"\"\"\n    collection = chroma_client.get_collection(collection_name)\n    collection_id = collection.id\n    segment_manager = chroma_client._server._manager\n    for scope in [SegmentScope.VECTOR, SegmentScope.METADATA]:\n        if scope in segment_manager.segment_cache:\n            cache = segment_manager.segment_cache[scope].cache\n            if collection_id in cache:\n                segment_manager.callback_cache_evict(cache[collection_id])\n    gc.collect()\n</code></pre> <p>Example Contributed</p> <p>The above example was enhanced and contributed by <code>Amir</code> (amdeilami) from our Discord comminity. We appreciate and encourage his work and contributions to the Chroma community.</p> Usage Example <pre><code>import chromadb\n\n\nclient = chromadb.PersistentClient(path=\"testds-1M/chroma-data\")\ncol=client.get_collection(\"test\")\nprint(col.count())\ncol.get(limit=1,include=[\"embeddings\"]) # force load the collection into memory\n\nunload_index(\"test\", client)\n</code></pre>"},{"location":"strategies/multi-category-filters/","title":"Multi-Category/Tag Filters","text":"<p>Sometimes you may want to filter documents in Chroma based on multiple categories or tags e.g. <code>games</code> and <code>movies</code>.</p>"},{"location":"strategies/multi-category-filters/#adding-categories","title":"Adding Categories","text":"Array Metadata (Chroma &gt;= 1.5.0)Boolean Fields (Pre-1.5.0) <p>Store categories directly as an array metadata field:</p> <pre><code>collection.add(\n    ids=[f\"{uuid.uuid4()}\"],\n    documents=[\"This is a document\"],\n    metadatas=[{\"categories\": [\"games\", \"movies\"]}],\n)\n</code></pre> <p>On older Chroma versions that don't support array metadata, add each category as a separate boolean field:</p> <p>No Empty Categories/Tags</p> <p>Only add categories an item belongs to with flags set to <code>True</code>. Do not add categories an item does not belong to and set the flag to <code>False</code>.</p> <pre><code>collection.add(\n    ids=[f\"{uuid.uuid4()}\"],\n    documents=[\"This is a document\"],\n    metadatas=[{\"games\": True, \"movies\": True}],\n)\n</code></pre>"},{"location":"strategies/multi-category-filters/#querying-by-category","title":"Querying by Category","text":"Array Metadata (Chroma &gt;= 1.5.0)Boolean Fields (Pre-1.5.0) <p>Use <code>$contains</code> to match documents with a specific category:</p> <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    where={\"categories\": {\"$contains\": \"games\"}},\n)\n</code></pre> <p>Match documents in any of several categories with <code>$or</code>:</p> <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    where={\n        \"$or\": [\n            {\"categories\": {\"$contains\": \"games\"}},\n            {\"categories\": {\"$contains\": \"movies\"}},\n        ]\n    },\n)\n</code></pre> <p>Exclude a category with <code>$not_contains</code>:</p> <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    where={\"categories\": {\"$not_contains\": \"sports\"}},\n)\n</code></pre> <p>Filter by a single category:</p> <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    where={\"games\": True},\n)\n</code></pre> <p>Filter by multiple categories with <code>$or</code>:</p> <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    where={\"$or\": [{\"games\": True}, {\"movies\": True}]},\n)\n</code></pre>"},{"location":"strategies/privacy/","title":"Privacy Strategies","text":""},{"location":"strategies/privacy/#overview","title":"Overview","text":"<p>TBD</p>"},{"location":"strategies/privacy/#encryption","title":"Encryption","text":""},{"location":"strategies/privacy/#document-encryption","title":"Document Encryption","text":""},{"location":"strategies/privacy/#client-side-document-encryption","title":"Client-side Document Encryption","text":"<p>Client-side document encryption notebook coming soon.</p>"},{"location":"strategies/rebuilding/","title":"Rebuilding Chroma DB","text":""},{"location":"strategies/rebuilding/#rebuilding-a-collection","title":"Rebuilding a Collection","text":"<p>Here are several reasons you might want to rebuild a collection:</p> <ul> <li>Your metadata or binary index is corrupted or even deleted</li> <li>Optimize performance of HNSW index after a large number of updates</li> </ul> <p>WAL Consistency and Backups</p> <p>Before you proceed, make sure to backup your data. Secondly make sure that your WAL contains all the data to allow  the proper rebuilding of the collection. For instance, after v0.4.22 you should not have run optimizations or WAL  cleanup.</p> <p>IMPORTANT</p> <p>Only do this on a stopped Chroma instance.</p> <p>Find the UUID of the target binary index directory to remove. Typically, the binary index directory is located in the persistent directory and is named after the collection vector segment (in <code>segments</code> table). You can find the UUID by running the following SQL query:</p> <pre><code>sqlite3 /path/to/db/chroma.sqlite3 \"select s.id, c.name from segments s join collections c on  s.collection=c.id where s.scope='VECTOR';\"\n</code></pre> <p>The above should print UUID dir and collection names.</p> <p>Once you remove/rename the UUID dir, restart Chroma and query your collection like so:</p> <pre><code>import chromadb\nclient = chromadb.HttpClient() # Adjust as per your client\nres = client.get_collection(\"my_collection\").get(limit=1,include=['embeddings'])\n</code></pre> <p>Chroma will recreate your collection from the WAL.</p> <p>Rebuilding the collection</p> <p>Depending on how large your collection is, this process can take a while.</p>"},{"location":"strategies/time-based-queries/","title":"Time-based Queries","text":""},{"location":"strategies/time-based-queries/#filtering-documents-by-timestamps","title":"Filtering Documents By Timestamps","text":"<p>In the example below, we create a collection with 100 documents, each with a random timestamp in the last two weeks. We then query the collection for documents that were created in the last week.</p> <p>The example demonstrates how Chroma metadata can be leveraged to filter documents based on how recently they were added or updated.</p> <pre><code>import uuid\nimport chromadb\n\nimport datetime\nimport random\n\nnow = datetime.datetime.now()\ntwo_weeks_ago = now - datetime.timedelta(days=14)\n\ndates = [\n    two_weeks_ago + datetime.timedelta(days=random.randint(0, 14))\n    for _ in range(100)\n]\ndates = [int(date.timestamp()) for date in dates]\n\n# convert epoch seconds to iso format\n\ndef iso_date(epoch_seconds): return datetime.datetime.fromtimestamp(\n    epoch_seconds).isoformat()\n\nclient = chromadb.EphemeralClient()\n\ncol = client.get_or_create_collection(\"test\")\n\ncol.add(ids=[f\"{uuid.uuid4()}\" for _ in range(100)], documents=[\n    f\"document {i}\" for i in range(100)], metadatas=[{\"date\": date} for date in dates])\n\nres = col.get(where={\"date\": {\"$gt\": (now - datetime.timedelta(days=7)).timestamp()}})\n\nfor i in res['metadatas']:\n    print(iso_date(i['date']))\n</code></pre> <p>Ref: https://gist.github.com/tazarov/3c9301d22ab863dca0b6fb1e5e3511b1</p>"},{"location":"strategies/multi-tenancy/","title":"Multi-Tenancy Strategies","text":""},{"location":"strategies/multi-tenancy/#introduction","title":"Introduction","text":"<p>Some deployment settings of Chroma may require multi-tenancy support. This document outlines the strategies for multi-tenancy approaches in Chroma.</p>"},{"location":"strategies/multi-tenancy/#approaches","title":"Approaches","text":"<ul> <li>Naive approach - This is a simple approach puts the onus of enforcing multi-tenancy on the   application. It is the simplest approach to implement, but is not very well suited for production environments.</li> <li>Multi-User Basic Auth - This article provides a stepping stone to more advanced   multi-tenancy where the Chroma   authentication allows for multiple users to access the same Chroma instance with their own credentials.</li> <li>Authorization Model with OpenFGA - Implement an advanced authorization model   with OpenFGA.</li> <li>Implementing OpenFGA Authorization Model In Chroma - Learn how to   implement OpenFGA authorization model in Chroma with full code example.</li> </ul>"},{"location":"strategies/multi-tenancy/authorization-model-impl-with-openfga/","title":"Implementing OpenFGA Authorization Model In Chroma","text":"<p>Source Code</p> <p>The source code for this article can be found here.</p>"},{"location":"strategies/multi-tenancy/authorization-model-impl-with-openfga/#preparation","title":"Preparation","text":"<p>To make things useful we also introduce an initial tuple set with permissions which will allows us to test the authorization model.</p> <p>We define three users:</p> <ul> <li><code>admin</code> part of <code>chroma</code> team as <code>owner</code></li> <li><code>user1</code> part of <code>chroma</code> team as <code>reader</code></li> <li><code>admin-ext</code> part of <code>external</code> team as <code>owner</code></li> </ul> <p>We will give enough permissions to these three users and their respective teams so that they can perform collection creation, deletion, add records, remove records, get records and query records in the context of their role within the team - <code>owner</code> has access to all API actions while <code>reader</code> can only read, list get, query.</p> <p>Abbreviate Example</p> <p>We have removed some of the data from the above example for brevity. The full tuple set can be found under data/data/initial-data.json</p> <pre><code>[\n  {\n    \"object\": \"team:chroma\",\n    \"relation\": \"owner\",\n    \"user\": \"user:admin\"\n  },\n  {\n    \"object\": \"team:chroma\",\n    \"relation\": \"reader\",\n    \"user\": \"user:user1\"\n  },\n  {\n    \"object\": \"team:external\",\n    \"relation\": \"owner\",\n    \"user\": \"user:admin-ext\"\n  },\n  {\n    \"object\": \"server:localhost\",\n    \"relation\": \"can_get_tenant\",\n    \"user\": \"team:chroma#owner\"\n  },\n  {\n    \"object\": \"tenant:default_tenant-default_database\",\n    \"relation\": \"can_get_database\",\n    \"user\": \"team:chroma#owner\"\n  },\n  {\n    \"object\": \"database:default_tenant-default_database\",\n    \"relation\": \"can_create_collection\",\n    \"user\": \"team:chroma#owner\"\n  },\n  {\n    \"object\": \"database:default_tenant-default_database\",\n    \"relation\": \"can_list_collections\",\n    \"user\": \"team:chroma#owner\"\n  },\n  {\n    \"object\": \"database:default_tenant-default_database\",\n    \"relation\": \"can_get_or_create_collection\",\n    \"user\": \"team:chroma#owner\"\n  },\n  {\n    \"object\": \"database:default_tenant-default_database\",\n    \"relation\": \"can_count_collections\",\n    \"user\": \"team:chroma#owner\"\n  }\n]\n</code></pre>"},{"location":"strategies/multi-tenancy/authorization-model-impl-with-openfga/#testing-the-model","title":"Testing the model","text":"<p>Let\u2019s spin up a quick docker compose to test our setup. In the repo we have provided <code>openfga/docker-compose.openfga-standalone.yaml</code></p> <pre><code>docker compose -f openfga/docker-compose.openfga-standalone.yaml up\n</code></pre> <p>For this next part ensure you have FGA CLI installed.</p> <p>Once the containers are up and running let\u2019s create a store and import the model:</p> <pre><code>export FGA_API_URL=http://localhost:8082 # our OpenFGA binds to 8082 on localhost\nfga store create --model data/models/model-article-p4.fga --name chromadb-auth\n</code></pre> <p>You should see a response like this:</p> <pre><code>{\n  \"store\": {\n    \"created_at\": \"2024-04-09T18:37:26.367747Z\",\n    \"id\": \"01HV3VB347NPY3NMX6VQ5N2E23\",\n    \"name\": \"chromadb-auth\",\n    \"updated_at\": \"2024-04-09T18:37:26.367747Z\"\n  },\n  \"model\": {\n    \"authorization_model_id\": \"01HV3VB34JAXWF0F3C00DFBZV4\"\n  }\n}\n</code></pre> <p>Let\u2019s import our initial tuple set. Before that make sure to export <code>FGA_STORE_ID</code> and <code>FGA_MODEL_ID</code> as per the output of the previous command:</p> <pre><code>export FGA_STORE_ID=01HV3VB347NPY3NMX6VQ5N2E23\nexport FGA_MODEL_ID=01HV3VB34JAXWF0F3C00DFBZV4\nfga tuple write --file data/data/initial-data.json\n</code></pre> <p>Let\u2019s test our imported model and tuples:</p> <pre><code>fga query check user:admin can_get_preflight server:localhost\n</code></pre> <p>If everything is working you should see this:</p> <pre><code>{\n  \"allowed\": true,\n  \"resolution\": \"\"\n}\n</code></pre>"},{"location":"strategies/multi-tenancy/authorization-model-impl-with-openfga/#implementing-authorization-plumbing-in-chroma","title":"Implementing Authorization Plumbing in Chroma","text":"<p>First we will start with making a few small changes to the authorization plugin we\u2019ve made. Why you ask? We need to introduce teams (aka groups). For that we\u2019ll resort to standard Apache <code>groupfile</code> as follows:</p> <pre><code>chroma: admin, user1\nexternal: admin-ext\n</code></pre> <p>The <code>groupfile</code> will be mounted to our Chroma container and read by the multi-user basic auth plugin. The changes to the authentication plugin are as follows:</p> <pre><code># imports as before\n\n@register_provider(\"multi_user_htpasswd_file\")\nclass MultiUserHtpasswdFileServerAuthCredentialsProvider(ServerAuthCredentialsProvider):\n    _creds: Dict[str, SecretStr]  # contains user:password-hash\n\n    def __init__(self, system: System) -&gt; None:\n        super().__init__(system)\n        try:\n            self.bc = importlib.import_module(\"bcrypt\")\n        except ImportError:\n            raise ValueError(aa\n                \"The bcrypt python package is not installed. \"\n                \"Please install it with `pip install bcrypt`\"\n            )\n        system.settings.require(\"chroma_server_auth_credentials_file\")\n        _file = str(system.settings.chroma_server_auth_credentials_file)\n        ...  # as before\n        _basepath = path.dirname(_file)\n        self._user_group_map = dict()\n        if path.exists(path.join(_basepath, \"groupfile\")):\n            _groups = dict()\n            with open(path.join(_basepath, \"groupfile\"), \"r\") as f:\n                for line in f:\n                    _raw_group = [v for v in line.strip().split(\":\")]\n                    if len(_raw_group) &lt; 2:\n                        raise ValueError(\n                            \"Invalid Htpasswd group file found in \"\n                            f\"[{path.join(_basepath, 'groupfile')}]. \"\n                            \"Must be &lt;groupname&gt;:&lt;username1&gt;,&lt;username2&gt;,...,&lt;usernameN&gt;.\"\n                        )\n                    _groups[_raw_group[0]] = [u.strip() for u in _raw_group[1].split(\",\")]\n                    for _group, _users in _groups.items():\n                        for _user in _users:\n                            if _user not in self._user_group_map:\n                                self._user_group_map[_user] = _group\n\n    @trace_method(  # type: ignore\n        \"MultiUserHtpasswdFileServerAuthCredentialsProvider.validate_credentials\",\n        OpenTelemetryGranularity.ALL,\n    )\n    @override\n    def validate_credentials(self, credentials: AbstractCredentials[T]) -&gt; bool:\n        ...  # as before\n\n    @override\n    def get_user_identity(\n            self, credentials: AbstractCredentials[T]\n    ) -&gt; Optional[SimpleUserIdentity]:\n        _creds = cast(Dict[str, SecretStr], credentials.get_credentials())\n        if _creds[\"username\"].get_secret_value() in self._user_group_map.keys():\n            return SimpleUserIdentity(\n                _creds[\"username\"].get_secret_value(),\n                attributes={\n                    \"team\": self._user_group_map[_creds[\"username\"].get_secret_value()]\n                },\n            )\n        return SimpleUserIdentity(_creds[\"username\"].get_secret_value(), attributes={\"team\": \"public\"})\n</code></pre> <p>Full code</p> <p>The code can be found under <code>chroma_auth/authn/basic/__**init__**.py</code></p> <p>We read the group file and for each user create a key in <code>self._user_group_map</code> to specify the group or team of that user. The information is returned as user identity attributes that is further used by the authz plugin.</p> <p>Now let\u2019s turn our attention to the authorization plugin. First let\u2019s start with that we\u2019re trying to achieve with it:</p> <ul> <li>Handle OpenFGA configuration from the import of the model as per the snippet above. This will help us to wire all   necessary parts of the code with correct authorization model configuration.</li> <li>Map all existing Chroma authorization actions to our authorization model</li> <li>Adapt any shortcomings or quirks in Chroma authorization to the way OpenFGA works</li> <li>Implement the Enforcement Point (EP) logic</li> <li>Implement OpenFGA Permissions API wrapper - this is a utility class that will help us update and keep updating the   OpenFGA tuples throughout collections\u2019 lifecycle.</li> </ul> <p>We\u2019ve split the implementation in two files:</p> <ul> <li><code>chroma_auth/authz/openfga/__init__.py</code> - Storing our OpenFGA authorization configuration reader and our authorization   plugin that adapts to Chroma authz model and enforces authorization decisions</li> <li><code>chroma_auth/authz/openfga/openfga_permissions.py</code> - Holds our OpenFGA permissions update logic.</li> <li><code>chroma_auth/instr/**__init__**.py</code> - holds our adapted FastAPI server from Chroma <code>0.4.24</code>. While the authz plugin   system in Chroma makes it easy to write the enforcement of authorization decisions, the update of permissions does   require us to into this rabbit hole. Don\u2019t worry the actual changes are minimal</li> </ul> <p>Let\u2019s cover things in a little more detail.</p> <p>Reading the configuration.</p> <pre><code>@register_provider(\"openfga_config_provider\")\nclass OpenFGAAuthorizationConfigurationProvider(\n    ServerAuthorizationConfigurationProvider[ClientConfiguration]\n):\n    _config_file: str\n    _config: ClientConfiguration\n\n    def __init__(self, system: System) -&gt; None:\n        super().__init__(system)\n        self._settings = system.settings\n        if \"FGA_API_URL\" not in os.environ:\n            raise ValueError(\"FGA_API_URL not set\")\n        self._config = self._try_load_from_file()\n\n        # TODO in the future we can also add credentials (preshared) or OIDC\n\n    def _try_load_from_file(self) -&gt; ClientConfiguration:\n        store_id = None\n        model_id = None\n        if \"FGA_STORE_ID\" in os.environ and \"FGA_MODEL_ID\" in os.environ:\n            return ClientConfiguration(\n                api_url=os.environ.get(\"FGA_API_URL\"),\n                store_id=os.environ[\"FGA_STORE_ID\"],\n                authorization_model_id=os.environ[\"FGA_MODEL_ID\"],\n            )\n        if \"FGA_CONFIG_FILE\" not in os.environ and not store_id and not model_id:\n            raise ValueError(\"FGA_CONFIG_FILE or FGA_STORE_ID/FGA_MODEL_ID env vars not set\")\n        with open(os.environ[\"FGA_CONFIG_FILE\"], \"r\") as f:\n            config = json.load(f)\n            return ClientConfiguration(\n                api_url=os.environ.get(\"FGA_API_URL\"),\n                store_id=config[\"store\"][\"id\"],\n                authorization_model_id=config[\"model\"][\"authorization_model_id\"],\n            )\n\n    @override\n    def get_configuration(self) -&gt; ClientConfiguration:\n        return self._config\n</code></pre> <p>This is a pretty simple and straightforward implementation that will either take env variables for the FGA Server URL, Store and Model or it will only take the server ULR + json configuration (the same as above).</p> <p>Next let\u2019s have a look at our <code>OpenFGAAuthorizationProvider</code> implementation. We\u2019ll start with the constructor where we adapt existing Chroma authorization actions to our model:</p> <pre><code>def __init__(self, system: System) -&gt; None:\n    # more code here, but we're skipping for brevity\n    self._authz_to_model_action_map = {\n        AuthzResourceActions.CREATE_DATABASE.value: \"can_create_database\",\n        AuthzResourceActions.GET_DATABASE.value: \"can_get_database\",\n        AuthzResourceActions.CREATE_TENANT.value: \"can_create_tenant\",\n        AuthzResourceActions.GET_TENANT.value: \"can_get_tenant\",\n        AuthzResourceActions.LIST_COLLECTIONS.value: \"can_list_collections\",\n        AuthzResourceActions.COUNT_COLLECTIONS.value: \"can_count_collections\",\n        AuthzResourceActions.GET_COLLECTION.value: \"can_get_collection\",\n        AuthzResourceActions.CREATE_COLLECTION.value: \"can_create_collection\",\n        AuthzResourceActions.GET_OR_CREATE_COLLECTION.value: \"can_get_or_create_collection\",\n        AuthzResourceActions.DELETE_COLLECTION.value: \"can_delete_collection\",\n        AuthzResourceActions.UPDATE_COLLECTION.value: \"can_update_collection\",\n        AuthzResourceActions.ADD.value: \"can_add_records\",\n        AuthzResourceActions.DELETE.value: \"can_delete_records\",\n        AuthzResourceActions.GET.value: \"can_get_records\",\n        AuthzResourceActions.QUERY.value: \"can_query_records\",\n        AuthzResourceActions.COUNT.value: \"can_count_records\",\n        AuthzResourceActions.UPDATE.value: \"can_update_records\",\n        AuthzResourceActions.UPSERT.value: \"can_upsert_records\",\n        AuthzResourceActions.RESET.value: \"can_reset\",\n    }\n\n    self._authz_to_model_object_map = {\n        AuthzResourceTypes.DB.value: \"database\",\n        AuthzResourceTypes.TENANT.value: \"tenant\",\n        AuthzResourceTypes.COLLECTION.value: \"collection\",\n    }\n</code></pre> <p>The above is located in <code>chroma_auth/authz/openfga/__init__.py</code></p> <p>The above is fairly straightforward mapping between <code>AuthzResourceActions</code> part of Chroma\u2019s auth framework and the relations (aka actions) we\u2019ve defined in our model above. Next we map also the <code>AuthzResourceTypes</code> to OpenFGA objects. This seem pretty simple right? Wrong, things are not so perfect and nothing exhibits this more than our next portion that takes the action and resource and returns object and relation to be checked:</p> <pre><code>def resolve_resource_action(self, resource: AuthzResource, action: AuthzAction) -&gt; tuple:\n    attrs = \"\"\n    tenant = None,\n    database = None\n    if \"tenant\" in resource.attributes:\n        attrs += f\"{resource.attributes['tenant']}\"\n        tenant = resource.attributes['tenant']\n    if \"database\" in resource.attributes:\n        attrs += f\"-{resource.attributes['database']}\"\n        database = resource.attributes['database']\n    if action.id == AuthzResourceActions.GET_TENANT.value or action.id == AuthzResourceActions.CREATE_TENANT.value:\n        return \"server:localhost\", self._authz_to_model_action_map[action.id]\n    if action.id == AuthzResourceActions.GET_DATABASE.value or action.id == AuthzResourceActions.CREATE_DATABASE.value:\n        return f\"tenant:{attrs}\", self._authz_to_model_action_map[action.id]\n    if action.id == AuthzResourceActions.CREATE_COLLECTION.value:\n        try:\n            cole_exists = self._api.get_collection(\n                resource.id, tenant=tenant, database=database\n            )\n            return f\"collection:{attrs}-{cole_exists.name}\", self._authz_to_model_action_map[\n                AuthzResourceActions.GET_COLLECTION.value]\n        except Exception as e:\n            return f\"{self._authz_to_model_object_map[resource.type]}:{attrs}\", self._authz_to_model_action_map[\n                action.id]\n    if resource.id == \"*\":\n        return f\"{self._authz_to_model_object_map[resource.type]}:{attrs}\", self._authz_to_model_action_map[action.id]\n    else:\n        return f\"{self._authz_to_model_object_map[resource.type]}:{attrs}-{resource.id}\",\n        self._authz_to_model_action_map[action.id]\n</code></pre> <p>Full code</p> <p>The above is located in <code>chroma_auth/authz/openfga/__init__.py</code></p> <p>The <code>resolve_resource_action</code> function demonstrates the idiosyncrasies of Chroma\u2019s auth. I have only myself to blame. The key takeaway is that there is room for improvement.</p> <p>The actual authorization enforcement is then dead simple:</p> <pre><code>def authorize(self, context: AuthorizationContext) -&gt; bool:\n    with OpenFgaClient(self._authz_config_provider.get_configuration()) as fga_client:\n        try:\n            obj, act = self.resolve_resource_action(resource=context.resource, action=context.action)\n            resp = fga_client.check(body=ClientCheckRequest(\n                user=f\"user:{context.user.id}\",\n                relation=act,\n                object=obj,\n            ))\n            # openfga_sdk.models.check_response.CheckResponse\n            return resp.allowed\n        except Exception as e:\n            logger.error(f\"Error while authorizing: {str(e)}\")\n            return False\n</code></pre> <p>At the end we\u2019ll look at the our permissions API wrapper. While a full-blown solution will implement all possible object lifecycle hooks, we\u2019re content with collections. Therefore we\u2019ll add lifecycle callbacks for creating and deleting collection (we\u2019re not considering, sharing of the collection with other users and change of ownership). So how does our create collection hook might look like you ask?</p> <pre><code>def create_collection_permissions(self, collection: Collection, request: Request) -&gt; None:\n    if not hasattr(request.state, \"user_identity\"):\n        return\n    identity = request.state.user_identity  # AuthzUser\n    tenant = request.query_params.get(\"tenant\")\n    database = request.query_params.get(\"database\")\n    _object = f\"collection:{tenant}-{database}-{collection.id}\"\n    _object_for_get_collection = f\"collection:{tenant}-{database}-{collection.name}\"  # this is a bug in the Chroma Authz that feeds in the name of the collection instead of ID\n    _user = f\"team:{identity.get_user_attributes()['team']}#owner\" if identity.get_user_attributes() and \"team\" in identity.get_user_attributes() else f\"user:{identity.get_user_id()}\"\n    _user_writer = f\"team:{identity.get_user_attributes()['team']}#writer\" if identity.get_user_attributes() and \"team\" in identity.get_user_attributes() else None\n    _user_reader = f\"team:{identity.get_user_attributes()['team']}#reader\" if identity.get_user_attributes() and \"team\" in identity.get_user_attributes() else None\n    with OpenFgaClient(self._fga_configuration) as fga_client:\n        fga_client.write_tuples(\n            body=[\n                ClientTuple(_user, \"can_add_records\", _object),\n                ClientTuple(_user, \"can_delete_records\", _object),\n                ClientTuple(_user, \"can_update_records\", _object),\n                ClientTuple(_user, \"can_get_records\", _object),\n                ClientTuple(_user, \"can_upsert_records\", _object),\n                ClientTuple(_user, \"can_count_records\", _object),\n                ClientTuple(_user, \"can_query_records\", _object),\n                ClientTuple(_user, \"can_get_collection\", _object_for_get_collection),\n                ClientTuple(_user, \"can_delete_collection\", _object_for_get_collection),\n                ClientTuple(_user, \"can_update_collection\", _object),\n            ]\n        )\n        if _user_writer:\n            fga_client.write_tuples(\n                body=[\n                    ClientTuple(_user_writer, \"can_add_records\", _object),\n                    ClientTuple(_user_writer, \"can_delete_records\", _object),\n                    ClientTuple(_user_writer, \"can_update_records\", _object),\n                    ClientTuple(_user_writer, \"can_get_records\", _object),\n                    ClientTuple(_user_writer, \"can_upsert_records\", _object),\n                    ClientTuple(_user_writer, \"can_count_records\", _object),\n                    ClientTuple(_user_writer, \"can_query_records\", _object),\n                    ClientTuple(_user_writer, \"can_get_collection\", _object_for_get_collection),\n                    ClientTuple(_user_writer, \"can_delete_collection\", _object_for_get_collection),\n                    ClientTuple(_user_writer, \"can_update_collection\", _object),\n                ]\n            )\n        if _user_reader:\n            fga_client.write_tuples(\n                body=[\n                    ClientTuple(_user_reader, \"can_get_records\", _object),\n                    ClientTuple(_user_reader, \"can_query_records\", _object),\n                    ClientTuple(_user_reader, \"can_count_records\", _object),\n                    ClientTuple(_user_reader, \"can_get_collection\", _object_for_get_collection),\n                ]\n            )\n</code></pre> <p>Full code</p> <p>You can find the full code in <code>chroma_auth/authz/openfga/openfga_permissions.py</code></p> <p>Looks pretty straight, but hold on I hear a thought creeping in your mind. \u201cWhy are you adding roles manually?\u201d</p> <p>You are right, it lacks that DRY-je-ne-sais-quoi, and I\u2019m happy to keep it simple an explicit. A more mature implementation can read the model figure out what type we\u2019re adding permissions for and then for each relation add the requisite users, but premature optimization is difficult to put in an article that won\u2019t turn into a book.</p> <p>With the above code we make the assumption that the collection doesn\u2019t exist ergo its permissions tuples don\u2019t exist. ( OpenFGA will fail to add tuples that already exist and there is not way around it other than deleting them first). Remember permission tuple lifecycle is your responsibility when adding authz to your application.</p> <p>The delete is oddly similar (that\u2019s why we\u2019ve skipped the bulk of it):</p> <pre><code>def delete_collection_permissions(self, collection: Collection, request: Request) -&gt; None:\n    if not hasattr(request.state, \"user_identity\"):\n        return\n    identity = request.state.user_identity\n\n    _object = f\"collection:{collection.tenant}-{collection.database}-{collection.id}\"\n    _object_for_get_collection = f\"collection:{collection.tenant}-{collection.database}-{collection.name}\"  # this is a bug in the Chroma Authz that feeds in the name of the collection instead of ID\n    _user = f\"team:{identity.get_user_attributes()['team']}#owner\" if identity.get_user_attributes() and \"team\" in identity.get_user_attributes() else f\"user:{identity.get_user_id()}\"\n    _user_writer = f\"team:{identity.get_user_attributes()['team']}#writer\" if identity.get_user_attributes() and \"team\" in identity.get_user_attributes() else None\n    _user_reader = f\"team:{identity.get_user_attributes()['team']}#reader\" if identity.get_user_attributes() and \"team\" in identity.get_user_attributes() else None\n    with OpenFgaClient(self._fga_configuration) as fga_client:\n        fga_client.delete_tuples(\n            body=[\n                ClientTuple(_user, \"can_add_records\", _object),\n                ClientTuple(_user, \"can_delete_records\", _object),\n                ClientTuple(_user, \"can_update_records\", _object),\n                ClientTuple(_user, \"can_get_records\", _object),\n                ClientTuple(_user, \"can_upsert_records\", _object),\n                ClientTuple(_user, \"can_count_records\", _object),\n                ClientTuple(_user, \"can_query_records\", _object),\n                ClientTuple(_user, \"can_get_collection\", _object_for_get_collection),\n                ClientTuple(_user, \"can_delete_collection\", _object_for_get_collection),\n                ClientTuple(_user, \"can_update_collection\", _object),\n            ]\n        )\n    # more code in the repo\n</code></pre> <p>Full code</p> <p>You can find the full code in <code>chroma_auth/authz/openfga/openfga_permissions.py</code></p> <p>Let\u2019s turn our attention at the last piece of code - the necessary evil of updating the FastAPI in Chroma to add our Permissions API hooks. We start simple by injecting our component using Chroma\u2019s DI (dependency injection).</p> <pre><code>from chroma_auth.authz.openfga.openfga_permissions import OpenFGAPermissionsAPI\n\nself._permissionsApi: OpenFGAPermissionsAPI = self._system.instance(OpenFGAPermissionsAPI)\n</code></pre> <p>The we add a hook for collection creation:</p> <pre><code>def create_collection(\n        self,\n        request: Request,\n        collection: CreateCollection,\n        tenant: str = DEFAULT_TENANT,\n        database: str = DEFAULT_DATABASE,\n) -&gt; Collection:\n    existing = None\n    try:\n        existing = self._api.get_collection(collection.name, tenant=tenant, database=database)\n    except ValueError as e:\n        if \"does not exist\" not in str(e):\n            raise e\n    collection = self._api.create_collection(\n        name=collection.name,\n        metadata=collection.metadata,\n        get_or_create=collection.get_or_create,\n        tenant=tenant,\n        database=database,\n    )\n    if not existing:\n        self._permissionsApi.create_collection_permissions(collection=collection, request=request)\n    return collection\n</code></pre> <p>Full code</p> <p>You can find the full code in <code>chroma_auth/instr/__init__.py</code></p> <p>And one for collection removal:</p> <pre><code>def delete_collection(\n        self,\n        request: Request,\n        collection_name: str,\n        tenant: str = DEFAULT_TENANT,\n        database: str = DEFAULT_DATABASE,\n) -&gt; None:\n    collection = self._api.get_collection(collection_name, tenant=tenant, database=database)\n    resp = self._api.delete_collection(\n        collection_name, tenant=tenant, database=database\n    )\n\n    self._permissionsApi.delete_collection_permissions(collection=collection, request=request)\n    return resp\n</code></pre> <p>Full code</p> <p>You can find the full code in <code>chroma_auth/instr/__init__.py</code></p> <p>The key thing to observe about the above snippets is that we invoke permissions API when we\u2019re sure things have been persisted in the DB. I know, I know, atomicity here is also important, but that is for another article. Just keep in mind that it is easier to fix broken permission than broken data.</p> <p>I promise this was the last bit of python code you\u2019ll see in this article.</p>"},{"location":"strategies/multi-tenancy/authorization-model-impl-with-openfga/#the-infra","title":"The Infra","text":"<p>Infrastructure!!! Finally, a sigh of relieve.</p> <p>Let\u2019s draw a diagrams:</p> <p></p> <p>Link</p> <p>We have our Chroma server, that relies on OpenFGA which persists data in PostgreSQL. \u201cOk, but \u2026\u201d, I can see you scratch your head, \u201c\u2026 how do I bring this magnificent architecture to live?\u201d. I thought you\u2019d never ask. We\u2019ll rely on our trusty docker compose skills with the following sequence in mind:</p> <p></p> <p>\u201cWhere is the <code>docker-compose.yaml</code>!\u201d. Voil\u00e0, my impatient friends:</p> <pre><code>version: '3.9'\n\nnetworks:\n  net:\n    driver: bridge\n\nservices:\n  server:\n    depends_on:\n      openfga:\n        condition: service_healthy\n      import:\n        condition: service_completed_successfully\n    image: chroma-server\n    build:\n      dockerfile: Dockerfile\n    volumes:\n      - ./chroma-data:/chroma/chroma\n      - ./server.htpasswd:/chroma/server.htpasswd\n      - ./groupfile:/chroma/groupfile\n      - ./data/:/data\n    command: \"--workers 1 --host 0.0.0.0 --port 8000 --proxy-headers --log-config chromadb/log_config.yml --timeout-keep-alive 30\"\n    environment:\n      - IS_PERSISTENT=TRUE\n      - CHROMA_SERVER_AUTH_PROVIDER=${CHROMA_SERVER_AUTH_PROVIDER}\n      - CHROMA_SERVER_AUTH_CREDENTIALS_FILE=${CHROMA_SERVER_AUTH_CREDENTIALS_FILE}\n      - CHROMA_SERVER_AUTH_CREDENTIALS=${CHROMA_SERVER_AUTH_CREDENTIALS}\n      - CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER=${CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER}\n      - CHROMA_SERVER_AUTH_TOKEN_TRANSPORT_HEADER=${CHROMA_SERVER_AUTH_TOKEN_TRANSPORT_HEADER}\n      - PERSIST_DIRECTORY=${PERSIST_DIRECTORY:-/chroma/chroma}\n      - CHROMA_OTEL_EXPORTER_ENDPOINT=${CHROMA_OTEL_EXPORTER_ENDPOINT}\n      - CHROMA_OTEL_EXPORTER_HEADERS=${CHROMA_OTEL_EXPORTER_HEADERS}\n      - CHROMA_OTEL_SERVICE_NAME=${CHROMA_OTEL_SERVICE_NAME}\n      - CHROMA_OTEL_GRANULARITY=${CHROMA_OTEL_GRANULARITY}\n      - CHROMA_SERVER_NOFILE=${CHROMA_SERVER_NOFILE}\n      - CHROMA_SERVER_AUTHZ_PROVIDER=${CHROMA_SERVER_AUTHZ_PROVIDER}\n      - CHROMA_SERVER_AUTHZ_CONFIG_PROVIDER=${CHROMA_SERVER_AUTHZ_CONFIG_PROVIDER}\n      - FGA_API_URL=http://openfga:8080\n      - FGA_CONFIG_FILE=/data/store.json # we expect that the import job will create this file\n    restart: unless-stopped # possible values are: \"no\", always\", \"on-failure\", \"unless-stopped\"\n    ports:\n      - \"8000:8000\"\n    healthcheck:\n      # Adjust below to match your container port\n      test: [ \"CMD\", \"curl\", \"-f\", \"http://localhost:8000/api/v1/heartbeat\" ]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n    networks:\n      - net\n  postgres:\n    image: postgres:14\n    container_name: postgres\n    networks:\n      - net\n    ports:\n      - \"5432:5432\"\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=password\n    healthcheck:\n      test: [ \"CMD-SHELL\", \"pg_isready -U postgres\" ]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n    volumes:\n      - postgres_data_openfga:/var/lib/postgresql/data\n\n  migrate:\n    depends_on:\n      postgres:\n        condition: service_healthy\n    image: openfga/openfga:latest\n    container_name: migrate\n    command: migrate\n    environment:\n      - OPENFGA_DATASTORE_ENGINE=postgres\n      - OPENFGA_DATASTORE_URI=postgres://postgres:password@postgres:5432/postgres?sslmode=disable\n    networks:\n      - net\n  openfga:\n    depends_on:\n      migrate:\n        condition: service_completed_successfully\n    image: openfga/openfga:latest\n    container_name: openfga\n    environment:\n      - OPENFGA_DATASTORE_ENGINE=postgres\n      - OPENFGA_DATASTORE_URI=postgres://postgres:password@postgres:5432/postgres?sslmode=disable\n      - OPENFGA_LOG_FORMAT=json\n    command: run\n    networks:\n      - net\n    ports:\n      # Needed for the http server\n      - \"8082:8080\"\n      # Needed for the grpc server (if used)\n      - \"8083:8081\"\n      # Needed for the playground (Do not enable in prod!)\n      - \"3003:3000\"\n    healthcheck:\n      test: [ \"CMD\", \"/usr/local/bin/grpc_health_probe\", \"-addr=openfga:8081\" ]\n      interval: 5s\n      timeout: 30s\n      retries: 3\n  import:\n    depends_on:\n      openfga:\n        condition: service_healthy\n    image: fga-cli\n    build:\n      context: .\n      dockerfile: Dockerfile-fgacli\n    container_name: import\n    volumes:\n      - ./data/:/data\n    command: |\n      /bin/sh -c \"/data/create_store_and_import.sh\"\n    environment:\n      - FGA_SERVER_URL=http://openfga:8080\n    networks:\n      - net\nvolumes:\n  postgres_data_openfga:\n    driver: local\n</code></pre> <p>Don\u2019t forget to create an <code>.env</code> file:</p> <pre><code>CHROMA_SERVER_AUTH_PROVIDER = \"chromadb.auth.basic.BasicAuthServerProvider\"\nCHROMA_SERVER_AUTH_CREDENTIALS_FILE = \"server.htpasswd\"\nCHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER = \"chroma_auth.authn.basic.MultiUserHtpasswdFileServerAuthCredentialsProvider\"\nCHROMA_SERVER_AUTHZ_PROVIDER = \"chroma_auth.authz.openfga.OpenFGAAuthorizationProvider\"\nCHROMA_SERVER_AUTHZ_CONFIG_PROVIDER = \"chroma_auth.authz.openfga.OpenFGAAuthorizationConfigurationProvider\"\n</code></pre> <p>Update your <code>server.htpasswd</code> to include the new user:</p> <pre><code>admin:$2\ny$05$vkBK4b1Vk5O98jNHgr.uduTJsTOfM395sKEKe48EkJCVPH / MBIeHK\nuser1:$2\ny$05$UQ0kC2x3T2XgeN4WU12BdekUwCJmLjJNhMaMtFNolYdj83OqiEpVu\nadmin - ext:$2\ny$05$9.\nL13wKQTHeXz9IH2UO2RurWEK. / Z24qapzyi6ywQGJds2DaC36C2\n</code></pre> <p>And the <code>groupfile</code> from before. And don\u2019t forget to take a look at the import script under - <code>data/create_store_and_import.sh</code></p> <p>Run the following command at the root of the repo and let things fail and burn down (or in the event this works - awe you, disclaimer - it worked on my machine):</p> <pre><code>docker\ncompose\nup - -build\n</code></pre>"},{"location":"strategies/multi-tenancy/authorization-model-impl-with-openfga/#tests-who-needs-test-when-you-have-stable-infra","title":"Tests, who needs test when you have stable infra!","text":"<p>Authorization is serious stuff, which is why we\u2019ve created a bare minimum set of tests to prove we\u2019re not totally wrong about it!</p> <p>Real Serious Note</p> <p>Serious Note: Take these things seriously and write a copious amounts of tests before rolling out things to prod. Don\u2019t become OWASP Top10 \u201cHero\u201d. Broken access controls is a thing that WILL keep you up at night.</p> <p>We\u2019ll focus on three areas:</p> <ul> <li>Testing admin (owner) access</li> <li>Testing team access for owner and reader roles</li> <li>Testing cross team permissions</li> </ul> <p>Admin Access</p> <p>Simple check to ensure that whoever created the collection (aka the owner) is allowed all actions.</p> <pre><code>import uuid\nimport chromadb\nfrom chromadb.config import Settings\n\nclient = chromadb.HttpClient(\n    settings=Settings(chroma_client_auth_provider=\"chromadb.auth.basic.BasicAuthClientProvider\",\n                      chroma_client_auth_credentials=\"admin:password123\"))\nclient.heartbeat()  # this should work with or without authentication - it is a public endpoint\nclient.list_collections()  # this is a protected endpoint and requires authentication\n\ncol = client.get_or_create_collection(f\"test_collection-{str(uuid.uuid4())}\")\ncol.add(ids=[\"1\"], documents=[\"test doc\"])\n\ncol.get()\ncol.update(ids=[\"1\"], documents=[\"test doc 2\"])\ncol.count()\ncol.upsert(ids=[\"1\"], documents=[\"test doc 3\"])\ncol.delete(ids=[\"1\"])\n\nclient.delete_collection(col.name)\n</code></pre> <p>Full code</p> <p>You can find the full code in <code>test_auth.ipynb</code></p> <p>Team Access</p> <p>Team access tests whether roles and permissions associated with those roles are correctly enforced.</p> <pre><code>import uuid\nimport chromadb\nfrom chromadb.config import Settings\n\nclient = chromadb.HttpClient(\n    settings=Settings(chroma_client_auth_provider=\"chromadb.auth.basic.BasicAuthClientProvider\",\n                      chroma_client_auth_credentials=\"admin:password123\"))\nclient.heartbeat()  # this should work with or without authentication - it is a public endpoint\nclient.list_collections()  # this is a protected endpoint and requires authentication\n\ncol_name = f\"test_collection-{str(uuid.uuid4())}\"\ncol = client.get_or_create_collection(col_name)\nprint(f\"Creating collection {col.id}\")\ncol.add(ids=[\"1\"], documents=[\"test doc\"])\n\nclient.get_collection(col_name)\nclient = chromadb.HttpClient(\n    settings=Settings(chroma_client_auth_provider=\"chromadb.auth.basic.BasicAuthClientProvider\",\n                      chroma_client_auth_credentials=\"user1:password123\"))\n\nclient.heartbeat()  # this should work with or without authentication - it is a public endpoint\nclient.list_collections()  # this is a protected endpoint and requires authentication\nclient.count_collections()\nprint(\"Getting collection \" + col_name)\ncol = client.get_collection(col_name)\ncol.get()\ncol.count()\n\ntry:\n    client.delete_collection(col_name)\nexcept Exception as e:\n    print(e)  #expect unauthorized error\n\nclient = chromadb.HttpClient(\n    settings=Settings(chroma_client_auth_provider=\"chromadb.auth.basic.BasicAuthClientProvider\",\n                      chroma_client_auth_credentials=\"admin:password123\"))\n\nclient.delete_collection(col_name)\n</code></pre> <p>Full code</p> <p>You can find the full code in <code>test_auth.ipynb</code></p> <p>Cross-team access</p> <p>In the cross team access scenario we\u2019ll create a collection with one team owner (<code>admin</code>) and will try to access it (aka delete it) with another team\u2019s owner in a very mano-a-mano (owner-to-owner way). It is important to observe that all these collections are created within the same database (<code>default_database</code>)</p> <pre><code>import uuid\nimport chromadb\nfrom chromadb.config import Settings\n\ncol_name = f\"test_collection-{str(uuid.uuid4())}\"\nclient = chromadb.HttpClient(\n    settings=Settings(chroma_client_auth_provider=\"chromadb.auth.basic.BasicAuthClientProvider\",\n                      chroma_client_auth_credentials=\"admin:password123\"))\n\nclient.get_or_create_collection(col_name)\n\nclient = chromadb.HttpClient(\n    settings=Settings(chroma_client_auth_provider=\"chromadb.auth.basic.BasicAuthClientProvider\",\n                      chroma_client_auth_credentials=\"admin-ext:password123\"))\n\nclient.get_or_create_collection(\"external-collection\")\n\ntry:\n    client.delete_collection(col_name)\nexcept Exception as e:\n    print(\"Expected error for admin-ext: \", str(e))  #expect unauthorized error\n\nclient = chromadb.HttpClient(\n    settings=Settings(chroma_client_auth_provider=\"chromadb.auth.basic.BasicAuthClientProvider\",\n                      chroma_client_auth_credentials=\"admin:password123\"))\nclient.delete_collection(col_name)\ntry:\n    client.delete_collection(\"external-collection\")\nexcept Exception as e:\n    print(\"Expected error for admin: \", str(e))  #expect unauthorized error\n</code></pre> <p>Full code</p> <p>You can find the full code in <code>test_auth.ipynb</code></p>"},{"location":"strategies/multi-tenancy/authorization-model-with-openfga/","title":"Chroma Authorization Model with OpenFGA","text":"<p>Source Code</p> <p>The source code for this article can be found here.</p> <p>This article will not provide any code that you can use immediately but will set the stage for our next article, which will introduce the actual Chroma-OpenFGA integration.</p> <p>With that in mind, let\u2019s get started.</p> <p>Who is this article for? The intended audience is DevSecOps, but engineers and architects could also use this to learn about Chroma and the authorization models.</p>"},{"location":"strategies/multi-tenancy/authorization-model-with-openfga/#authorization-model","title":"Authorization Model","text":"<p>Authorization models are an excellent way to abstract the way you wish your users to access your application form the actual implementation.</p> <p>There are many ways to do authz, ranging from commercial Auth0 FGA to OSS options like Ory Keto/Kratos, CASBIN, Permify, and Kubescape, but for this article, we\u2019ve decided to use OpenFGA (which technically is Auth0\u2019s open-source framework for FGA).</p> <p>Why OpenFGA, I hear you ask? Here are a few reasons:</p> <ul> <li>Apache-2 licensed</li> <li>CNCF Incubating project</li> <li>Zanzibar alignment in that it is a ReBAC (Relation-based access control) system</li> <li>DSL for modeling and testing permissions (as well as JSON-base version for those with masochistic tendencies)</li> </ul> <p>OpenFGA has done a great job explaining the steps to building an Authorization model, which you can read here. We will go over those while keeping our goal of creating an authorization model for Chroma.</p> <p>It is worth noting that the resulting authorization model that we will create here will be suitable for many GenAI applications, such as general-purpose RAG systems. Still, it is not a one-size-fits-all solution to all problems. For instance, if you want to implement authz in Chroma within your organization, OpenFGA might not be the right tool for the job, and you should consult with your IT/Security department for guidance on integrating with existing systems.</p>"},{"location":"strategies/multi-tenancy/authorization-model-with-openfga/#the-goal","title":"The Goal","text":"<p>Our goal is to achieve the following:</p> <ul> <li>Allow fine-grained access to the following resources - collection, database, tenant, and Chroma server.</li> <li>AlGrouping of users for improved permission management.</li> <li>Individual user access to resources</li> <li>Roles - owner, writer, reader</li> </ul> <p>Document-Level Access</p> <p>Although granting access to individual documents in a collection can be beneficial in some contexts, we have left that part out of our goals to keep things as simple and short as possible. If you are interested in this topic, reach out, and we will help you.</p> <p>This article will not cover user management, commonly called Identity Access Management (IAM). We\u2019ll cover that in a subsequent article.</p>"},{"location":"strategies/multi-tenancy/authorization-model-with-openfga/#modeling-fundamentals","title":"Modeling Fundamentals","text":"<p>Let\u2019s start with the fundamentals:</p> <p><code>Why could user U perform an action A on an object O?</code></p> <p>We will attempt to answer the question in the context of Chroma by following OpenFGA approach to refining the model. The steps are:</p> <ol> <li>Pick the most important features.</li> <li>List of object types</li> <li>List of relations for the types</li> <li>Test the model</li> <li>Iterate</li> </ol> <p>Given that OpenFGA is Zanzibar inspired, the basic primitive for it is a tuple of the following format:</p> <pre><code>(User,Relation,Object)\n</code></pre> <p>With the above we can express any relation between a user (or a team or even another object) the action the user performs (captured by object relations) and the object (aka API resource).</p>"},{"location":"strategies/multi-tenancy/authorization-model-with-openfga/#pick-the-features","title":"Pick the features","text":"<p>In the context of Chroma, the features are the actions the user can perform on Chroma API (as of this writing v0.4.24).</p> <p>Let\u2019s explore what are the actions that users can perform:</p> <ul> <li>Create a tenant</li> <li>Get a tenant</li> <li>Create a database for a tenant</li> <li>Get a database for a tenant</li> <li>Create a collection in a database</li> <li>Delete a collection from a database</li> <li>Update collection name and metadata</li> <li>List collections in a database</li> <li>Count collections in a database</li> <li>Add records to a collection</li> <li>Delete records from a collection</li> <li>Update records in a collection</li> <li>Upsert records in a collection</li> <li>Count records in a collection</li> <li>Get records from a collection</li> <li>Query records in a collection</li> <li>Get pre-flight-checks</li> </ul> <p>Open Endpoints</p> <p>Note we will omit get <code>hearbeat</code> and get <code>version</code>actions as this is generally a good idea to be open so that orchestrators (docker/k8s) can get the health status of chroma.</p> <p>To make it easy to reason about relations in our authorization model we will rephrase the above to the following format:</p> <pre><code>A user {user} can perform action {action} to/on/in {object types} ... IF {conditions}\n</code></pre> <ul> <li>A user can perform action create tenant on Chroma server if they are owner of the server</li> <li>A user can perform action get tenant on Chroma server if they are a reader or writer or owner of the server</li> <li>A user can perform action create database on a tenant if they are an owner of the tenant</li> <li>A user can perform action get database on a tenant if they are reader, writer or owner of the tenant</li> <li>A user can perform action create collection on a database if they are a writer or an owner of the database</li> <li>A user can perform action delete collection on a database if they are a writer or an owner of the database</li> <li>A user can perform action update collection name or metadata on a database if they are a writer or an owner of the   database</li> <li>A user can perform action list collections in a database if they are a writer or an owner of the database</li> <li>A user can perform action count collections in a database if they are a writer or an owner of the database</li> <li>A user can perform action add records on a collection if they are writer or owner of the collection</li> <li>A user can perform action delete records on a collection if they are writer or owner of the collection</li> <li>A user can perform action update records on a collection if they are writer or owner of the collection</li> <li>A user can perform action upsert records on a collection if they are writer or owner of the collection</li> <li>A user can perform action get records on a collection if they are writer or owner or reader of the collection</li> <li>A user can perform action count records on a collection if they are writer or owner or reader of the collection</li> <li>A user can perform action query records on a collection if they are writer or owner or reader of the collection</li> <li>A user can perform action get pre-flight-checks on a Chroma server if they are writer or owner or reader of the server</li> </ul> <p>We don\u2019t have to get it all right in the first iteration, but the above is a good starting point that can be adapted further.</p> <p>The above statements alone are already a great introspection as to what we can do within Chroma and who is supposed to be able to do what. Please note that your mileage may vary, as per your authz requirements, but in our experience the variations are generally around the who.</p> <p>As an astute reader you have already noted that we\u2019re generally outlined some RBAC stuff in the form of owner, writer and reader.</p>"},{"location":"strategies/multi-tenancy/authorization-model-with-openfga/#list-the-objects","title":"List the objects!!!","text":"<p>Now that we know what our users can do, let\u2019s figure solidify our understanding of on what our users will be performing these actions, aka the object types.</p> <p>Let\u2019s call them out:</p> <ul> <li>User - this is basic and pretty obvious object type that we want to model our users after</li> <li>Chroma server - this is our top level object in the access relations</li> <li>Tenant - for most Chroma developers this will equate to a team or a group</li> <li>Database</li> <li>Collection</li> </ul> <p>We can also examine all of the <code>of the &lt;object&gt;</code> in the above statements to ensure we haven\u2019t missed any objects. So far seems we\u2019re all good.</p> <p>Now that we have our objects let\u2019s create a first iteration of our authorization model using OpenFGA DSL:</p> <pre><code>model\n  schema 1.1\n\ntype server\ntype user\ntype tenant\ntype database\ntype collection\n</code></pre> <p>OpenFGA CLI</p> <p>You will need to install openfga CLI - https://openfga.dev/docs/getting-started/install-sdk. Also check the VSCode extension for OpenFGA.</p> <p>Let\u2019s validate our work:</p> <pre><code>fga model validate --file model-article-p1.fga\n</code></pre> <p>You should see the following output:</p> <pre><code>{\n  \"is_valid\":true\n}\n</code></pre>"},{"location":"strategies/multi-tenancy/authorization-model-with-openfga/#relations","title":"Relations","text":"<p>Now that we have the actions and the objects, let us figure out the relationships we want to build into our model.</p> <p>To come up with our relations we can follow these two rules:</p> <ul> <li>Any noun of the type <code>{noun} of a/an/the {type}</code> expression (e.g. <code>of the collection</code>)</li> <li>Any verb or action described with <code>can {action} on/in {type}</code></li> </ul> <p>So now let\u2019s work on our model to expand it with relationships:</p> <pre><code>model\n  schema 1.1\n\ntype user\n\ntype server\n  relations\n    define owner: [user]\n    define reader: [user]\n    define writer: [user]\n    define can_get_preflight: reader or owner or writer\n    define can_create_tenant: owner or writer\n\ntype tenant\n  relations\n    define owner: [user]\n    define reader: [user]\n    define writer: [user]\n    define belongsTo: [server]\n    define can_create_database: owner from belongsTo or writer from belongsTo or owner or writer\n    define can_get_database: reader or owner or writer or owner from belongsTo or reader from belongsTo or writer from belongsTo\n\ntype database\n  relations\n    define owner: [user]\n    define reader: [user]\n    define writer: [user]\n    define belongsTo: [tenant]\n    define can_create_collection: owner from belongsTo or writer from belongsTo or owner or writer\n    define can_delete_collection: owner from belongsTo or writer from belongsTo or owner or writer\n    define can_list_collections: owner or writer or owner from belongsTo or writer from belongsTo\n    define can_get_collection: owner or writer or owner from belongsTo or writer from belongsTo\n    define can_get_or_create_collection: owner or writer or owner from belongsTo or writer from belongsTo\n    define can_count_collections: owner or writer or owner from belongsTo or writer from belongsTo\n\ntype collection\n  relations\n    define owner: [user]\n    define reader: [user]\n    define writer: [user]\n    define belongsTo: [database]\n    define can_add_records: writer or reader or owner from belongsTo or writer from belongsTo\n    define can_delete_records: writer or owner from belongsTo or writer from belongsTo\n    define can_update_records: writer or owner from belongsTo or writer from belongsTo\n    define can_get_records: reader or owner or writer or owner from belongsTo or reader from belongsTo or writer from belongsTo\n    define can_upsert_records: writer or owner from belongsTo or writer from belongsTo\n    define can_count_records: reader or owner or writer or owner from belongsTo or reader from belongsTo or writer from belongsTo\n    define can_query_records: reader or owner or writer or owner from belongsTo or reader from belongsTo or writer from belongsTo\n</code></pre> <p>Let\u2019s validated:</p> <pre><code>fga model validate --file model-article-p2.fga\n</code></pre> <p>This seems mostly accurate and should do ok as Authorization model. But let us see if we can make it better. If we are to implement the above we will end up with lots of permissions in OpenFGA, not that it can\u2019t handle them, but as we go into the implementation details it will become cumbersome to update and maintain all these permissions. So let\u2019s look for opportunity to simplify things a little.</p> <p>Can we make the model a little simpler and the first question we ask is do we really need owner, reader, writer on every object or can we make a decision about our model and simplify this. As it turns out we can. The way that most multi-user systems work is that they tend to gravitate to grouping things as a way to reduce the need to maintain a large number of permissions. In our case we can group our users into <code>team</code> and in each team we\u2019ll have owner, writer, reader</p> <p>Let\u2019s see the results:</p> <pre><code>model\n  schema 1.1\n\ntype user\n\ntype team\n  relations\n    define owner: [user]\n    define writer: [user]\n    define reader: [user]\n\ntype server\n  relations\n    define can_get_preflight: [user, team#owner, team#writer, team#reader]\n    define can_create_tenant: [user, team#owner, team#writer]\n    define can_get_tenant: [user, team#owner, team#writer, team#reader]\n\ntype tenant\n  relations\n    define can_create_database: [user, team#owner, team#writer]\n    define can_get_database: [user, team#owner, team#writer, team#reader]\n\ntype database\n  relations\n    define can_create_collection: [user, team#owner, team#writer]\n    define can_list_collections: [user, team#owner, team#writer, team#reader]\n    define can_get_or_create_collection: [user, team#owner, team#writer]\n    define can_count_collections: [user, team#owner, team#writer, team#reader]\n\ntype collection\n  relations\n    define can_delete_collection: [user, team#owner, team#writer]\n    define can_get_collection: [user, team#owner, team#writer, team#reader]\n    define can_update_collection: [user, team#owner, team#writer]\n    define can_add_records: [user, team#owner, team#writer]\n    define can_delete_records: [user, team#owner, team#writer]\n    define can_update_records: [user, team#owner, team#writer]\n    define can_get_records: [user, team#owner, team#writer, team#reader]\n    define can_upsert_records: [user, team#owner, team#writer]\n    define can_count_records: [user, team#owner, team#writer, team#reader]\n    define can_query_records: [user, team#owner, team#writer, team#reader]\n</code></pre> <p>That is arguably more readable.</p> <p>As you will observe we have also added <code>[user]</code> in the permissions of each object, why is that you may ask. The reason is that we want to build a fine-grained authorization, which means while a collection can be belong to a team, we can also grant individual permissions to users. This gives us a great way to play around with permissions at the cost of a more complex implementation of how permissions are managed, but we will get to that in the next post.</p> <p>We have also removed the <code>belongsTo</code> relationship as we no longer need it. Reason: OpenFGA does not allow access of relations more than a single layer into the hierarchy thus a collection cannot use the owner of its team for permissions (there are other ways to implement that outside of the scope of this article).</p> <p>Let\u2019s recap what is our model capable of doing:</p> <ul> <li>Fine-grained access control to objects is possible via relations</li> <li>Users can be grouped into teams (a single user per team is also acceptable for cases where you need a user to be the   sole owner of a collection or a database)</li> <li>Access to resources can be granted to individual users via object relations</li> <li>Define roles within a team (this can be extended to allow roles per resource, but is outside of the scope of this   article)</li> </ul> <p>In short we have achieved the goals we have initially set, with a relatively simple and understandable model. However, does our model work? Let\u2019s find out in the next section.</p>"},{"location":"strategies/multi-tenancy/authorization-model-with-openfga/#testing-the-model","title":"Testing the model","text":"<p>Luckily OpenFGA folks have provided a great developer experience by making it easy to write and run tests. This is a massive W and time-saver.</p> <ul> <li>An individual user can be given access to specific resources via relations</li> <li>Users can be part of any of the team roles</li> <li>An object can access by a team</li> </ul> <pre><code>name: Chroma Authorization Model Tests # optional\n\nmodel_file: ./model-article-p4.fga # you can specify an external .fga file, or include it inline\n\n# tuple_file: ./tuples.yaml # you can specify an external file, or include it inline\ntuples:\n  - user: user:jane\n    relation: owner\n    object: team:chroma\n  - user: user:john\n    relation: writer\n    object: team:chroma\n  - user: user:jill\n    relation: reader\n    object: team:chroma\n  - user: user:sam\n    relation: can_create_tenant\n    object: server:server1\n  - user: user:sam\n    relation: can_get_tenant\n    object: server:server1\n  - user: user:sam\n    relation: can_get_preflight\n    object: server:server1\n  - user: user:michelle\n    relation: can_create_tenant\n    object: server:server1\n  - user: team:chroma#owner\n    relation: can_get_preflight\n    object: server:server1\n  - user: team:chroma#owner\n    relation: can_create_tenant\n    object: server:server1\n  - user: team:chroma#owner\n    relation: can_get_tenant\n    object: server:server1\n  - user: team:chroma#writer\n    relation: can_get_preflight\n    object: server:server1\n  - user: team:chroma#writer\n    relation: can_create_tenant\n    object: server:server1\n  - user: team:chroma#writer\n    relation: can_get_tenant\n    object: server:server1\n  - user: team:chroma#reader\n    relation: can_get_preflight\n    object: server:server1\n  - user: team:chroma#reader\n    relation: can_get_tenant\n    object: server:server1\n\ntests:\n  - name: Users should have team roles\n    check:\n      - user: user:jane\n        object: team:chroma\n        assertions:\n          owner: true\n          writer: false\n          reader: false\n      - user: user:john\n        object: team:chroma\n        assertions:\n          writer: true\n          owner: false\n          reader: false\n      - user: user:jill\n        object: team:chroma\n        assertions:\n          writer: false\n          owner: false\n          reader: true\n      - user: user:unknown\n        object: team:chroma\n        assertions:\n          writer: false\n          owner: false\n          reader: false\n      - user: user:jane\n        object: team:unknown\n        assertions:\n          writer: false\n          owner: false\n          reader: false\n      - user: user:unknown\n        object: team:unknown\n        assertions:\n          writer: false\n          owner: false\n          reader: false\n  - name: Users should have direct access to server\n    check:\n      - user: user:sam\n        object: server:server1\n        assertions:\n          can_get_preflight: true\n          can_create_tenant: true\n          can_get_tenant: true\n      - user: user:michelle\n        object: server:server1\n        assertions:\n          can_get_preflight: false\n          can_create_tenant: true\n          can_get_tenant: false\n      - user: user:unknown\n        object: server:server1\n        assertions:\n          can_get_preflight: false\n          can_create_tenant: false\n          can_get_tenant: false\n      - user: user:jill\n        object: server:serverX\n        assertions:\n          can_get_preflight: false\n          can_create_tenant: false\n          can_get_tenant: false\n  - name: Users of a team should have access to server\n    check:\n      - user: user:jane\n        object: server:server1\n        assertions:\n          can_create_tenant: true\n          can_get_tenant: true\n          can_get_preflight: true\n      - user: user:john\n        object: server:server1\n        assertions:\n          can_create_tenant: true\n          can_get_tenant: true\n          can_get_preflight: true\n      - user: user:jill\n        object: server:server1\n        assertions:\n          can_create_tenant: false\n          can_get_tenant: true\n          can_get_preflight: true\n      - user: user:unknown\n        object: server:server1\n        assertions:\n          can_create_tenant: false\n          can_get_tenant: false\n          can_get_preflight: false\n</code></pre> <p>Let\u2019s run the tests:</p> <pre><code>fga model test --tests test.model-article-p4.fga.yaml\n</code></pre> <p>This will result in the following output:</p> <pre><code># Test Summary #\nTests 3/3 passing\nChecks 42/42 passing\n</code></pre> <p>That is all folks. We try to keep things as concise as possible and this article has already our levels of comfort in that area. The bottom line is that authorization is no joke and it should take as long of a time as needed.</p> <p>Writing out all tests will not be concise (maybe we\u2019ll add that to the repo).</p>"},{"location":"strategies/multi-tenancy/authorization-model-with-openfga/#conclusion","title":"Conclusion","text":"<p>In this article we\u2019ve have built an authorization model for Chroma from scratch using OpenFGA. Admittedly it is a simple model, it still gives is a lot of flexibility to control access to Chroma resources.</p>"},{"location":"strategies/multi-tenancy/authorization-model-with-openfga/#resources","title":"Resources","text":"<ul> <li>https://github.com/amikos-tech/chromadb-auth - the companion repo for this article (files are stored   under <code>openfga/basic/</code>)</li> <li>https://openfga.dev/docs - Read it, understand it, code it!</li> <li>https://marketplace.visualstudio.com/items?itemName=openfga.openfga-vscode - It makes your life easier</li> </ul>"},{"location":"strategies/multi-tenancy/multi-user-basic-auth/","title":"Multi-User Basic Auth","text":""},{"location":"strategies/multi-tenancy/multi-user-basic-auth/#why-multi-user-auth","title":"Why Multi-user Auth?","text":"<p>Multi-user authentication can be crucial for several reasons. Let's delve into this topic.</p> <p>Security\u2014The primary concern is the security of your deployments. You need to control who can access your data and ensure they are authorized to do so. You may wonder, since Chroma offers basic and token-based authentication, why is multi-user authentication necessary?</p> <p>You should never share your Chroma access credentials with your users or any app that depends on Chroma. The answer to this concern is a categorical NO.</p> <p>Another reason to consider multi-user authentication is to differentiate access to your data. However, the solution presented here doesn't provide this. It's a stepping stone towards our upcoming article on multi-tenancy and securing Chroma data.</p> <p>Last but not least is auditing. While we acknowledge this is not for everybody, there is ~~an~~ increasing pressure to provide visibility into your app via auditable events.</p> <p>Multi-user experiences - Not all GenAI apps are intended to be private or individual. This is another reason to consider and implement multi-user authentication and authorization.</p>"},{"location":"strategies/multi-tenancy/multi-user-basic-auth/#dive-right-in","title":"Dive right in.","text":"<p>Let's get straight to the point and build a multi-user authorization with basic authentication. Here's our goal:</p> <ul> <li>Develop a server-side authorization provider that can read multiple users from a <code>.htpasswd</code> file</li> <li>Generate a multi-user <code>.htpasswd</code> file with several test users</li> <li>Package our plugin with the Chroma base image and execute it using Docker Compose</li> </ul> <p>Auth CIP</p> <p>Chroma has detailed info about how its authentication and authorization are implemented. Should you want to learn more go read the CIP (Chroma Improvement Proposal doc).</p>"},{"location":"strategies/multi-tenancy/multi-user-basic-auth/#the-plugin","title":"The Plugin","text":"<pre><code>import importlib\nimport logging\nfrom typing import Dict, cast, TypeVar, Optional\n\nfrom chromadb.auth import (\n    ServerAuthCredentialsProvider,\n    AbstractCredentials,\n    SimpleUserIdentity,\n)\nfrom chromadb.auth.registry import register_provider\nfrom chromadb.config import System\nfrom chromadb.telemetry.opentelemetry import (\n    OpenTelemetryGranularity,\n    trace_method,\n    add_attributes_to_current_span,\n)\nfrom pydantic import SecretStr\nfrom overrides import override\n\nT = TypeVar(\"T\")\n\nlogger = logging.getLogger(__name__)\n\n\n@register_provider(\"multi_user_htpasswd_file\")\nclass MultiUserHtpasswdFileServerAuthCredentialsProvider(ServerAuthCredentialsProvider):\n    _creds: Dict[str, SecretStr]  # contains user:password-hash\n\n    def __init__(self, system: System) -&gt; None:\n        super().__init__(system)\n        try:\n            self.bc = importlib.import_module(\"bcrypt\")\n        except ImportError:\n            raise ValueError(\n                \"The bcrypt python package is not installed. \"\n                \"Please install it with `pip install bcrypt`\"\n            )\n        system.settings.require(\"chroma_server_auth_credentials_file\")\n        _file = str(system.settings.chroma_server_auth_credentials_file)\n        self._creds = dict()\n        with open(_file, \"r\") as f:\n            for line in f:\n                _raw_creds = [v for v in line.strip().split(\":\")]\n                if len(_raw_creds) != 2:\n                    raise ValueError(\n                        \"Invalid Htpasswd credentials found in \"\n                        f\"[{str(system.settings.chroma_server_auth_credentials_file)}]. \"\n                        \"Must be &lt;username&gt;:&lt;bcrypt passwd&gt;.\"\n                    )\n                self._creds[_raw_creds[0]] = SecretStr(_raw_creds[1])\n\n    @trace_method(  # type: ignore\n        \"MultiUserHtpasswdFileServerAuthCredentialsProvider.validate_credentials\",\n        OpenTelemetryGranularity.ALL,\n    )\n    @override\n    def validate_credentials(self, credentials: AbstractCredentials[T]) -&gt; bool:\n        _creds = cast(Dict[str, SecretStr], credentials.get_credentials())\n\n        if len(_creds) != 2 or \"username\" not in _creds or \"password\" not in _creds:\n            logger.error(\n                \"Returned credentials did match expected format: \"\n                \"dict[username:SecretStr, password: SecretStr]\"\n            )\n            add_attributes_to_current_span(\n                {\n                    \"auth_succeeded\": False,\n                    \"auth_error\": \"Returned credentials did match expected format: \"\n                                  \"dict[username:SecretStr, password: SecretStr]\",\n                }\n            )\n            return False  # early exit on wrong format\n        _user_pwd_hash = (\n            self._creds[_creds[\"username\"].get_secret_value()]\n            if _creds[\"username\"].get_secret_value() in self._creds\n            else None\n        )\n        validation_response = _user_pwd_hash is not None and self.bc.checkpw(\n            _creds[\"password\"].get_secret_value().encode(\"utf-8\"),\n            _user_pwd_hash.get_secret_value().encode(\"utf-8\"),\n        )\n        add_attributes_to_current_span(\n            {\n                \"auth_succeeded\": validation_response,\n                \"auth_error\": f\"Failed to validate credentials for user {_creds['username'].get_secret_value()}\"\n                if not validation_response\n                else \"\",\n            }\n        )\n        return validation_response\n\n    @override\n    def get_user_identity(\n            self, credentials: AbstractCredentials[T]\n    ) -&gt; Optional[SimpleUserIdentity]:\n        _creds = cast(Dict[str, SecretStr], credentials.get_credentials())\n        return SimpleUserIdentity(_creds[\"username\"].get_secret_value())\n</code></pre> <p>In less than 80 lines of code, we have our plugin. Let's delve into and explain some of the key points of the code above:</p> <ul> <li><code>__init__</code> - Here, we dynamically import bcrypt, which we'll use to check user credentials. We also read the   configured credentials file - <code>server.htpasswd</code> line by line, to retrieve each user (we assume each line contains a   new user with its bcrypt hash).</li> <li><code>validate_credentials</code> - This is where the magic happens. We initially perform some lightweight validations on the   credentials parsed by Chroma and passed to the plugin. Then, we attempt to retrieve the user and its hash from   the <code>_creds</code> dictionary. The final step is to verify the hash. We've also added some attributes to monitor our   authentication process in our observability layer (we have an upcoming article about this).</li> <li><code>get_user_identity</code> - Constructs a simple user identity, which the authorization plugin uses to verify permissions.   Although not needed for now, each authentication plugin must implement this, as user identities are crucial for   authorization.</li> </ul> <p>We'll store our plugin in <code>__init__.py</code> within the following directory structure - <code>chroma_auth/authn/basic/__init__.py</code> (refer to the repository for details).</p>"},{"location":"strategies/multi-tenancy/multi-user-basic-auth/#password-file","title":"Password file","text":"<p>Now that we have our plugin let\u2019s create a password file with a few users:</p> <p>Initial user:</p> <pre><code>echo \"password123\" | htpasswd -iBc server.htpasswd admin\n</code></pre> <p>The above will create (<code>-c</code> flag) a new server.htpasswd file with initial user <code>admin</code> and the password will be read from stdin (<code>-i</code> flag) and saved as bcrypt hash (<code>-B</code> flag)</p> <p>Let\u2019s add another user:</p> <pre><code>echo \"password123\" | htpasswd -iB server.htpasswd user1\n</code></pre> <p>Now our <code>server.htpasswd</code> file will look like this:</p> <pre><code>admin:$2y$05$vkBK4b1Vk5O98jNHgr.uduTJsTOfM395sKEKe48EkJCVPH/MBIeHK\nuser1:$2y$05$UQ0kC2x3T2XgeN4WU12BdekUwCJmLjJNhMaMtFNolYdj83OqiEpVu\n</code></pre> <p>Moving on to docker setup.</p>"},{"location":"strategies/multi-tenancy/multi-user-basic-auth/#docker-compose-setup","title":"Docker compose setup","text":"<p>Let\u2019s create a <code>Dockerfile</code> to bundle our plugin with the official Chroma image:</p> <pre><code>ARG CHROMA_VERSION=0.4.24\nFROM ghcr.io/chroma-core/chroma:${CHROMA_VERSION} as base\n\nCOPY chroma_auth/ /chroma/chroma_auth\n</code></pre> <p>This will pick up the official docker image for Chroma and will add our plugin directory structure so that we can use it.</p> <p>Now let\u2019s create an <code>.env</code> file to load our plugin:</p> <pre><code>CHROMA_SERVER_AUTH_PROVIDER=\"chromadb.auth.basic.BasicAuthServerProvider\"\nCHROMA_SERVER_AUTH_CREDENTIALS_FILE=\"server.htpasswd\"\nCHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER=\"chroma_auth.authn.basic.MultiUserHtpasswdFileServerAuthCredentialsProvider\"\n</code></pre> <p>And finally our <code>docker-compose.yaml</code>:</p> <pre><code>version: '3.9'\n\nnetworks:\n  net:\n    driver: bridge\n\nservices:\n  server:\n    image: chroma-server\n    build:\n      dockerfile: Dockerfile\n    volumes:\n      - ./chroma-data:/chroma/chroma\n      - ./server.htpasswd:/chroma/server.htpasswd\n    command: \"--workers 1 --host 0.0.0.0 --port 8000 --proxy-headers --log-config chromadb/log_config.yml --timeout-keep-alive 30\"\n    environment:\n      - IS_PERSISTENT=TRUE\n      - CHROMA_SERVER_AUTH_PROVIDER=${CHROMA_SERVER_AUTH_PROVIDER}\n      - CHROMA_SERVER_AUTH_CREDENTIALS_FILE=${CHROMA_SERVER_AUTH_CREDENTIALS_FILE}\n      - CHROMA_SERVER_AUTH_CREDENTIALS=${CHROMA_SERVER_AUTH_CREDENTIALS}\n      - CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER=${CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER}\n      - CHROMA_SERVER_AUTH_TOKEN_TRANSPORT_HEADER=${CHROMA_SERVER_AUTH_TOKEN_TRANSPORT_HEADER}\n      - PERSIST_DIRECTORY=${PERSIST_DIRECTORY:-/chroma/chroma}\n      - CHROMA_OTEL_EXPORTER_ENDPOINT=${CHROMA_OTEL_EXPORTER_ENDPOINT}\n      - CHROMA_OTEL_EXPORTER_HEADERS=${CHROMA_OTEL_EXPORTER_HEADERS}\n      - CHROMA_OTEL_SERVICE_NAME=${CHROMA_OTEL_SERVICE_NAME}\n      - CHROMA_OTEL_GRANULARITY=${CHROMA_OTEL_GRANULARITY}\n      - CHROMA_SERVER_NOFILE=${CHROMA_SERVER_NOFILE}\n    restart: unless-stopped # possible values are: \"no\", always\", \"on-failure\", \"unless-stopped\"\n    ports:\n      - \"8000:8000\"\n    healthcheck:\n      # Adjust below to match your container port\n      test: [ \"CMD\", \"curl\", \"-f\", \"http://localhost:8000/api/v1/heartbeat\" ]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n    networks:\n      - net\n</code></pre>"},{"location":"strategies/multi-tenancy/multi-user-basic-auth/#the-test","title":"The test","text":"<p>Let\u2019s run our docker compose setup:</p> <pre><code>docker compose --env-file ./.env up --build\n</code></pre> <p>You should see the following log message if the plugin was successfully loaded:</p> <pre><code>server-1  | DEBUG:    [01-04-2024 14:10:13] Starting component MultiUserHtpasswdFileServerAuthCredentialsProvider\nserver-1  | DEBUG:    [01-04-2024 14:10:13] Starting component BasicAuthServerProvider\nserver-1  | DEBUG:    [01-04-2024 14:10:13] Starting component FastAPIChromaAuthMiddleware\n</code></pre> <p>Once our container is up and running, let\u2019s see if our multi-user auth works:</p> <pre><code>import chromadb\nfrom chromadb.config import Settings\n\nclient = chromadb.HttpClient(\n    settings=Settings(chroma_client_auth_provider=\"chromadb.auth.basic.BasicAuthClientProvider\",chroma_client_auth_credentials=\"admin:password123\"))\nclient.heartbeat()  # this should work with or without authentication - it is a public endpoint\nclient.get_or_create_collection(\"test_collection\")  # this is a protected endpoint and requires authentication\nclient.list_collections()  # this is a protected endpoint and requires authentication\n</code></pre> <p>The above code should return the list of collections, a single collection <code>test_collection</code> that we created.</p> <pre><code>(chromadb-multi-user-basic-auth-py3.11) [chromadb-multi-user-basic-auth]python                                                                                                                                                                                                            19:51:38  \u2601  main \u2602 \u26a1 \u271a\nPython 3.11.7 (main, Dec 30 2023, 14:03:09) [Clang 15.0.0 (clang-1500.1.0.2.5)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import chromadb\n&gt;&gt;&gt; from chromadb.config import Settings\n&gt;&gt;&gt; \n&gt;&gt;&gt; client = chromadb.HttpClient(\n...     settings=Settings(chroma_client_auth_provider=\"chromadb.auth.basic.BasicAuthClientProvider\",chroma_client_auth_credentials=\"admin:password123\"))\n&gt;&gt;&gt; client.heartbeat()  # this should work with or without authentication - it is a public endpoint\n1711990302270211007\n&gt;&gt;&gt; \n&gt;&gt;&gt; client.list_collections()  # this is a protected endpoint and requires authentication\n[]\n</code></pre> <p>Great, now let\u2019s test for our other user:</p> <pre><code>client = chromadb.HttpClient(\n    settings=Settings(chroma_client_auth_provider=\"chromadb.auth.basic.BasicAuthClientProvider\",chroma_client_auth_credentials=\"user1:password123\"))\n</code></pre> <p>Works just as well (logs omitted for brevity).</p> <p>To ensure that our plugin works as expected let\u2019s also test with an user that is not in our <code>server.htpasswd</code> file:</p> <pre><code>client = chromadb.HttpClient(\n    settings=Settings(chroma_client_auth_provider=\"chromadb.auth.basic.BasicAuthClientProvider\",chroma_client_auth_credentials=\"invalid_user:password123\"))\n</code></pre> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/Users/tazarov/Library/Caches/pypoetry/virtualenvs/chromadb-multi-user-basic-auth-vIZuPNTE-py3.11/lib/python3.11/site-packages/chromadb/__init__.py\", line 197, in HttpClient\n    return ClientCreator(tenant=tenant, database=database, settings=settings)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tazarov/Library/Caches/pypoetry/virtualenvs/chromadb-multi-user-basic-auth-vIZuPNTE-py3.11/lib/python3.11/site-packages/chromadb/api/client.py\", line 144, in __init__\n    self._validate_tenant_database(tenant=tenant, database=database)\n  File \"/Users/tazarov/Library/Caches/pypoetry/virtualenvs/chromadb-multi-user-basic-auth-vIZuPNTE-py3.11/lib/python3.11/site-packages/chromadb/api/client.py\", line 445, in _validate_tenant_database\n    raise e\n  File \"/Users/tazarov/Library/Caches/pypoetry/virtualenvs/chromadb-multi-user-basic-auth-vIZuPNTE-py3.11/lib/python3.11/site-packages/chromadb/api/client.py\", line 438, in _validate_tenant_database\n    self._admin_client.get_tenant(name=tenant)\n  File \"/Users/tazarov/Library/Caches/pypoetry/virtualenvs/chromadb-multi-user-basic-auth-vIZuPNTE-py3.11/lib/python3.11/site-packages/chromadb/api/client.py\", line 486, in get_tenant\n    return self._server.get_tenant(name=name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tazarov/Library/Caches/pypoetry/virtualenvs/chromadb-multi-user-basic-auth-vIZuPNTE-py3.11/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py\", line 127, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/tazarov/Library/Caches/pypoetry/virtualenvs/chromadb-multi-user-basic-auth-vIZuPNTE-py3.11/lib/python3.11/site-packages/chromadb/api/fastapi.py\", line 200, in get_tenant\n    raise_chroma_error(resp)\n  File \"/Users/tazarov/Library/Caches/pypoetry/virtualenvs/chromadb-multi-user-basic-auth-vIZuPNTE-py3.11/lib/python3.11/site-packages/chromadb/api/fastapi.py\", line 649, in raise_chroma_error\n    raise chroma_error\nchromadb.errors.AuthorizationError: Unauthorized\n</code></pre> <p>As expected, we get auth error when trying to connect to Chroma (the client initialization validates the tenant and DB which are both protected endpoints which raises the exception above).</p>"},{"location":"strategies/multi-tenancy/naive-multi-tenancy/","title":"Naive Multi-tenancy Strategies","text":"<p>Single-note Chroma</p> <p>The below strategies are applicable to single-node Chroma only. The strategies require your app to act as both PEP (Policy Enforcement Point)  and PDP (Policy Decision Point) for authorization. This is a naive approach to multi-tenancy and is probably not suited for production environments, however it is a good and simple way to get started with multi-tenancy in Chroma.</p> <p>Authorization</p> <p>We are in the process of creating a list of articles on how to implement proper authorization in Chroma,  leveraging the an external service and Chroma's auth plugins. The first article of the series is available in  Medium  and will also be made available here soon.</p>"},{"location":"strategies/multi-tenancy/naive-multi-tenancy/#introduction","title":"Introduction","text":"<p>There are several multi-tenancy strategies available to users of Chroma. The actual strategy will depend on the needs of the user and the application. The strategies below apply to multi-user environments, but do no factor in partly-shared resources like groups or teams.</p> <ul> <li>User-Per-Doc: In this scenario, the app maintains multiple collections and each collection document is associated   with a single user.</li> <li>User-Per-Collection: In this scenario, the app maintains multiple collections and each collection is   associated with a single user.</li> <li>User-Per-Database: In this scenario, the app maintains multiple databases with a single tenant and each database   is   associated with a single user.</li> <li>User-Per-Tenant: In this scenario, the app maintains multiple tenants and each tenant is associated with a single   user.</li> </ul>"},{"location":"strategies/multi-tenancy/naive-multi-tenancy/#user-per-doc","title":"User-Per-Doc","text":"<p>The goal of this strategy is to grant user permissions to access individual documents.</p> <p></p> <p>To implement this strategy you need to add some sort of user identification to each document that belongs to a user. For this example we will assume it is <code>user_id</code>.</p> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient()\ncollection = client.get_or_create_collection(\"my-collection\")\ncollection.add(\n    documents=[\"This is document1\", \"This is document2\"],\n    metadatas=[{\"user_id\": \"user1\"}, {\"user_id\": \"user2\"}],\n    ids=[\"doc1\", \"doc2\"],\n)\n</code></pre> <p>At query time you will have to provide the <code>user_id</code> as a filter to your query like so:</p> <pre><code>results = collection.query(\n    query_texts=[\"This is a query document\"],\n    where=[{\"user_id\": \"user1\"}],\n)\n</code></pre> <p>To successfully implement this strategy your code needs to consistently add and filter on the <code>user_id</code> metadata to ensure separation of data.</p> <p>Drawbacks:</p> <ul> <li>Error-prone: Messing up the filtering can lead to data being leaked across users.</li> <li>Scalability: As the number of users and documents grow, doing filtering on metadata can become slow.</li> </ul>"},{"location":"strategies/multi-tenancy/naive-multi-tenancy/#user-per-collection","title":"User-Per-Collection","text":"<p>The goal of this strategy is to grant a user access to all documents in a collection.</p> <p></p> <p>To implement this strategy you need to create a collection for each user. For this example we will assume it is <code>user_id</code>.</p> <pre><code>import chromadb\n\nclient = chromadb.PersistentClient()\nuser_id = \"user1\"\ncollection = client.get_or_create_collection(f\"user-collection:{user_id}\")\ncollection.add(\n    documents=[\"This is document1\", \"This is document2\"],\n    ids=[\"doc1\", \"doc2\"],\n)\n</code></pre> <p>At query time you will have to provide the <code>user_id</code> as a filter to your query like so:</p> <pre><code>user_id = \"user1\"\nuser_collection = client.get_collection(f\"user-collection:{user_id}\")\nresults = user_collection.query(\n    query_texts=[\"This is a query document\"],\n)\n</code></pre> <p>To successfully implement this strategy your code needs to consistently create and query the correct collection for the user.</p> <p>Drawbacks:</p> <ul> <li>Error-prone: Messing up the collection name can lead to data being leaked across users.</li> <li>Shared document search: If you want to maintain some documents shared then you will have to create a separate   collection for those documents and allow users to query the shared collection as well.</li> </ul>"},{"location":"strategies/multi-tenancy/naive-multi-tenancy/#user-per-database","title":"User-Per-Database","text":"<p>The goal of this strategy is to associate a user with a single database thus granting them access to all collections and documents within the database.</p> <p></p> <pre><code>import chromadb\nfrom chromadb import DEFAULT_TENANT\nfrom chromadb import Settings\n\nadminClient = chromadb.AdminClient(Settings(\n    is_persistent=True,\n    persist_directory=\"multitenant\",\n))\n\n\n# For Remote Chroma server:\n# \n# adminClient= chromadb.AdminClient(Settings(\n#   chroma_api_impl=\"chromadb.api.fastapi.FastAPI\",\n#   chroma_server_host=\"localhost\",\n#   chroma_server_http_port=\"8000\",\n# ))\n\ndef get_or_create_db_for_user(user_id):\n    database = f\"db:{user_id}\"\n    try:\n        adminClient.get_database(database)\n    except Exception as e:\n        adminClient.create_database(database, DEFAULT_TENANT)\n    return DEFAULT_TENANT, database\n\n\nuser_id = \"user_John\"\n\ntenant, database = get_or_create_db_for_user(user_id)\n# replace with chromadb.HttpClient for remote Chroma server\nclient = chromadb.PersistentClient(path=\"multitenant\", tenant=tenant, database=database)\ncollection = client.get_or_create_collection(\"user_collection\")\ncollection.add(\n    documents=[\"This is document1\", \"This is document2\"],\n    ids=[\"doc1\", \"doc2\"],\n)\n</code></pre> <p>In the above code we do the following:</p> <ul> <li>We create or get a database for each user in the <code>DEFAULT_TENANT</code> using the <code>chromadb.AdminClient</code>.</li> <li>We then create a <code>PersistentClient</code> for each user with the <code>tenant</code> and <code>database</code> we got from the <code>AdminClient</code>.</li> <li>We then create or get collection and add data to it.</li> </ul> <p>Drawbacks:</p> <ul> <li>This strategy requires consistent management of tenants and databases and their use in the client application.</li> </ul>"},{"location":"strategies/multi-tenancy/naive-multi-tenancy/#user-per-tenant","title":"User-Per-Tenant","text":"<p>The goal of this strategy is to associate a user with a single tenant thus granting them access to all databases, collections, and documents within the tenant.</p> <p></p> <pre><code>import chromadb\nfrom chromadb import DEFAULT_DATABASE\nfrom chromadb import Settings\n\nadminClient = chromadb.AdminClient(Settings(\n    chroma_api_impl=\"chromadb.api.segment.SegmentAPI\",\n    is_persistent=True,\n    persist_directory=\"multitenant\",\n))\n\n\n# For Remote Chroma server:\n# \n# adminClient= chromadb.AdminClient(Settings(\n#   chroma_api_impl=\"chromadb.api.fastapi.FastAPI\",\n#   chroma_server_host=\"localhost\",\n#   chroma_server_http_port=\"8000\",\n# ))\n\ndef get_or_create_tenant_for_user(user_id):\n    tenant_id = f\"tenant_user:{user_id}\"\n    try:\n        adminClient.get_tenant(tenant_id)\n    except Exception as e:\n        adminClient.create_tenant(tenant_id)\n        adminClient.create_database(DEFAULT_DATABASE, tenant_id)\n    return tenant_id, DEFAULT_DATABASE\n\n\nuser_id = \"user1\"\n\ntenant, database = get_or_create_tenant_for_user(user_id)\n# replace with chromadb.HttpClient for remote Chroma server\nclient = chromadb.PersistentClient(path=\"multitenant\", tenant=tenant, database=database)\ncollection = client.get_or_create_collection(\"user_collection\")\ncollection.add(\n    documents=[\"This is document1\", \"This is document2\"],\n    ids=[\"doc1\", \"doc2\"],\n)\n</code></pre> <p>In the above code we do the following:</p> <ul> <li>We create or get a tenant for each user with <code>DEFAULT_DATABASE</code> using the <code>chromadb.AdminClient</code>.</li> <li>We then create a <code>PersistentClient</code> for each user with the <code>tenant</code> and <code>database</code> we got from the <code>AdminClient</code>.</li> <li>We then create or get collection and add data to it.</li> </ul> <p>Drawbacks:</p> <ul> <li>This strategy requires consistent management of tenants and databases and their use in the client application.</li> </ul>"}]}